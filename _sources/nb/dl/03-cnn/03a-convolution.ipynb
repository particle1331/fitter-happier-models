{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbfeb70",
   "metadata": {
    "papermill": {
     "duration": 0.004582,
     "end_time": "2024-11-27T10:39:53.213158",
     "exception": false,
     "start_time": "2024-11-27T10:39:53.208576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686edef",
   "metadata": {
    "papermill": {
     "duration": 0.002941,
     "end_time": "2024-11-27T10:39:53.218965",
     "exception": false,
     "start_time": "2024-11-27T10:39:53.216024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Consider classifying images using a linear model. Flattening the image into a vector \n",
    "and feeding it into a fully-connected network is not the best approach.\n",
    "The large flattened input vector requires a very large weight matrix.\n",
    "Moreover, it does not consider local spatial correlation of image pixels ({numref}`cat-conv`)\n",
    "(e.g. applying a fixed permutation to input data results in an equivalent network).\n",
    "This motivates only mixing nearby pixels in a linear combination resulting in a very sparse banded weight matrix ({numref}`toeplitz`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebf022",
   "metadata": {
    "papermill": {
     "duration": 0.001908,
     "end_time": "2024-11-27T10:39:53.223276",
     "exception": false,
     "start_time": "2024-11-27T10:39:53.221368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55df8bd",
   "metadata": {
    "papermill": {
     "duration": 0.001336,
     "end_time": "2024-11-27T10:39:53.238126",
     "exception": false,
     "start_time": "2024-11-27T10:39:53.236790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```{figure} ../../../img/cat.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: cat-conv\n",
    "---\n",
    "Nearby pixels combine to form meaningful features of an image. [Source](https://www.nationalgeographic.com/animals/mammals/facts/domestic-cat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae756a",
   "metadata": {
    "papermill": {
     "duration": 0.000779,
     "end_time": "2024-11-27T10:39:53.239639",
     "exception": false,
     "start_time": "2024-11-27T10:39:53.238860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let $\\boldsymbol{\\mathsf X}$ be an $n \\times n$ input image and $\\boldsymbol{\\mathsf{S}}$ be the $m \\times m$ output feature map. The banded weight matrix reduces the nonzero entries of the weight matrix from $m^2 n^2$ to $m^2{k}^2$ where a local region of $k \\times k$ pixels in the input image are mixed. If the feature detector is translationally invariant across the image,\n",
    "then the weights in each band are **shared**. This further reduces the number of weights to $O(k^2).$\n",
    "The resulting linear operation is called a **convolution** in two spatial dimensions:\n",
    "\n",
    "\n",
    "```{math}\n",
    "\\boldsymbol{\\mathsf{S}}_{ij} = (\\boldsymbol{\\mathsf X} \\circledast \\boldsymbol{\\mathsf{K}})_{ij} = \\sum_{x = 0}^{{k}-1} \\sum_{y=0}^{{k}-1} {\\boldsymbol{\\mathsf X}}_{i + x, j + y} \\, {\\boldsymbol{\\mathsf{K}}}_{xy}.\n",
    "```\n",
    "\n",
    "\n",
    "Observe that spatial ordering of the pixels in the input $\\boldsymbol{\\mathsf X}$ is preserved in the output $\\boldsymbol{\\mathsf{S}}.$ This is nice since we want spatial information and orientation across a stack of convolution operations to be passed down into the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c954b",
   "metadata": {
    "papermill": {
     "duration": 0.000667,
     "end_time": "2024-11-27T10:39:53.241006",
     "exception": false,
     "start_time": "2024-11-27T10:39:53.240339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```{figure} ../../../img/conv-cat-99.png\n",
    "---\n",
    "width: 30em\n",
    "align: center\n",
    "name: toeplitz\n",
    "---\n",
    "Banded Toeplitz matrix for classifying cat images [[Source]](https://deepimaging.github.io/lectures/lecture_9_intro_to_CNN's.pdf). The horizontal vectors contain the same pixel values. Note that there can be multiple bands for a 2D kernel. See this [SO answer](https://stackoverflow.com/a/44039201/1091950).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7531eef",
   "metadata": {
    "papermill": {
     "duration": 0.000787,
     "end_time": "2024-11-27T10:39:53.242491",
     "exception": false,
     "start_time": "2024-11-27T10:39:53.241704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "**Remark.** It can be shown that convolution has [translation equivariance](https://en.wikipedia.org/wiki/Equivariant_map) due to weight sharing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403b1b0a",
   "metadata": {
    "papermill": {
     "duration": 0.000646,
     "end_time": "2024-11-27T10:39:53.243794",
     "exception": false,
     "start_time": "2024-11-27T10:39:53.243148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```{figure} ../../../img/nn/03-cnn-weight-sharing.png\n",
    "---\n",
    "width: 600px\n",
    "align: center\n",
    "name: 03-cnn-weight-sharing\n",
    "---\n",
    "Having local neural connectivity and weight sharing (both kernel and bias weights) characterizes the convolution. [Source](https://maurice-weiler.gitlab.io/blog_post/cnn-book_2_conventional_cnns/)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.017501,
   "end_time": "2024-11-27T10:39:53.360685",
   "environment_variables": {},
   "exception": null,
   "input_path": "03a-convolution.ipynb",
   "output_path": "03a-convolution.ipynb",
   "parameters": {},
   "start_time": "2024-11-27T10:39:52.343184",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}