

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title> &mdash; OK Transformer</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=c854b03d" />

  
    <link rel="shortcut icon" href="_static/favicon.png"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=9eb32ce0"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
      <script src="_static/scripts/sphinx-book-theme.js"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="_static/design-tabs.js?v=f930bc37"></script>
      <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
      <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Introduction to Neural Networks" href="nb/dl/01-intro/01-intro.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#">
            
              <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="nb/dl/01-intro/01-intro.html">Introduction to Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/dl/02-optim/02-optim.html">Gradient Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/dl/00-backprop/00-backprop.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/dl/03-cnn/03-cnn.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/dl/04-sequence-models/04-intro.html">Language modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/dl/05-rnns/05-intro.html">Recurrent Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/dl/07-attention.html">Attention and Transformers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Project name not set</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"></li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/intro.md" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <figure class="align-default" id="okt">
<img alt="_images/okt.png" src="_images/okt.png" />
</figure>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ok-transformer --help
Exploring machine learning engineering and operations. ❚
</pre></div>
</div>
<p><a class="reference external" href="https://actions-badge.atrox.dev/particle1331/ok-transformer/goto?ref=master"><img alt="build-status" src="https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2Fparticle1331%2Fok-transformer%2Fbadge%3Fref%3Dmaster&amp;label=build&amp;logo=none" /></a>
<img alt="last-commit" src="https://img.shields.io/github/last-commit/particle1331/ok-transformer/master" />
<img alt="python" src="https://shields.io/badge/python-3.12%20-blue" />
<a class="reference external" href="https://github.com/particle1331/ok-transformer"><img alt="stars" src="https://img.shields.io/github/stars/particle1331/ok-transformer?style=social" /></a></p>
<br>
<figure class="align-default" id="banner">
<img alt="_images/banner.png" src="_images/banner.png" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Effect of batch normalization on the magnitude of preactivation gradients.</span><a class="headerlink" href="#banner" title="Link to this image"></a></p>
</figcaption>
</figure>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1><a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
<section id="frameworks">
<h2>Frameworks<a class="headerlink" href="#frameworks" title="Link to this heading"></a></h2>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/frameworks.drawio.svg"><img alt="_images/frameworks.drawio.svg" src="_images/frameworks.drawio.svg" style="width: 600px;" /></a>
</figure>
<br>
<br>
</section>
<section id="hardware">
<h2>Hardware<a class="headerlink" href="#hardware" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Component</strong></p></th>
<th class="head"><p><strong>Kaggle Notebook</strong></p></th>
<th class="head"><p><strong>MacBook Air M1</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>GPU 0</strong></p></td>
<td><p>Tesla P100-PCIE-16GB</p></td>
<td><p>Apple M1 Integrated GPU</p></td>
</tr>
<tr class="row-odd"><td><p><strong>CPU</strong></p></td>
<td><p>Intel Xeon CPU &#64; 2.00GHz</p></td>
<td><p>Apple M1 8-core CPU</p></td>
</tr>
<tr class="row-even"><td><p><strong>Core</strong></p></td>
<td><p>1</p></td>
<td><p>4 high-performance, 4 efficiency</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Threads per core</strong></p></td>
<td><p>2</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p><strong>L3 Cache</strong></p></td>
<td><p>38.5 MiB</p></td>
<td><p>12 MiB</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Memory</strong></p></td>
<td><p>15 GB</p></td>
<td><p>8 GB Unified Memory</p></td>
</tr>
</tbody>
</table>
<br>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://drive.google.com/drive/folders/1lYSP_FuM5boq1u1iYP_bHKYu1pDCIGI4?usp=sharing"><img alt="Google Drive" src="https://img.shields.io/badge/Google%20Drive-4285F4?logo=googledrive&amp;logoColor=fff" /></a></p>
<div class="toggle docutils container">
<div class="docutils container" id="id3">
<div role="list" class="citation-list">
<div class="citation" id="id89" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Bar91<span class="fn-bracket">]</span></span>
<p>A. R. Barron. Approximation and estimation bounds for artificial neural networks. In <em>Proceedings of the Fourth Annual Workshop on Computational Learning Theory</em>, COLT '91, 243–249. San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc.</p>
</div>
<div class="citation" id="id64" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BDVJ03<span class="fn-bracket">]</span></span>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. <em>J. Mach. Learn. Res.</em>, 3:1137–1155, March 2003. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=944919.944966">http://dl.acm.org/citation.cfm?id=944919.944966</a>.</p>
</div>
<div class="citation" id="id117" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CvMG+14<span class="fn-bracket">]</span></span>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a>, <a class="reference external" href="https://arxiv.org/abs/1406.1078">arXiv:1406.1078</a>.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cho21<span class="fn-bracket">]</span></span>
<p>François Chollet. <em>Deep Learning with Python, Second Edition</em>. Manning, 2021. ISBN 9781617296864.</p>
</div>
<div class="citation" id="id91" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CHM+15<span class="fn-bracket">]</span></span>
<p>Anna Choromanska, MIkael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The Loss Surfaces of Multilayer Networks. In Guy Lebanon and S. V. N. Vishwanathan, editors, <em>Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</em>, volume 38 of Proceedings of Machine Learning Research, 192–204. San Diego, California, USA, 09–12 May 2015. PMLR. URL: <a class="reference external" href="https://proceedings.mlr.press/v38/choromanska15.html">https://proceedings.mlr.press/v38/choromanska15.html</a>.</p>
</div>
<div class="citation" id="id92" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DPG+14<span class="fn-bracket">]</span></span>
<p>Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In <em>Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2</em>, NIPS'14, 2933–2941. Cambridge, MA, USA, 2014. MIT Press.</p>
</div>
<div class="citation" id="id8" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Geron19<span class="fn-bracket">]</span></span>
<p>Aurélien Géron. <em>Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems, Second Edition</em>. O'Reilly Media, Sebastopol, CA, 2019. ISBN 978-1491962299.</p>
</div>
<div class="citation" id="id69" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GDG+17<span class="fn-bracket">]</span></span>
<p>Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. 2017. URL: <a class="reference external" href="https://arxiv.org/abs/1706.02677">https://arxiv.org/abs/1706.02677</a>, <a class="reference external" href="https://doi.org/10.48550/ARXIV.1706.02677">doi:10.48550/ARXIV.1706.02677</a>.</p>
</div>
<div class="citation" id="id98" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GZR21<span class="fn-bracket">]</span></span>
<p>Diego Granziol, Stefan Zohren, and Stephen Roberts. Learning rates as a function of batch size: a random matrix theory approach to neural network training. 2021. <a class="reference external" href="https://arxiv.org/abs/2006.09092">arXiv:2006.09092</a>.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HZRS15<span class="fn-bracket">]</span></span>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. <em>CoRR</em>, 2015. URL: <a class="reference external" href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>, <a class="reference external" href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a>.</p>
</div>
<div class="citation" id="id74" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hin12<span class="fn-bracket">]</span></span>
<p>Geoffrey Hinton. Neural networks for machine learning: lecture 6a overview of mini-batch gradient descent. Lecture Slides, Coursera, 2012. Accessed: 2024-08-28. URL: <a class="reference external" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a>.</p>
</div>
<div class="citation" id="id88" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HS97a<span class="fn-bracket">]</span></span>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. <em>Neural Computation</em>, 9(1):1–42, 1997. <a class="reference external" href="https://doi.org/10.1162/neco.1997.9.1.1">doi:10.1162/neco.1997.9.1.1</a>.</p>
</div>
<div class="citation" id="id118" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HS97b<span class="fn-bracket">]</span></span>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. <em>Neural Computation</em>, 9(8):1735–1780, 1997. <a class="reference external" href="https://doi.org/10.1162/neco.1997.9.8.1735">doi:10.1162/neco.1997.9.8.1735</a>.</p>
</div>
<div class="citation" id="id100" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HLP+17<span class="fn-bracket">]</span></span>
<p>Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. Snapshot ensembles: train 1, get M for free. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1704.00109">http://arxiv.org/abs/1704.00109</a>, <a class="reference external" href="https://arxiv.org/abs/1704.00109">arXiv:1704.00109</a>.</p>
</div>
<div class="citation" id="id28" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>IS15<span class="fn-bracket">]</span></span>
<p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. <em>CoRR</em>, 2015. URL: <a class="reference external" href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</a>, <a class="reference external" href="https://arxiv.org/abs/1502.03167">arXiv:1502.03167</a>.</p>
</div>
<div class="citation" id="id86" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KMN+16a<span class="fn-bracket">]</span></span>
<p>Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: generalization gap and sharp minima. <em>CoRR</em>, 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1609.04836">http://arxiv.org/abs/1609.04836</a>, <a class="reference external" href="https://arxiv.org/abs/1609.04836">arXiv:1609.04836</a>.</p>
</div>
<div class="citation" id="id95" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KMN+16b<span class="fn-bracket">]</span></span>
<p>Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: generalization gap and sharp minima. <em>CoRR</em>, 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1609.04836">http://arxiv.org/abs/1609.04836</a>, <a class="reference external" href="https://arxiv.org/abs/1609.04836">arXiv:1609.04836</a>.</p>
</div>
<div class="citation" id="id46" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KB15<span class="fn-bracket">]</span></span>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, <em>3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>. 2015. URL: <a class="reference external" href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a>.</p>
</div>
<div class="citation" id="id96" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KUMH17<span class="fn-bracket">]</span></span>
<p>Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1706.02515">http://arxiv.org/abs/1706.02515</a>, <a class="reference external" href="https://arxiv.org/abs/1706.02515">arXiv:1706.02515</a>.</p>
</div>
<div class="citation" id="id80" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KSH12<span class="fn-bracket">]</span></span>
<p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, <em>Advances in Neural Information Processing Systems 25</em>, pages 1097–1105. Curran Associates, Inc., 2012. URL: <a class="reference external" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.</p>
</div>
<div class="citation" id="id79" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LBBH98<span class="fn-bracket">]</span></span>
<p>Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In <em>Proceedings of the IEEE</em>, volume 86, 2278–2324. 1998. URL: <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665</a>.</p>
</div>
<div class="citation" id="id90" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LXTG17<span class="fn-bracket">]</span></span>
<p>Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1712.09913">http://arxiv.org/abs/1712.09913</a>, <a class="reference external" href="https://arxiv.org/abs/1712.09913">arXiv:1712.09913</a>.</p>
</div>
<div class="citation" id="id115" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LLH+19<span class="fn-bracket">]</span></span>
<p>Zehui Lin, Pengfei Liu, Luyao Huang, Junkun Chen, Xipeng Qiu, and Xuanjing Huang. Dropattention: A regularization method for fully-connected self-attention networks. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1907.11065">http://arxiv.org/abs/1907.11065</a>, <a class="reference external" href="https://arxiv.org/abs/1907.11065">arXiv:1907.11065</a>.</p>
</div>
<div class="citation" id="id102" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LH16<span class="fn-bracket">]</span></span>
<p>Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. <em>CoRR</em>, 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1608.03983">http://arxiv.org/abs/1608.03983</a>, <a class="reference external" href="https://arxiv.org/abs/1608.03983">arXiv:1608.03983</a>.</p>
</div>
<div class="citation" id="id101" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LH17<span class="fn-bracket">]</span></span>
<p>Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1711.05101">http://arxiv.org/abs/1711.05101</a>, <a class="reference external" href="https://arxiv.org/abs/1711.05101">arXiv:1711.05101</a>.</p>
</div>
<div class="citation" id="id99" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ML18<span class="fn-bracket">]</span></span>
<p>Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1804.07612">http://arxiv.org/abs/1804.07612</a>, <a class="reference external" href="https://arxiv.org/abs/1804.07612">arXiv:1804.07612</a>.</p>
</div>
<div class="citation" id="id85" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NKB+19<span class="fn-bracket">]</span></span>
<p>Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: where bigger models and more data hurt. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1912.02292">http://arxiv.org/abs/1912.02292</a>, <a class="reference external" href="https://arxiv.org/abs/1912.02292">arXiv:1912.02292</a>.</p>
</div>
<div class="citation" id="id114" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PSL22<span class="fn-bracket">]</span></span>
<p>Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: attention with linear biases enables input length extrapolation. 2022. <a class="reference external" href="https://arxiv.org/abs/2108.12409">arXiv:2108.12409</a>.</p>
</div>
<div class="citation" id="id10" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RLMD22<span class="fn-bracket">]</span></span>
<p>S. Raschka, Y. Liu, V. Mirjalili, and D. Dzhulgakov. <em>Machine Learning with PyTorch and Scikit-Learn: Develop Machine Learning and Deep Learning Models with Python</em>. Expert insight. Packt Publishing, 2022. ISBN 9781801819312. URL: <a class="reference external" href="https://books.google.com.ph/books?id=UHbNzgEACAAJ">https://books.google.com.ph/books?id=UHbNzgEACAAJ</a>.</p>
</div>
<div class="citation" id="id105" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RSW+17<span class="fn-bracket">]</span></span>
<p>Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam, and Christopher Ré. Data programming: creating large training sets, quickly. 2017. <a class="reference external" href="https://arxiv.org/abs/1605.07723">arXiv:1605.07723</a>.</p>
</div>
<div class="citation" id="id119" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SP97<span class="fn-bracket">]</span></span>
<p>M. Schuster and K.K. Paliwal. Bidirectional recurrent neural networks. <em>IEEE Transactions on Signal Processing</em>, 45(11):2673–2681, 1997. <a class="reference external" href="https://doi.org/10.1109/78.650093">doi:10.1109/78.650093</a>.</p>
</div>
<div class="citation" id="id78" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SZ14<span class="fn-bracket">]</span></span>
<p>Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>.</p>
</div>
<div class="citation" id="id103" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ST17<span class="fn-bracket">]</span></span>
<p>Leslie N. Smith and Nicholay Topin. Super-convergence: very fast training of residual networks using large learning rates. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1708.07120">http://arxiv.org/abs/1708.07120</a>, <a class="reference external" href="https://arxiv.org/abs/1708.07120">arXiv:1708.07120</a>.</p>
</div>
<div class="citation" id="id67" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SDBR14<span class="fn-bracket">]</span></span>
<p>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: the all convolutional net. 2014. URL: <a class="reference external" href="https://arxiv.org/abs/1412.6806">https://arxiv.org/abs/1412.6806</a>, <a class="reference external" href="https://doi.org/10.48550/ARXIV.1412.6806">doi:10.48550/ARXIV.1412.6806</a>.</p>
</div>
<div class="citation" id="id65" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SHK+14<span class="fn-bracket">]</span></span>
<p>Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. <em>Journal of Machine Learning Research</em>, 15(1):1929–1958, 2014. URL: <a class="reference external" href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf</a>.</p>
</div>
<div class="citation" id="id106" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>vdODZ+16<span class="fn-bracket">]</span></span>
<p>Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. <em>CoRR</em>, 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1609.03499">http://arxiv.org/abs/1609.03499</a>, <a class="reference external" href="https://arxiv.org/abs/1609.03499">arXiv:1609.03499</a>.</p>
</div>
<div class="citation" id="id111" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>VSP+17<span class="fn-bracket">]</span></span>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>, <a class="reference external" href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a>.</p>
</div>
<div class="citation" id="id116" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>XYH+20<span class="fn-bracket">]</span></span>
<p>Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. <em>CoRR</em>, 2020. URL: <a class="reference external" href="https://arxiv.org/abs/2002.04745">https://arxiv.org/abs/2002.04745</a>, <a class="reference external" href="https://arxiv.org/abs/2002.04745">arXiv:2002.04745</a>.</p>
</div>
<div class="citation" id="id94" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZFM+20<span class="fn-bracket">]</span></span>
<p>Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, and E. Weinan. Towards theoretically understanding why sgd generalizes better than adam in deep learning. In <em>Proceedings of the 34th International Conference on Neural Information Processing Systems</em>, NIPS'20. Red Hook, NY, USA, 2020. Curran Associates Inc.</p>
</div>
</div>
</div>
</div>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nb/dl/01-intro/01-intro.html" class="btn btn-neutral float-right" title="Introduction to Neural Networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>