
@InProceedings{pmlr-v32-hutter14,
  title = 	 {An Efficient Approach for Assessing Hyperparameter Importance},
  author = 	 {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {754--762},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/hutter14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/hutter14.html},
  abstract = 	 {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that—even in very high-dimensional cases—most performance variation is attributable to just a few hyperparameters.}
}

@misc{hansen2016cma,
      title={The CMA Evolution Strategy: A Tutorial}, 
      author={Nikolaus Hansen},
      year={2016},
      eprint={1604.00772},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{akiba2019optuna,
      title={Optuna: A Next-generation Hyperparameter Optimization Framework}, 
      author={Takuya Akiba and Shotaro Sano and Toshihiko Yanase and Takeru Ohta and Masanori Koyama},
      year={2019},
      eprint={1907.10902},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{geron2019hands-on,
    added-at = {2018-04-06T05:58:31.000+0200},
    address = {Sebastopol, CA},
    author = {Géron, Aurélien},
    biburl = {https://www.bibsonomy.org/bibtex/2a91270a3a516f4edaa5d459c40317fcc/achakraborty},
    interhash = {e2bd4a803c6cba6cca1d926b393806ad},
    intrahash = {a91270a3a516f4edaa5d459c40317fcc},
    isbn = {978-1491962299},
    keywords = {2019 book machine-learning oreilly tensorflow textbook},
    publisher = {O'Reilly Media},
    timestamp = {2018-04-06T05:59:31.000+0200},
    title = {Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems, Second Edition},
    year = {2019}
}

@misc{stat451,
  author = {Sebastian Raschka},
  title = {{STAT 451 -- Introduction to Machine Learning and Statistical Pattern Classification (Fall 2020)}},
  url = {https://github.com/rasbt/stat451-machine-learning-fs20/},
  year = {2020},
}

@book{AAAMLP,
    author = {Thakur, Abishek},
    isbn = {B089P13QHT},
    publisher = {Abhishek Thakur},
    title = {Approaching (Almost) Any Machine Learning Problem},
    year = {2020}
}

@book{bergstra,
    author = {J. S. Bergstra and R. Bardenet and Y. Bengio and B. Kégl},
    publisher = {Advances in Neural Information Processing Systems 24},
    title = {Algorithms for Hyper-Parameter Optimization},
    year = {2011}
}

@book{James2021,
  added-at = {2019-10-12T20:03:56.000+0200},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/2444186c86d18bddb4433c12fa126f6be/lopusz_kdd},
  interhash = {b3febabdc45a8629023cee7323dfbd86},
  intrahash = {444186c86d18bddb4433c12fa126f6be},
  keywords = {general_machine_learning},
  publisher = {Springer},
  timestamp = {2019-10-12T23:45:37.000+0200},
  title = {An Introduction to Statistical Learning: with Applications in R. Second Edition},
  url = {https://www.statlearning.com/},
  year = {2021}
}

@book{Wolpert1996,
  added-at = {2019-10-12T20:03:56.000+0200},
  author = {David H. Wolpert},
  publisher = {Neural Computation. October 1, 1996, Vol. 8, No. 7, Pages 1341-1390},
  timestamp = {2019-10-12T23:45:37.000+0200},
  title = {The Lack of A Priori Distinctions Between Learning Algorithms},
  year = {1996}
}

@inproceedings{holdgraf_evidence_2014,
    address = {Brisbane, Australia, Australia},
    title = {Evidence for {Predictive} {Coding} in {Human} {Auditory} {Cortex}},
    booktitle = {International {Conference} on {Cognitive} {Neuroscience}},
    publisher = {Frontiers in Neuroscience},
    author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Knight, Robert T.},
    year = {2014}
}

@article{holdgraf_rapid_2016,
    title = {Rapid tuning shifts in human auditory cortex enhance speech intelligibility},
    volume = {7},
    issn = {2041-1723},
    url = {http://www.nature.com/doifinder/10.1038/ncomms13654},
    doi = {10.1038/ncomms13654},
    number = {May},
    journal = {Nature Communications},
    author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Rieger, Jochem W. and Crone, Nathan and Lin, Jack J. and Knight, Robert T. and Theunissen, Frédéric E.},
    year = {2016},
    pages = {13654},
    file = {Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:C\:\\Users\\chold\\Zotero\\storage\\MDQP3JWE\\Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:application/pdf}
}

@inproceedings{holdgraf_portable_2017,
    title = {Portable learning environments for hands-on computational instruction using container-and cloud-based technology to teach data science},
    volume = {Part F1287},
    isbn = {978-1-4503-5272-7},
    doi = {10.1145/3093338.3093370},
    abstract = {© 2017 ACM. There is an increasing interest in learning outside of the traditional classroom setting. This is especially true for topics covering computational tools and data science, as both are challenging to incorporate in the standard curriculum. These atypical learning environments offer new opportunities for teaching, particularly when it comes to combining conceptual knowledge with hands-on experience/expertise with methods and skills. Advances in cloud computing and containerized environments provide an attractive opportunity to improve the effciency and ease with which students can learn. This manuscript details recent advances towards using commonly-Available cloud computing services and advanced cyberinfrastructure support for improving the learning experience in bootcamp-style events. We cover the benets (and challenges) of using a server hosted remotely instead of relying on student laptops, discuss the technology that was used in order to make this possible, and give suggestions for how others could implement and improve upon this model for pedagogy and reproducibility.},
    booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
    author = {Holdgraf, Christopher Ramsay and Culich, A. and Rokem, A. and Deniz, F. and Alegro, M. and Ushizima, D.},
    year = {2017},
    keywords = {Teaching, Bootcamps, Cloud computing, Data science, Docker, Pedagogy}
}

@article{holdgraf_encoding_2017,
    title = {Encoding and decoding models in cognitive electrophysiology},
    volume = {11},
    issn = {16625137},
    doi = {10.3389/fnsys.2017.00061},
    abstract = {© 2017 Holdgraf, Rieger, Micheli, Martin, Knight and Theunissen. Cognitive neuroscience has seen rapid growth in the size and complexity of data recorded from the human brain as well as in the computational tools available to analyze this data. This data explosion has resulted in an increased use of multivariate, model-based methods for asking neuroscience questions, allowing scientists to investigate multiple hypotheses with a single dataset, to use complex, time-varying stimuli, and to study the human brain under more naturalistic conditions. These tools come in the form of “Encoding” models, in which stimulus features are used to model brain activity, and “Decoding” models, in which neural features are used to generated a stimulus output. Here we review the current state of encoding and decoding models in cognitive electrophysiology and provide a practical guide toward conducting experiments and analyses in this emerging field. Our examples focus on using linear models in the study of human language and audition. We show how to calculate auditory receptive fields from natural sounds as well as how to decode neural recordings to predict speech. The paper aims to be a useful tutorial to these approaches, and a practical introduction to using machine learning and applied statistics to build models of neural activity. The data analytic approaches we discuss may also be applied to other sensory modalities, motor systems, and cognitive systems, and we cover some examples in these areas. In addition, a collection of Jupyter notebooks is publicly available as a complement to the material covered in this paper, providing code examples and tutorials for predictive modeling in python. The aimis to provide a practical understanding of predictivemodeling of human brain data and to propose best-practices in conducting these analyses.},
    journal = {Frontiers in Systems Neuroscience},
    author = {Holdgraf, Christopher Ramsay and Rieger, J.W. and Micheli, C. and Martin, S. and Knight, R.T. and Theunissen, F.E.},
    year = {2017},
    keywords = {Decoding models, Encoding models, Electrocorticography (ECoG), Electrophysiology/evoked potentials, Machine learning applied to neuroscience, Natural stimuli, Predictive modeling, Tutorials}
}

@book{ruby,
  title     = {The Ruby Programming Language},
  author    = {Flanagan, David and Matsumoto, Yukihiro},
  year      = {2008},
  publisher = {O'Reilly Media}
}
