
@InProceedings{pmlr-v32-hutter14,
  title = 	 {An Efficient Approach for Assessing Hyperparameter Importance},
  author = 	 {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {754--762},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/hutter14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/hutter14.html},
  abstract = 	 {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that—even in very high-dimensional cases—most performance variation is attributable to just a few hyperparameters.}
}

@misc{hansen2016cma,
      title={The CMA Evolution Strategy: A Tutorial}, 
      author={Nikolaus Hansen},
      year={2016},
      eprint={1604.00772},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{Minsky1969,
    author = {Minsky, M. Papert, S.},
    publisher = {The MIT Press},
    title = {Perceptron: an introduction to computational geometry},
    year = {1969}
}

@misc{akiba2019optuna,
      title={Optuna: A Next-generation Hyperparameter Optimization Framework}, 
      author={Takuya Akiba and Shotaro Sano and Toshihiko Yanase and Takeru Ohta and Masanori Koyama},
      year={2019},
      eprint={1907.10902},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{geron2019hands-on,
    added-at = {2018-04-06T05:58:31.000+0200},
    address = {Sebastopol, CA},
    author = {Géron, Aurélien},
    biburl = {https://www.bibsonomy.org/bibtex/2a91270a3a516f4edaa5d459c40317fcc/achakraborty},
    interhash = {e2bd4a803c6cba6cca1d926b393806ad},
    intrahash = {a91270a3a516f4edaa5d459c40317fcc},
    isbn = {978-1491962299},
    keywords = {2019 book machine-learning oreilly tensorflow textbook},
    publisher = {O'Reilly Media},
    timestamp = {2018-04-06T05:59:31.000+0200},
    title = {Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems, Second Edition},
    year = {2019}
}

@book{RaschkaMirjalili2019,  
address = {Birmingham, UK},  
author = {Raschka, Sebastian and Mirjalili, Vahid},  
edition = {3rd},  
isbn = {978-1789955750},   
publisher = {Packt Publishing},  
title = {{Python Machine Learning, 3rd Ed.}},  
year = {2019}  
}

@book{raschka2022,
  title={Machine Learning with PyTorch and Scikit-Learn: Develop Machine Learning and Deep Learning Models with Python},
  author={Raschka, S. and Liu, Y. and Mirjalili, V. and Dzhulgakov, D.},
  isbn={9781801819312},
  series={Expert insight},
  url={https://books.google.com.ph/books?id=UHbNzgEACAAJ},
  year={2022},
  publisher={Packt Publishing}
}


@book{Voron2021,  
author = {François Voron},  
edition = {1st},
isbn = {978-1801079211},   
publisher = {Packt Publishing},  
title = {{Building Data Science Applications with FastAPI}},  
year = {2021}  
}

@misc{stat451,
  author = {Sebastian Raschka},
  title = {{STAT 451 -- Introduction to Machine Learning and Statistical Pattern Classification (Fall 2020)}},
  url = {https://github.com/rasbt/stat451-machine-learning-fs20/},
  year = {2020},
}

@book{AAAMLP,
    author = {Thakur, Abishek},
    isbn = {B089P13QHT},
    publisher = {Abhishek Thakur},
    title = {Approaching (Almost) Any Machine Learning Problem},
    year = {2020}
}

@book{bergstra,
    author = {J. S. Bergstra and R. Bardenet and Y. Bengio and B. Kégl},
    publisher = {Advances in Neural Information Processing Systems 24},
    title = {Algorithms for Hyper-Parameter Optimization},
    year = {2011}
}

@book{James2021,
  added-at = {2019-10-12T20:03:56.000+0200},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/2444186c86d18bddb4433c12fa126f6be/lopusz_kdd},
  interhash = {b3febabdc45a8629023cee7323dfbd86},
  intrahash = {444186c86d18bddb4433c12fa126f6be},
  keywords = {general_machine_learning},
  publisher = {Springer},
  timestamp = {2019-10-12T23:45:37.000+0200},
  title = {An Introduction to Statistical Learning: with Applications in R. Second Edition},
  url = {https://www.statlearning.com/},
  year = {2021}
}

@book{Wolpert1996,
  added-at = {2019-10-12T20:03:56.000+0200},
  author = {David H. Wolpert},
  publisher = {Neural Computation. October 1, 1996, Vol. 8, No. 7, Pages 1341-1390},
  timestamp = {2019-10-12T23:45:37.000+0200},
  title = {The Lack of A Priori Distinctions Between Learning Algorithms},
  year = {1996}
}

@inproceedings{holdgraf_evidence_2014,
    address = {Brisbane, Australia, Australia},
    title = {Evidence for {Predictive} {Coding} in {Human} {Auditory} {Cortex}},
    booktitle = {International {Conference} on {Cognitive} {Neuroscience}},
    publisher = {Frontiers in Neuroscience},
    author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Knight, Robert T.},
    year = {2014}
}

@article{holdgraf_rapid_2016,
    title = {Rapid tuning shifts in human auditory cortex enhance speech intelligibility},
    volume = {7},
    issn = {2041-1723},
    url = {http://www.nature.com/doifinder/10.1038/ncomms13654},
    doi = {10.1038/ncomms13654},
    number = {May},
    journal = {Nature Communications},
    author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Rieger, Jochem W. and Crone, Nathan and Lin, Jack J. and Knight, Robert T. and Theunissen, Frédéric E.},
    year = {2016},
    pages = {13654},
    file = {Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:C\:\\Users\\chold\\Zotero\\storage\\MDQP3JWE\\Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:application/pdf}
}

@inproceedings{holdgraf_portable_2017,
    title = {Portable learning environments for hands-on computational instruction using container-and cloud-based technology to teach data science},
    volume = {Part F1287},
    isbn = {978-1-4503-5272-7},
    doi = {10.1145/3093338.3093370},
    abstract = {© 2017 ACM. There is an increasing interest in learning outside of the traditional classroom setting. This is especially true for topics covering computational tools and data science, as both are challenging to incorporate in the standard curriculum. These atypical learning environments offer new opportunities for teaching, particularly when it comes to combining conceptual knowledge with hands-on experience/expertise with methods and skills. Advances in cloud computing and containerized environments provide an attractive opportunity to improve the effciency and ease with which students can learn. This manuscript details recent advances towards using commonly-Available cloud computing services and advanced cyberinfrastructure support for improving the learning experience in bootcamp-style events. We cover the benets (and challenges) of using a server hosted remotely instead of relying on student laptops, discuss the technology that was used in order to make this possible, and give suggestions for how others could implement and improve upon this model for pedagogy and reproducibility.},
    booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
    author = {Holdgraf, Christopher Ramsay and Culich, A. and Rokem, A. and Deniz, F. and Alegro, M. and Ushizima, D.},
    year = {2017},
    keywords = {Teaching, Bootcamps, Cloud computing, Data science, Docker, Pedagogy}
}

@article{holdgraf_encoding_2017,
    title = {Encoding and decoding models in cognitive electrophysiology},
    volume = {11},
    issn = {16625137},
    doi = {10.3389/fnsys.2017.00061},
    abstract = {© 2017 Holdgraf, Rieger, Micheli, Martin, Knight and Theunissen. Cognitive neuroscience has seen rapid growth in the size and complexity of data recorded from the human brain as well as in the computational tools available to analyze this data. This data explosion has resulted in an increased use of multivariate, model-based methods for asking neuroscience questions, allowing scientists to investigate multiple hypotheses with a single dataset, to use complex, time-varying stimuli, and to study the human brain under more naturalistic conditions. These tools come in the form of “Encoding” models, in which stimulus features are used to model brain activity, and “Decoding” models, in which neural features are used to generated a stimulus output. Here we review the current state of encoding and decoding models in cognitive electrophysiology and provide a practical guide toward conducting experiments and analyses in this emerging field. Our examples focus on using linear models in the study of human language and audition. We show how to calculate auditory receptive fields from natural sounds as well as how to decode neural recordings to predict speech. The paper aims to be a useful tutorial to these approaches, and a practical introduction to using machine learning and applied statistics to build models of neural activity. The data analytic approaches we discuss may also be applied to other sensory modalities, motor systems, and cognitive systems, and we cover some examples in these areas. In addition, a collection of Jupyter notebooks is publicly available as a complement to the material covered in this paper, providing code examples and tutorials for predictive modeling in python. The aimis to provide a practical understanding of predictivemodeling of human brain data and to propose best-practices in conducting these analyses.},
    journal = {Frontiers in Systems Neuroscience},
    author = {Holdgraf, Christopher Ramsay and Rieger, J.W. and Micheli, C. and Martin, S. and Knight, R.T. and Theunissen, F.E.},
    year = {2017},
    keywords = {Decoding models, Encoding models, Electrocorticography (ECoG), Electrophysiology/evoked potentials, Machine learning applied to neuroscience, Natural stimuli, Predictive modeling, Tutorials}
}

@book{ruby,
  title     = {The Ruby Programming Language},
  author    = {Flanagan, David and Matsumoto, Yukihiro},
  year      = {2008},
  publisher = {O'Reilly Media}
}

@INPROCEEDINGS{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}
}

@incollection{imagenet-paper,
  added-at = {2016-11-14T12:05:24.000+0100},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  biburl = {https://www.bibsonomy.org/bibtex/2886c491fe45049fee3c9660df30bb5c4/albinzehe},
  booktitle = {Advances in Neural Information Processing Systems 25},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  interhash = {74bbb5dea5afb1b088bd10e317f1f0d2},
  intrahash = {886c491fe45049fee3c9660df30bb5c4},
  keywords = {cnn deeplearning ma-zehe neuralnet},
  pages = {1097--1105},
  publisher = {Curran Associates, Inc.},
  timestamp = {2016-11-14T12:05:24.000+0100},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  year = 2012
}

@article{mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{fashion-mnist,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
               Algorithms},
  journal   = {CoRR},
  volume    = {abs/1708.07747},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07747},
  eprinttype = {arXiv},
  eprint    = {1708.07747},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-07747.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{celeb_a,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015} 
}

@book{mlbook2022,  
address = {Birmingham, UK},  
author = {Sebastian Raschka, and Yuxi Liu, and Vahid Mirjalili},  
isbn = {978-1801819312},   
publisher = {Packt Publishing},  
title = {{Machine Learning with PyTorch and Scikit-Learn}},  
year = {2022}  
}

@article{batchnorm,
  author    = {Sergey Ioffe and
               Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing
               Internal Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/1502.03167},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03167},
  eprinttype = {arXiv},
  eprint    = {1502.03167},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/IoffeS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mobilenet,
  author    = {Mark Sandler and
               Andrew G. Howard and
               Menglong Zhu and
               Andrey Zhmoginov and
               Liang{-}Chieh Chen},
  title     = {Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification,
               Detection and Segmentation},
  journal   = {CoRR},
  volume    = {abs/1801.04381},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.04381},
  eprinttype = {arXiv},
  eprint    = {1801.04381},
  timestamp = {Tue, 12 Jan 2021 15:30:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-04381.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{resnet,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  eprinttype = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@book{keras2,
    author = {Chollet, François},
    isbn = {9781617296864},
    publisher = {Manning},
    title = {Deep Learning with Python, Second Edition},
    year = {2021}
}

@misc{cams,
  doi = {10.48550/ARXIV.1512.04150},
  url = {https://arxiv.org/abs/1512.04150},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning Deep Features for Discriminative Localization},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{10.1371/journal.pone.0237978,
    doi = {10.1371/journal.pone.0237978},
    author = {Brackbill, Devon AND Centola, Damon},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Impact of network structure on collective learning: An experimental study in a data science competition},
    year = {2020},
    month = {09},
    volume = {15},
    url = {https://doi.org/10.1371/journal.pone.0237978},
    pages = {1-13},
    abstract = {Do efficient communication networks accelerate solution discovery? The most prominent theory of organizational design for collective learning maintains that informationally efficient collaboration networks increase a group’s ability to find innovative solutions to complex problems. We test this idea against a competing theory that argues that communication networks that are less efficient for information transfer will increase the discovery of novel solutions to complex problems. We conducted a series of experimentally designed Data Science Competitions, in which we manipulated the efficiency of the communication networks among distributed groups of data scientists attempting to find better solutions for complex statistical modeling problems. We present findings from 16 independent competitions, where individuals conduct greedy search and only adopt better solutions. We show that groups with inefficient communication networks consistently discovered better solutions. In every experimental trial, groups with inefficient networks outperformed groups with efficient networks, as measured by both the group’s average solution quality and the best solution found by a group member.},
    number = {9},

}


@article{DBLP:journals/corr/KeskarMNST16,
  author    = {Nitish Shirish Keskar and
               Dheevatsa Mudigere and
               Jorge Nocedal and
               Mikhail Smelyanskiy and
               Ping Tak Peter Tang},
  title     = {On Large-Batch Training for Deep Learning: Generalization Gap and
               Sharp Minima},
  journal   = {CoRR},
  volume    = {abs/1609.04836},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.04836},
  eprinttype = {arXiv},
  eprint    = {1609.04836},
  timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KeskarMNST16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.1705.08292,
  doi = {10.48550/ARXIV.1705.08292},
  url = {https://arxiv.org/abs/1705.08292},
  author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The Marginal Value of Adaptive Gradient Methods in Machine Learning},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DBLP:journals/corr/Ruder16,
  author    = {Sebastian Ruder},
  title     = {An overview of gradient descent optimization algorithms},
  journal   = {CoRR},
  volume    = {abs/1609.04747},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.04747},
  eprinttype = {arXiv},
  eprint    = {1609.04747},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Ruder16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{groupnorm,
  doi = {10.48550/ARXIV.1803.08494},
  url = {https://arxiv.org/abs/1803.08494},
  author = {Wu, Yuxin and He, Kaiming},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Group Normalization},
  publisher = {arXiv},  
  year = {2018},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{layernorm,
  doi = {10.48550/ARXIV.1607.06450},  
  url = {https://arxiv.org/abs/1607.06450},  
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {Layer Normalization},  
  publisher = {arXiv},  
  year = {2016},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{0483bd9444a348c8b59d54a190839ec9,
title = "Deep learning",
abstract = "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
author = "Yann Lecun and Yoshua Bengio and Geoffrey Hinton",
note = "Funding Information: Acknowledgements The authors would like to thank the Natural Sciences and Engineering Research Council of Canada, the Canadian Institute For Advanced Research (CIFAR), the National Science Foundation and Office of Naval Research for support. Y.L. and Y.B. are CIFAR fellows. Publisher Copyright: {\textcopyright} 2015 Macmillan Publishers Limited. All rights reserved.",
year = "2015",
month = may,
day = "27",
doi = "10.1038/nature14539",
language = "English (US)",
volume = "521",
pages = "436--444",
journal = "Nature Cell Biology",
issn = "1465-7392",
publisher = "Nature Publishing Group",
number = "7553",
}

@misc{arxiv.1712.09913,
  doi = {10.48550/ARXIV.1712.09913},
  url = {https://arxiv.org/abs/1712.09913},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Visualizing the Loss Landscape of Neural Nets},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{xavier,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{bengio2003a,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {1137–1155},
numpages = {19}
}

@misc{santurkar2019does,
      title={How Does Batch Normalization Help Optimization?}, 
      author={Shibani Santurkar and Dimitris Tsipras and Andrew Ilyas and Aleksander Madry},
      year={2019},
      eprint={1805.11604},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{daneshmand2020batch,
      title={Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks}, 
      author={Hadi Daneshmand and Jonas Kohler and Francis Bach and Thomas Hofmann and Aurelien Lucchi},
      year={2020},
      eprint={2003.01652},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{bnautotuning,
  author    = {Sanjeev Arora and
               Zhiyuan Li and
               Kaifeng Lyu},
  title     = {Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},
  journal   = {CoRR},
  volume    = {abs/1812.03981},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.03981},
  eprinttype = {arXiv},
  eprint    = {1812.03981},
  timestamp = {Mon, 25 Nov 2019 14:34:48 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-03981.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kaiming,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
               ImageNet Classification},
  journal   = {CoRR},
  volume    = {abs/1502.01852},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.01852},
  eprinttype = {arXiv},
  eprint    = {1502.01852},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZR015.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{densenet,
  author    = {Gao Huang and
               Zhuang Liu and
               Kilian Q. Weinberger},
  title     = {Densely Connected Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1608.06993},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.06993},
  eprinttype = {arXiv},
  eprint    = {1608.06993},
  timestamp = {Mon, 10 Sep 2018 15:49:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HuangLW16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{timviera,
  title = {Evaluating ∇f(x) is as fast as f(x)},
  month = {9},
  day = {25},
  year = {2016},
  organization = {timvieira.github.io},
  author = {Tim Vieira},
  url = {https://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/},
}

@misc{backprop-offconvex,
  title = {Back-propagation, an introduction},
  month = {12},
  day = {20},
  year = {2016},
  organization = {www.offconvex.org},
  author = {Sanjeev Arora and Tengyu Ma},
  url = {http://www.offconvex.org/2016/12/20/backprop/},
}

@misc{pytorch-autograd,
  title = {PyTorch Autograd Explained - In-depth Tutorial},
  month = {11},
  day = {3},
  year = {2018},
  organization = {https://www.youtube.com/c/elliotwaite},
  author = {Elliot Waite},
  url = {https://www.youtube.com/watch?v=MswxJw-8PvE},
}

@misc{micrograd,
  title = {The spelled-out intro to neural networks and backpropagation: building micrograd},
  month = {8},
  day = {17},
  year = {2022},
  organization = {https://www.youtube.com/c/AndrejKarpathy},
  author = {Andrej Karpathy},
  url = {https://www.youtube.com/watch?v=VMj-3S1tku0},
}

@misc{makemore1,
  title = {The spelled-out intro to language modeling: building makemore},
  month = {9},
  day = {8},
  year = {2022},
  organization = {https://www.youtube.com/c/AndrejKarpathy},
  author = {Andrej Karpathy},
  url = {https://www.youtube.com/watch?v=PaCmpygFfXo},
}

@misc{makemore2,
  title = {Building makemore Part 2: MLP},
  month = {9},
  day = {12},
  year = {2022},
  organization = {https://www.youtube.com/c/AndrejKarpathy},
  author = {Andrej Karpathy},
  url = {https://www.youtube.com/watch?v=TCH_1BHY58I},
}

@misc{makemore3,
  title = {Building makemore Part 3: Activations & Gradients, BatchNorm},
  month = {10},
  day = {5},
  year = {2022},
  organization = {https://www.youtube.com/c/AndrejKarpathy},
  author = {Andrej Karpathy},
  url = {https://www.youtube.com/watch?v=P6sfmUTpUmc},
}

@misc{uvadlc_activations,
  title = {Tutorial 3: Activation Functions},
  month = {8},
  day = {11},
  year = {2022},
  organization = {UvA Deep Learning Course},
  author = {Philip Lippe},
  url = {https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html},
}

@misc{uvadlc_optim,
  title = {Tutorial 4: Optimization and Initialization},
  month = {8},
  day = {11},
  year = {2022},
  organization = {UvA Deep Learning Course},
  author = {Philip Lippe},
  url = {https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html},
}

@inproceedings{leakyrelu,
  title={Rectifier Nonlinearities Improve Neural Network Acoustic Models},
  author={Andrew L. Maas},
  year={2013},
  booktitle={Proceedings of the 30th International Conference on Machine Learning},
  volume={28}
}

@inproceedings{elu,
  added-at = {2020-03-02T10:16:12.000+0100},
  author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
  biburl = {https://www.bibsonomy.org/bibtex/2d4f174f6f50dfc7e00b4a4b784462384/nosebrain},
  booktitle = {ICLR (Poster)},
  crossref = {conf/iclr/2016},
  editor = {Bengio, Yoshua and LeCun, Yann},
  ee = {http://arxiv.org/abs/1511.07289},
  interhash = {f71d17de01942c5cce2e18a8d48852cf},
  intrahash = {d4f174f6f50dfc7e00b4a4b784462384},
  keywords = {activation deep elu function learning},
  timestamp = {2020-03-02T10:16:12.000+0100},
  title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2016.html#ClevertUH15},
  year = 2016
}

@article{swish,
  author    = {Prajit Ramachandran and
               Barret Zoph and
               Quoc V. Le},
  title     = {Searching for Activation Functions},
  journal   = {CoRR},
  volume    = {abs/1710.05941},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.05941},
  eprinttype = {arXiv},
  eprint    = {1710.05941},
  timestamp = {Mon, 13 Aug 2018 16:48:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-05941.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{ganegedara2022tensorflow,
  title={TensorFlow in Action},
  author={Ganegedara, T.},
  isbn={9781617298349},
  url={https://books.google.com.ph/books?id=Hgh0zgEACAAJ},
  year={2022},
  publisher={Manning}
}

@article{weng2018VAE,
  title   = "From Autoencoder to Beta-VAE",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2018",
  url     = "https://lilianweng.github.io/posts/2018-08-12-vae/"
}

@article{d2l,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    journal={arXiv preprint arXiv:2106.11342},
    year={2021}
}

@article{Bengio2003,
  acmid = {944966},
  added-at = {2016-04-06T14:15:15.000+0200},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  biburl = {https://www.bibsonomy.org/bibtex/25bc1d3d1be6247dd6365014919a711ef/dallmann},
  interhash = {99a471df7eb1ed9d39f9e7cb08859ec9},
  intrahash = {5bc1d3d1be6247dd6365014919a711ef},
  issn = {1532-4435},
  issue_date = {3/1/2003},
  journal = {J. Mach. Learn. Res.},
  keywords = {deep_learning language_model thema thema:forward_neural_network},
  month = mar,
  numpages = {19},
  pages = {1137--1155},
  publisher = {JMLR.org},
  timestamp = {2016-04-07T10:12:54.000+0200},
  title = {A Neural Probabilistic Language Model},
  url = {http://dl.acm.org/citation.cfm?id=944919.944966},
  volume = 3,
  year = 2003
}

@article{dropout,
  added-at = {2017-06-12T16:47:02.000+0200},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  biburl = {https://www.bibsonomy.org/bibtex/24a1dac2f78b37c1f374a8228b0e272aa/franckyf},
  interhash = {bdad866eb5fd8994c2aeae46af6def20},
  intrahash = {4a1dac2f78b37c1f374a8228b0e272aa},
  journal = {Journal of Machine Learning Research},
  keywords = {},
  number = 1,
  pages = {1929-1958},
  timestamp = {2017-06-12T16:47:02.000+0200},
  title = {Dropout: a simple way to prevent neural networks from overfitting.},
  url = {http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf},
  volume = 15,
  year = 2014
}

@misc{rethinkingBN,
  doi = {10.48550/ARXIV.2105.07576},
  
  url = {https://arxiv.org/abs/2105.07576},
  
  author = {Wu, Yuxin and Johnson, Justin},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Rethinking "Batch" in BatchNorm},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{allconv,
  doi = {10.48550/ARXIV.1412.6806},
  
  url = {https://arxiv.org/abs/1412.6806},
  
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Striving for Simplicity: The All Convolutional Net},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{precisebn,
  author    = {Yuxin Wu and
               Justin Johnson},
  title     = {Rethinking "Batch" in BatchNorm},
  journal   = {CoRR},
  volume    = {abs/2105.07576},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.07576},
  eprinttype = {arXiv},
  eprint    = {2105.07576},
  timestamp = {Thu, 20 May 2021 16:18:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-07576.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{imagenet1hour,
  doi = {10.48550/ARXIV.1706.02677},
  
  url = {https://arxiv.org/abs/1706.02677},
  
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{gelu,
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. The GELU nonlinearity weights inputs by their magnitude, rather than gates inputs by their sign as in ReLUs. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  title  = {Gaussian Error Linear Units (GELUs)},
  url = {http://arxiv.org/abs/1606.08415v3},
  journal   = {arxiv},
  eprintclass = {cs.LG},
  eprinttype = {arXiv},
  year = 2016
}

@article{mish,
  author    = {Diganta Misra},
  title     = {Mish: {A} Self Regularized Non-Monotonic Neural Activation Function},
  journal   = {CoRR},
  volume    = {abs/1908.08681},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.08681},
  eprinttype = {arXiv},
  eprint    = {1908.08681},
  timestamp = {Sat, 23 Jan 2021 01:21:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08681.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{understandingbn,
  added-at = {2020-03-06T00:00:00.000+0100},
  author = {Bjorck, Johan and Gomes, Carla P. and Selman, Bart and Weinberger, Kilian Q.},
  biburl = {https://www.bibsonomy.org/bibtex/2d19d1f60b5caaea308c6e94effbb9877/dblp},
  booktitle = {NeurIPS},
  crossref = {conf/nips/2018},
  editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
  ee = {http://papers.nips.cc/paper/7996-understanding-batch-normalization},
  interhash = {a713395fa18636e7fc009cb47204ed51},
  intrahash = {d19d1f60b5caaea308c6e94effbb9877},
  keywords = {dblp},
  pages = {7705-7716},
  timestamp = {2020-03-07T11:50:35.000+0100},
  title = {Understanding Batch Normalization.},
  url = {http://dblp.uni-trier.de/db/conf/nips/nips2018.html#BjorckGSW18},
  year = 2018
}

@Techreport{cifar10,
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
 address = {Toronto, Ontario},
 institution = {University of Toronto},
 number = {0},
 publisher = {Technical report, University of Toronto},
 title = {Learning multiple layers of features from tiny images},
 year = {2009},
 title_with_no_special_chars = {Learning multiple layers of features from tiny images}
}

@inproceedings{batchnormrank,
  TITLE = {Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks},
  AUTHOR = {Daneshmand, Hadi and Kohler, Jonas and Bach, Francis and Hofmann, Thomas and Lucchi, Aurelien},
  URL = {https://hal.science/hal-03454386},
  BOOKTITLE = {NeurIPS 2020 - Thirty-fourth Conference on Neural Information Processing Systems},
  ADDRESS = {Virtual, France},
  YEAR = {2020},
  MONTH = Dec,
  PDF = {https://hal.science/hal-03454386/file/NeurIPS-2020-batch-normalization-provably-avoids-ranks-collapse-for-randomly-initialised-deep-networks-Paper%20%284%29.pdf},
  HAL_ID = {hal-03454386},
  HAL_VERSION = {v1},
}

@article{svdrandom,
author = {Dang-Zheng Liu and Dong Wang and Lun Zhang},
title = {{Bulk and soft-edge universality for singular values of products of Ginibre random matrices}},
volume = {52},
journal = {Annales de l'Institut Henri Poincaré, Probabilités et Statistiques},
number = {4},
publisher = {Institut Henri Poincaré},
pages = {1734 -- 1762},
keywords = {Airy kernel, Complex Ginibre matrix, Product of random matrices, Sine kernel, Truncated unitary matrix},
year = {2016},
doi = {10.1214/15-AIHP696},
URL = {https://doi.org/10.1214/15-AIHP696}
}

 @book{prince2023understanding,
 author = "Simon J.D. Prince",
 title = "Understanding Deep Learning",
 publisher = "MIT Press",
 year = 2023,
 url = "https://udlbook.github.io/udlbook/"
}

@article{vgg,
  added-at = {2016-11-19T13:14:27.000+0100},
  author = {Simonyan, Karen and Zisserman, Andrew},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {https://www.bibsonomy.org/bibtex/20ee0434e0a70b329d5518f43f1742f7a/albinzehe},
  interhash = {4e6fa56cb7cf99400d5701543ee228de},
  intrahash = {0ee0434e0a70b329d5518f43f1742f7a},
  journal = {CoRR},
  keywords = {cnn ma-zehe neuralnet},
  timestamp = {2016-11-19T13:14:27.000+0100},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  url = {http://arxiv.org/abs/1409.1556},
  volume = {abs/1409.1556},
  year = 2014
}

@inproceedings{lenet,
  added-at = {2010-06-28T21:14:36.000+0200},
  author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
  biburl = {https://www.bibsonomy.org/bibtex/29aa18bc67d862bdb83b6081e5506f050/mhwombat},
  booktitle = {Proceedings of the IEEE},
  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  file = {:neural_nets/lecun-98.pdf:PDF;:lecun-98.pdf:PDF},
  groups = {public},
  interhash = {7a82cccacd23cf06b25ff5325a6c86c7},
  intrahash = {9aa18bc67d862bdb83b6081e5506f050},
  keywords = {MSc character_recognition checked mnist network neural},
  number = 11,
  pages = {2278--2324},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {Gradient-Based Learning Applied to Document
                 Recognition},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  username = {mhwombat},
  volume = 86,
  year = 1998
}

@incollection{alexnet,
  added-at = {2016-11-14T12:05:24.000+0100},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  biburl = {https://www.bibsonomy.org/bibtex/2886c491fe45049fee3c9660df30bb5c4/albinzehe},
  booktitle = {Advances in Neural Information Processing Systems 25},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  interhash = {74bbb5dea5afb1b088bd10e317f1f0d2},
  intrahash = {886c491fe45049fee3c9660df30bb5c4},
  keywords = {cnn deeplearning ma-zehe neuralnet},
  pages = {1097--1105},
  publisher = {Curran Associates, Inc.},
  timestamp = {2016-11-14T12:05:24.000+0100},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  year = 2012
}


@book{durr2020competitive,
  author       = {Dürr, Christoph and Vie, Jill-Jênn and Gibbons, Greg and Gibbons, Danièle},
  title        = {Competitive Programming in Python: 128 Algorithms to Develop your Coding Skills},
  translator   = {Gibbons, Greg and Gibbons, Danièle},
  year         = {2020},
  publisher    = {Cambridge University Press},
  address      = {New York},
  edition      = {First},
  isbn         = {9781108716826},
  note         = {Includes bibliographical references and index},
  url          = {https://lccn.loc.gov/2020022774},
  keywords     = {Python, Algorithms},
  lccn         = {2020022774},
  language     = {english},
}

@book{Laaksonen20,
  author       = {Antti Laaksonen},
  title        = {Guide to Competitive Programming - Learning and Improving Algorithms
                  Through Contests, Second Edition},
  series       = {Undergraduate Topics in Computer Science},
  publisher    = {Springer},
  year         = {2020},
  url          = {https://doi.org/10.1007/978-3-030-39357-1},
  doi          = {10.1007/978-3-030-39357-1},
  isbn         = {978-3-030-39356-4},
  timestamp    = {Tue, 19 May 2020 12:08:25 +0200},
  biburl       = {https://dblp.org/rec/series/utcs/Laaksonen20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{double-descent,
  author       = {Preetum Nakkiran and
                  Gal Kaplun and
                  Yamini Bansal and
                  Tristan Yang and
                  Boaz Barak and
                  Ilya Sutskever},
  title        = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  journal      = {CoRR},
  volume       = {abs/1912.02292},
  year         = {2019},
  url          = {http://arxiv.org/abs/1912.02292},
  eprinttype    = {arXiv},
  eprint       = {1912.02292},
  timestamp    = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1912-02292.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{flat-minima,
  author       = {Nitish Shirish Keskar and
                  Dheevatsa Mudigere and
                  Jorge Nocedal and
                  Mikhail Smelyanskiy and
                  Ping Tak Peter Tang},
  title        = {On Large-Batch Training for Deep Learning: Generalization Gap and
                  Sharp Minima},
  journal      = {CoRR},
  volume       = {abs/1609.04836},
  year         = {2016},
  url          = {http://arxiv.org/abs/1609.04836},
  eprinttype    = {arXiv},
  eprint       = {1609.04836},
  timestamp    = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KeskarMNST16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{schmidhuber1997,
  title={Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability},
  author={Schmidhuber, Jürgen},
  journal={Neural Networks},
  volume={10},
  number={5},
  pages={857-873},
  year={1997},
  doi={10.1016/s0893-6080(96)00127-x},
  PMID={12662875}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1-42},
  year={1997},
  doi={10.1162/neco.1997.9.1.1},
  PMID={9117894}
}

@inproceedings{barron,
author = {Barron, A. R.},
title = {Approximation and Estimation Bounds for Artificial Neural Networks},
year = {1991},
isbn = {1558602135},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Fourth Annual Workshop on Computational Learning Theory},
pages = {243–249},
numpages = {7},
location = {Santa Cruz, California, USA},
series = {COLT '91}
}

@article{visualizing_loss,
  author       = {Hao Li and
                  Zheng Xu and
                  Gavin Taylor and
                  Tom Goldstein},
  title        = {Visualizing the Loss Landscape of Neural Nets},
  journal      = {CoRR},
  volume       = {abs/1712.09913},
  year         = {2017},
  url          = {http://arxiv.org/abs/1712.09913},
  eprinttype    = {arXiv},
  eprint       = {1712.09913},
  timestamp    = {Thu, 16 May 2019 13:19:49 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1712-09913.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v38-choromanska15,
  title = 	 {{The Loss Surfaces of Multilayer Networks}},
  author = 	 {Choromanska, Anna and Henaff, MIkael and Mathieu, Michael and Ben Arous, Gerard and LeCun, Yann},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {192--204},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/choromanska15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/choromanska15.html},
  abstract = 	 {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.}
}

@inproceedings{dauphin2014,
author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2933–2941},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@article{adagrad,
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
journal = {J. Mach. Learn. Res.},
month = {jul},
pages = {2121–2159},
numpages = {39}
}

@inproceedings{sgd_better_than_adam,
author = {Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven and Weinan, E.},
title = {Towards Theoretically Understanding Why SGD Generalizes Better than ADAM in Deep Learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {It is not clear yet why ADAM-alike adaptive gradient algorithms suffer from worse generalization performance than SGD despite their faster training speed. This work aims to provide understandings on this generalization gap by analyzing their local convergence behaviors. Specifically, we observe the heavy tails of gradient noise in these algorithms. This motivates us to analyze these algorithms through their L\'{e}vy-driven stochastic differential equations (SDEs) because of the similar convergence behaviors of an algorithm and its SDE. Then we establish the escaping time of these SDEs from a local basin. The result shows that (1) the escaping time of both SGD and ADAM depends on the Radon measure of the basin positively and the heaviness of gradient noise negatively; (2) for the same basin, SGD enjoys smaller escaping time than ADAM, mainly because (a) the geometry adaptation in ADAM via adaptively scaling each gradient coordinate well diminishes the anisotropic structure in gradient noise and results in larger Radon measure of a basin; (b) the exponential gradient average in ADAM smooths its gradient and leads to lighter gradient noise tails than SGD. So SGD is more locally unstable than ADAM at sharp minima defined as the minima whose local basins have small Radon measure, and can better escape from them to flatter ones with larger Radon measure. As flat minima here which often refer to the minima at flat or asymmetric basins/valleys often generalize better than sharp ones [1,2], our result explains the better generalization performance of SGD over ADAM. Finally, experimental results confirm our heavy-tailed gradient noise assumption and theoretical affirmation.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1787},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{sharp_minima_bad,
  author       = {Nitish Shirish Keskar and
                  Dheevatsa Mudigere and
                  Jorge Nocedal and
                  Mikhail Smelyanskiy and
                  Ping Tak Peter Tang},
  title        = {On Large-Batch Training for Deep Learning: Generalization Gap and
                  Sharp Minima},
  journal      = {CoRR},
  volume       = {abs/1609.04836},
  year         = {2016},
  url          = {http://arxiv.org/abs/1609.04836},
  eprinttype    = {arXiv},
  eprint       = {1609.04836},
  timestamp    = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KeskarMNST16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{selu,
  author       = {G{\"{u}}nter Klambauer and
                  Thomas Unterthiner and
                  Andreas Mayr and
                  Sepp Hochreiter},
  title        = {Self-Normalizing Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1706.02515},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.02515},
  eprinttype    = {arXiv},
  eprint       = {1706.02515},
  timestamp    = {Sat, 23 Jan 2021 01:19:44 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/KlambauerUMH17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gradcam,
  author       = {Ramprasaath R. Selvaraju and
                  Abhishek Das and
                  Ramakrishna Vedantam and
                  Michael Cogswell and
                  Devi Parikh and
                  Dhruv Batra},
  title        = {Grad-CAM: Why did you say that? Visual Explanations from Deep Networks
                  via Gradient-based Localization},
  journal      = {CoRR},
  volume       = {abs/1610.02391},
  year         = {2016},
  url          = {http://arxiv.org/abs/1610.02391},
  eprinttype    = {arXiv},
  eprint       = {1610.02391},
  timestamp    = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SelvarajuDVCPB16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{learning-rate-function-of-batch-size,
      title={Learning Rates as a Function of Batch Size: A Random Matrix Theory Approach to Neural Network Training}, 
      author={Diego Granziol and Stefan Zohren and Stephen Roberts},
      year={2021},
      eprint={2006.09092},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{batch-size-32,
  author       = {Dominic Masters and
                  Carlo Luschi},
  title        = {Revisiting Small Batch Training for Deep Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1804.07612},
  year         = {2018},
  url          = {http://arxiv.org/abs/1804.07612},
  eprinttype    = {arXiv},
  eprint       = {1804.07612},
  timestamp    = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1804-07612.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{snapshot-ensembles,
  author       = {Gao Huang and
                  Yixuan Li and
                  Geoff Pleiss and
                  Zhuang Liu and
                  John E. Hopcroft and
                  Kilian Q. Weinberger},
  title        = {Snapshot Ensembles: Train 1, get {M} for free},
  journal      = {CoRR},
  volume       = {abs/1704.00109},
  year         = {2017},
  url          = {http://arxiv.org/abs/1704.00109},
  eprinttype    = {arXiv},
  eprint       = {1704.00109},
  timestamp    = {Fri, 18 Nov 2022 15:40:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/HuangLPLHW17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{adamw,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Fixing Weight Decay Regularization in Adam},
  journal      = {CoRR},
  volume       = {abs/1711.05101},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.05101},
  eprinttype    = {arXiv},
  eprint       = {1711.05101},
  timestamp    = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{sgd-warm-restarts,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {{SGDR:} Stochastic Gradient Descent with Restarts},
  journal      = {CoRR},
  volume       = {abs/1608.03983},
  year         = {2016},
  url          = {http://arxiv.org/abs/1608.03983},
  eprinttype    = {arXiv},
  eprint       = {1608.03983},
  timestamp    = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/LoshchilovH16a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{super-convergence-resnet,
  author       = {Leslie N. Smith and
                  Nicholay Topin},
  title        = {Super-Convergence: Very Fast Training of Residual Networks Using Large
                  Learning Rates},
  journal      = {CoRR},
  volume       = {abs/1708.07120},
  year         = {2017},
  url          = {http://arxiv.org/abs/1708.07120},
  eprinttype    = {arXiv},
  eprint       = {1708.07120},
  timestamp    = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1708-07120.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{structure-gen-weak-supervision,
  author       = {Stephen H. Bach and
                  Bryan Dawei He and
                  Alexander Ratner and
                  Christopher R{\'{e}}},
  title        = {Learning the Structure of Generative Models without Labeled Data},
  journal      = {CoRR},
  volume       = {abs/1703.00854},
  year         = {2017},
  url          = {http://arxiv.org/abs/1703.00854},
  eprinttype    = {arXiv},
  eprint       = {1703.00854},
  timestamp    = {Mon, 13 Aug 2018 16:47:16 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BachHRR17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{data-programming,
      title={Data Programming: Creating Large Training Sets, Quickly}, 
      author={Alexander Ratner and Christopher De Sa and Sen Wu and Daniel Selsam and Christopher Ré},
      year={2017},
      eprint={1605.07723},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{wavenet,
  author       = {A{\"{a}}ron van den Oord and
                  Sander Dieleman and
                  Heiga Zen and
                  Karen Simonyan and
                  Oriol Vinyals and
                  Alex Graves and
                  Nal Kalchbrenner and
                  Andrew W. Senior and
                  Koray Kavukcuoglu},
  title        = {WaveNet: {A} Generative Model for Raw Audio},
  journal      = {CoRR},
  volume       = {abs/1609.03499},
  year         = {2016},
  url          = {http://arxiv.org/abs/1609.03499},
  eprinttype    = {arXiv},
  eprint       = {1609.03499},
  timestamp    = {Thu, 14 Oct 2021 09:15:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/OordDZSVGKSK16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{understanding-layernorm,
  author       = {Jingjing Xu and
                  Xu Sun and
                  Zhiyuan Zhang and
                  Guangxiang Zhao and
                  Junyang Lin},
  title        = {Understanding and Improving Layer Normalization},
  journal      = {CoRR},
  volume       = {abs/1911.07013},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.07013},
  eprinttype    = {arXiv},
  eprint       = {1911.07013},
  timestamp    = {Sat, 09 Sep 2023 00:04:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-07013.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{feng2022rank,
      title={Rank Diminishing in Deep Neural Networks}, 
      author={Ruili Feng and Kecheng Zheng and Yukun Huang and Deli Zhao and Michael Jordan and Zheng-Jun Zha},
      year={2022},
      eprint={2206.06072},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{dong2021,
  author       = {Yihe Dong and
                  Jean{-}Baptiste Cordonnier and
                  Andreas Loukas},
  title        = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially
                  with Depth},
  journal      = {CoRR},
  volume       = {abs/2103.03404},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.03404},
  eprinttype    = {arXiv},
  eprint       = {2103.03404},
  timestamp    = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-03404.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/VeitWB16,
  author       = {Andreas Veit and
                  Michael J. Wilber and
                  Serge J. Belongie},
  title        = {Residual Networks are Exponential Ensembles of Relatively Shallow
                  Networks},
  journal      = {CoRR},
  volume       = {abs/1605.06431},
  year         = {2016},
  url          = {http://arxiv.org/abs/1605.06431},
  eprinttype    = {arXiv},
  eprint       = {1605.06431},
  timestamp    = {Mon, 13 Aug 2018 16:47:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/VeitWB16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{transformers,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/HeZR016,
  author       = {Kaiming He and
                  Xiangyu Zhang and
                  Shaoqing Ren and
                  Jian Sun},
  title        = {Identity Mappings in Deep Residual Networks},
  journal      = {CoRR},
  volume       = {abs/1603.05027},
  year         = {2016},
  url          = {http://arxiv.org/abs/1603.05027},
  eprinttype    = {arXiv},
  eprint       = {1603.05027},
  timestamp    = {Wed, 25 Jan 2023 11:01:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/HeZR016.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{transformer-tears,
  doi = {10.5281/ZENODO.3525484},
  url = {https://zenodo.org/record/3525484},
  author = {Nguyen, Toan Q. and Salazar, Julian},
  language = {en},
  title = {Transformers without Tears: Improving the Normalization of Self-Attention},
  publisher = {Zenodo},
  journal = {},
  year = {2019},  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{alibi,
      title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}, 
      author={Ofir Press and Noah A. Smith and Mike Lewis},
      year={2022},
      eprint={2108.12409},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
