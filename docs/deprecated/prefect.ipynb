{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous module, we learned about experiment tracking and model registry.\n",
    "In particular, we discussed how to get a candidate model and promote it from staging to production.\n",
    "In this module, we learn how to automate this process, and having this scheduled with workflow orchestration using [Prefect 2.0](https://orion-docs.prefect.io/).\n",
    "Prefect allows us to orchestrate and observe workflows. One of the primary goals of Prefect is to minimize time spent on **negative engineering**, i.e. coding against all possible causes of failure, by having fault-tolerating data pipelines. Prefect provides tools for setting up pipelines using code, logging, scheduling, retries, caching, notifications, visualizing dependencies, as well as ad hoc execution and parametrization of scheduled tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep prefect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefect flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **flow** in Prefect is simply a Python function. This consists of **tasks** which can be thought of as a minimally observable unit of work. Flows can also be called inside flows as subflows. Consider the following example. Here we simulate getting data from an unreliable API, augmenting the fetched data, and writing the resulting data into a database. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[Getting Started with Prefect 2](https://www.prefect.io/guide/blog/getting-started-prefect-2/)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "from prefect import flow, task \n",
    "\n",
    "\n",
    "@task(retries=3, retry_delay_seconds=5)\n",
    "def call_unreliable_api():\n",
    "    choices = [{\"data\": 42}, {\"data\": -1}, {\"data\": 0}]\n",
    "    res = random.choice(choices)\n",
    "    if res[\"data\"] <= 0:\n",
    "        raise Exception(\"Our unreliable service failed.\")\n",
    "    else:\n",
    "        time.sleep(10)\n",
    "        return res\n",
    "\n",
    "@task\n",
    "def augment_data(data: dict, msg: str):\n",
    "    data[\"message\"] = msg\n",
    "    return data\n",
    "\n",
    "@task\n",
    "def write_to_database(data: dict):\n",
    "    print(f\"Wrote {data} to database successfully!\")\n",
    "    return \"Success!\"\n",
    "\n",
    "@flow(log_prints=True)\n",
    "def pipeline(msg: str):\n",
    "    api_result = call_unreliable_api()\n",
    "    augmented_data = augment_data(data=api_result, msg=msg)\n",
    "    write_to_database(augmented_data)\n",
    "\n",
    "\n",
    "pipeline(msg=\"0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefect Orion UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the server:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this failed before pushing through. We can start the UI by calling `prefect orion start` in any directory (`.prefect` is saved in the system's root directory). This starts the Prefect Orion server in port 4200."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ prefect orion start\n",
    "Starting...\n",
    "\n",
    " ___ ___ ___ ___ ___ ___ _____    ___  ___ ___ ___  _  _\n",
    "| _ \\ _ \\ __| __| __/ __|_   _|  / _ \\| _ \\_ _/ _ \\| \\| |\n",
    "|  _/   / _|| _|| _| (__  | |   | (_) |   /| | (_) | .` |\n",
    "|_| |_|_\\___|_| |___\\___| |_|    \\___/|_|_\\___\\___/|_|\\_|\n",
    "\n",
    "Configure Prefect to communicate with the server with:\n",
    "\n",
    "    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n",
    "\n",
    "Check out the dashboard at http://127.0.0.1:4200\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We navigate around the UI to find the `pipeline` flow and its most recent which, as we have seen in the logs, was able to complete its execution. Here we see that this flow started on `2022/06/10 11:05:51 PM` and ended on `2022/06/10 11:05:52 PM`. We also see the logs has the details of the exception when the API call failed. In the second tab, we can see the tasks that make up this flow. There is also the subflow tab which shows that we can call flows from a parent flow.\n",
    "\n",
    "```{figure} ../../../img/hello-world-2.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more interesting features of the dashboard is **Radar** on the right. This shows the dependence between tasks. Notice the linear dependence of the tasks, e.g. `write_to_database` depends on `augment_data` task but not on `call_unreliable_api`. Hovering on the tasks show the backward and forward data dependencies. Having tasks arranged in concentric circles allow for a heirarchy of dependence. Note that the runtime for each task is also conveniently displayed.\n",
    "\n",
    "```{figure} ../../../img/hello-world-1.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "Finally, let us look at a flow which failed to complete all its tasks. Here all calls to the API failed despite the retries. The radar plot nicely shows where the flow has failed. This is really useful, especially when we have a dozens task and multiple subflows happening in our data pipeline.\n",
    "\n",
    "\n",
    "```{figure} ../../../img/hello-world-3.png\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Remark.** Note also that geometrically there is more space available to grow the dependence tree compared to top-down or left-right approaches due to nodes being farther apart as we move radially with a fixed angle, this also allows Radar to minimize edge crossing by combining radial and circumferential movement for the edges between task nodes. \n",
    "\n",
    "Furthermore, Radar dynamically updates as tasks complete (or fails). The mini-map, edge tracing, and node selection tools make workflow inspection doable even for highly complex graphs. See [*Introducing Radar*](https://www.prefect.io/guide/blog/introducing-radar/) for further reading.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow runs as flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will write Prefect flows for running modelling experiments as a flow in Prefect. Our idea is to define a `main` flow which consists of one subflow `preprocess_data` and two tasks which will execute MLflow runs. Note that the output of the `preprocess_data` subflow will be used by the two MLflow runs, so there will be some data dependency. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`utils.py`](https://github.com/particle1331/ok-transformer/blob/85993956250601edeccf0d1bd5f192bb20873677/docs/nb/mlops/3-prefect/utils.py)\n",
    "```\n",
    "```python\n",
    "@task\n",
    "def load_training_dataframe(file_path, y_min=1, y_max=60):\n",
    "    \"\"\"Load data from disk and preprocess for training.\"\"\"\n",
    "    \n",
    "    # Load data from disk\n",
    "    data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Create target column and filter outliers\n",
    "    data['duration'] = data.lpep_dropoff_datetime - data.lpep_pickup_datetime\n",
    "    data['duration'] = data.duration.dt.total_seconds() / 60\n",
    "    data = data[(data.duration >= y_min) & (data.duration <= y_max)]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "@task\n",
    "def fit_preprocessor(train_data):\n",
    "    \"\"\"Fit and save preprocessing pipeline.\"\"\"\n",
    "\n",
    "    # Unpack passed data\n",
    "    y_train = train_data.duration.values\n",
    "    X_train = train_data.drop('duration', axis=1)    \n",
    "\n",
    "    # Initialize pipeline\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "\n",
    "    preprocessor = make_pipeline(\n",
    "        PrepareFeatures(categorical, numerical),\n",
    "        DictVectorizer(),\n",
    "    )\n",
    "\n",
    "    # Fit only on train set\n",
    "    preprocessor.fit(X_train, y_train)\n",
    "    joblib.dump(preprocessor, artifacts / 'preprocessor.pkl')\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "@task\n",
    "def create_model_features(preprocessor, train_data, valid_data):\n",
    "    \"\"\"Fit feature engineering pipeline. Transform training dataframes.\"\"\"\n",
    "\n",
    "    # Unpack passed data\n",
    "    y_train = train_data.duration.values\n",
    "    y_valid = valid_data.duration.values\n",
    "    X_train = train_data.drop('duration', axis=1)\n",
    "    X_valid = valid_data.drop('duration', axis=1)\n",
    "    \n",
    "    # Feature engineering\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_valid = preprocessor.transform(X_valid)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "@flow\n",
    "def preprocess_data(train_data_path, valid_data_path):\n",
    "    \"\"\"Return feature and target arrays from paths. \n",
    "    Note: This just combines all the functions above in a single step.\"\"\"\n",
    "\n",
    "    train_data = load_training_dataframe(train_data_path)\n",
    "    valid_data = load_training_dataframe(valid_data_path)\n",
    "    preprocessor = fit_preprocessor(train_data)\n",
    "\n",
    "    # X_train, y_train, X_valid, y_valid\n",
    "    return create_model_features(preprocessor, train_data, valid_data).result()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create the `main` flow described above. Here we are passing around a [`PrefectFuture`](https://orion-docs.prefect.io/api-ref/prefect/futures/) object instead of Python objects. Futures represent the execution of a task and allow retrieval of the task run's state. This so that Prefect is able to track data dependency between tasks &mdash; converting to Python objects, i.e. using `.result()`, breaks this lineage. Note that once a future has been passed into the function, then we can treat it as a usual Python object as the `task` wrapper has done work to unpack the future object into Python objects.\n",
    "\n",
    "For the `main` flow, we execute the following sequentially: running a subflow for preprocessing the datasets for modelling, training a linear regression baseline model, and training XGBoost models with different hyperparameters sampled using [TPE](https://optunity.readthedocs.io/en/latest/user/solvers/TPE.html). Sequential execution ensures that all resources are allocated to a single learning algorithm at each point in the flow run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`main.py`](https://github.com/particle1331/ok-transformer/blob/85993956250601edeccf0d1bd5f192bb20873677/docs/nb/mlops/3-prefect/main.py)\n",
    "```\n",
    "```python\n",
    "def objective(params, xgb_train, y_train, xgb_valid, y_valid):\n",
    "    \"\"\"Compute validation RMSE (one trial = one run).\"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=xgb_train,\n",
    "            num_boost_round=100,\n",
    "            evals=[(xgb_valid, 'validation')],\n",
    "            early_stopping_rounds=5,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "\n",
    "    return {'loss': rmse_valid, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "@task\n",
    "def xgboost_runs(num_runs, data):\n",
    "    \"\"\"Run TPE algorithm on search space to minimize objective.\"\"\"\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "    xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    xgb_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    search_space = {\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "        'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "        'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "        'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    best_result = fmin(\n",
    "        fn=partial(\n",
    "            objective, \n",
    "            xgb_train=xgb_train, y_train=y_train, \n",
    "            xgb_valid=xgb_valid, y_valid=y_valid,\n",
    "        ),\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_runs,\n",
    "        trials=Trials()\n",
    "    )\n",
    "\n",
    "\n",
    "@task\n",
    "def linreg_runs(data):\n",
    "    \"\"\"Run linear regression training.\"\"\"\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "\n",
    "        \n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main(\n",
    "    train_data_path, \n",
    "    valid_data_path, \n",
    "    num_xgb_runs, \n",
    "    experiment_name,\n",
    "    tracking_uri,\n",
    "):\n",
    "    # Set and run experiment\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    data = preprocess_data(train_data_path, valid_data_path)\n",
    "    linreg_runs(data)\n",
    "    xgboost_runs(num_xgb_runs, data)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dashboard, we can see a run of the `main` flow. As expected, this consists of 3 tasks that are executed sequentially as indicated in the timeline. \n",
    "\n",
    "```{figure} ../../../img/mlflow-runs-dashboard.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "Recall that the first task `preprocess-data` is a subflow. This has concurrent execution which we can see from overlapping lines in its timeline graph. This subflow consists of four tasks.\n",
    "\n",
    "```{figure} ../../../img/radar_preprocessing.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check out the radar of the `main` flow, we see the following. Here in an earlier screenshot, we see that `xgboost_runs` is currently running for 1 minute and 14 seconds. Note that both MLflow runs depending on the preprocessing subflow can be seen by the presence of edges. We can go down on the radar for the `preprocess-data` subflow by clicking on the `4 task runs` button.\n",
    "\n",
    "\n",
    "```{figure} ../../../img/radar_xgb.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "Here we see the radar plot. Hovering on each task shows its data dependence on other tasks. For each task, the forward and backward data dependency edges are shown by the Radar graph. For example, in the figure below we can see the dependencies for the task `load_training_dataframe` that loads the train dataset. This sends data to the preprocessor (for training) and to the final task (for transformation) which returns all processed data for modelling, so we can see two forward dependencies.\n",
    "\n",
    "```{figure} ../../../img/radar.png\n",
    "---\n",
    "---\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we deploy a workflow that puts the best model in an MLflow experiment to staging in the model registry. This can be useful for regularly staging candidate models models trained on new data. The staged models can then be further checked if it should be deployed into production. The code below is covered in [Experiment Tracking and Model Management](https://particle1331.github.io/ok-transformer/nb/mlops/2-mlflow/2-mlflow.html#api-workflows)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model staging review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will run MLflow on localhost with SQLite. This can be easily adapted to a remote tracking server we only have to change `MLFLOW_TRACKING_URI`. Connecting to the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "\n",
    "def print_experiment(experiment):\n",
    "    print(f\"(Experiment)\")\n",
    "    print(f\"    experiment_id={experiment.experiment_id}\")\n",
    "    print(f\"    name='{experiment.name}'\")\n",
    "    print(f\"    artifact_location='{experiment.artifact_location}'\")\n",
    "    print()\n",
    "\n",
    "for experiment in client.list_experiments():\n",
    "    print_experiment(experiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the flow performs training a linear model and XGBoost models with different parameters. As sort of minimum requirements, we only consider those models with validation RMSE less than `6.5` and inference time less than `2e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = client.search_runs(\n",
    "    experiment_ids=1,\n",
    "    filter_string='metrics.rmse_valid < 6.5 and metrics.inference_time < 20e-6',\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.rmse_valid ASC\"]\n",
    ")\n",
    "\n",
    "for run in candidates:\n",
    "    print(f\"run_id: {run.info.run_id}   rmse_valid: {run.data.metrics['rmse_valid']:.3f}   inference_time: {run.data.metrics['inference_time']:.4e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having satisfied the operating requirements, we can take the model with lowest error as the new version and set it up for staging. Note that this flow can fail when there are no experiments in our tracker. But that is okay as it serves as notification for us to look into our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_stage = candidates[0]\n",
    "\n",
    "registered_model = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{model_to_stage.info.run_id}/model\", \n",
    "    name='NYCRideDurationModel'\n",
    ")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name='NYCRideDurationModel',\n",
    "    version=registered_model.version, \n",
    "    stage='Staging',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/mlflow-automatic-staging.png\n",
    "---\n",
    "---\n",
    "Staged model from code cells above.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow staging flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, so we collect the above code cells along into a workflow which will create a new experiment, perform the experiment runs, and filter the best model for staging. We will then schedule this workflow to be run at fixed intervals using Prefect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`main.py`](https://github.com/particle1331/ok-transformer/blob/06a1ecfea5456430538d13cdebbbe4c23dbbe93c/docs/nb/mlops/3-prefect/main.py)\n",
    "```\n",
    "```python\n",
    "@task\n",
    "def stage_model(tracking_uri, experiment_name):\n",
    "    \"\"\"Register and stage best model.\"\"\"\n",
    "\n",
    "    # Get best model from current experiment\n",
    "    client = MlflowClient(tracking_uri=tracking_uri)\n",
    "    candidates = client.search_runs(\n",
    "        experiment_ids=client.get_experiment_by_name(experiment_name).experiment_id,\n",
    "        filter_string='metrics.rmse_valid < 6.5 and metrics.inference_time < 20e-6',\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=5,\n",
    "        order_by=[\"metrics.rmse_valid ASC\"]\n",
    "    )\n",
    "\n",
    "    # Register and stage best model\n",
    "    best_model = candidates[0]\n",
    "    registered_model = mlflow.register_model(\n",
    "        model_uri=f\"runs:/{best_model.info.run_id}/model\", \n",
    "        name='NYCRideDurationModel'\n",
    "    )\n",
    "\n",
    "    client.transition_model_version_stage(\n",
    "        name='NYCRideDurationModel',\n",
    "        version=registered_model.version, \n",
    "        stage='Staging',\n",
    "    )\n",
    "\n",
    "    # Update description of staged model\n",
    "    client.update_model_version(\n",
    "        name='NYCRideDurationModel',\n",
    "        version=registered_model.version,\n",
    "        description=f\"[{datetime.now()}] The model version {registered_model.version} from experiment '{experiment_name}' was transitioned to Staging.\"\n",
    "    )\n",
    "\n",
    "...\n",
    "\n",
    "@flow(name='mlflow-staging', task_runner=SequentialTaskRunner())\n",
    "def mlflow_staging(\n",
    "    train_data_path, \n",
    "    valid_data_path, \n",
    "    num_xgb_runs=1\n",
    "):    \n",
    "    # Setup experiment\n",
    "    ctx = get_run_context()\n",
    "    MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "    EXPERIMENT_NAME = f\"nyc-taxi-experiment-{ctx.flow_run.expected_start_time}\"\n",
    "\n",
    "    # Make experiment runs\n",
    "    main(\n",
    "        train_data_path=train_data_path, \n",
    "        valid_data_path=valid_data_path, \n",
    "        num_xgb_runs=num_xgb_runs, \n",
    "        experiment_name=EXPERIMENT_NAME,\n",
    "        tracking_uri=MLFLOW_TRACKING_URI,\n",
    "    )\n",
    "    \n",
    "    # Stage best model\n",
    "    stage_model(\n",
    "        tracking_uri=MLFLOW_TRACKING_URI, \n",
    "        experiment_name=EXPERIMENT_NAME\n",
    "    )\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `mlflow_staging` creates a new experiment every time it runs with name based on the scheduled start time (obtained using the run context `ctx`). It's important that injecting the correct datetime parameter (for identifying the experiment) is delegated to the orchestrator instead of the executing machine. For example, the clock on the executing machine might be rouge, in a different timezone, or the job may be queued for some reason so that execution time is delayed. This is the case for the test run in the following figure.\n",
    "\n",
    "\n",
    "```{figure} ../../../img/flow-context.png\n",
    "---\n",
    "---\n",
    "Run context for a test flow that simply logs the scheduled time. For some reason, execution is delayed so that the execution time differs from the scheduled time. Also, the orchestrator time is in UTC (standard) while the other values are in UTC+8 (system time).\n",
    "```\n",
    "\n",
    "\n",
    "Later we see that we can specify a parameter dictionary for passing parameter values in the flow during deployment. So it is good practice to run a flow with a test parameters dict. Thus, we append the script with the following:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parameters={\n",
    "        \"train_data_path\": './data/green_tripdata_2021-01.parquet',\n",
    "        \"valid_data_path\": './data/green_tripdata_2021-02.parquet',\n",
    "        \"num_xgb_runs\": 3,\n",
    "    }\n",
    "\n",
    "    mlflow_staging(**parameters)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out `flashy-dove` in the Prefect Orion UI. Looks good. Here we see the `main` subflow for training and the `stage_model` task for staging the best model in the experiment that was just performed. This can all be checked in MLflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/flashy-dove.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGTM, let's deploy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local storage setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a deployment in Prefect let us first setup a local storage for saving for persisting flow code for deployments, task results, and flow results. This is simple enough to do. Take note of the storage identifier as this will be used later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ prefect storage create\n",
    "\n",
    "Found the following storage types:\n",
    "0) Azure Blob Storage\n",
    "    Store data in an Azure blob storage container.\n",
    "1) File Storage\n",
    "    Store data as a file on local or remote file systems.\n",
    "2) Google Cloud Storage\n",
    "    Store data in a GCS bucket.\n",
    "3) Local Storage\n",
    "    Store data in a run's local file system.\n",
    "4) S3 Storage\n",
    "    Store data in an AWS S3 bucket.\n",
    "5) Temporary Local Storage\n",
    "    Store data in a temporary directory in a run's local file system.\n",
    "Select a storage type to create: 3\n",
    "You've selected Local Storage. It has 1 option(s).\n",
    "STORAGE PATH: ~/.prefect/local-storage\n",
    "Choose a new name for this storage configuration: local-storage\n",
    "Registered storage 'local-storage' with identifier\n",
    "'33133d27-a83b-468c-bb72-a9f19bd0d157'.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment specification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the deployment, we run the `mlflow-staging` flow every 5 minutes locally. We also set the parameters for each run. This adds a bit of flexibility. For now, the datasets are fixed, but it makes sense to run this on fresh data so that we actually get an updated model at each scheduled run. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`deployments.py`](https://github.com/particle1331/ok-transformer/blob/85993956250601edeccf0d1bd5f192bb20873677/docs/nb/mlops/3-prefect/deployments.py)\n",
    "```\n",
    "```python\n",
    "from prefect.deployments import DeploymentSpec\n",
    "from prefect.orion.schemas.schedules import IntervalSchedule\n",
    "from prefect.flow_runners import SubprocessFlowRunner\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "DeploymentSpec(\n",
    "    name=\"deploy-mlflow-staging\",\n",
    "    flow_name='mlflow-staging',\n",
    "    schedule=IntervalSchedule(interval=timedelta(minutes=1)),\n",
    "    flow_location=\"./main.py\",\n",
    "    flow_storage=\"33133d27-a83b-468c-bb72-a9f19bd0d157\", # local storage id\n",
    "    flow_runner=SubprocessFlowRunner(),\n",
    "    parameters={\n",
    "        \"train_data_path\": './data/green_tripdata_2021-01.parquet',\n",
    "        \"valid_data_path\": './data/green_tripdata_2021-02.parquet',\n",
    "        \"num_xgb_runs\": 10,\n",
    "    },\n",
    "    tags=[\"ml\"]\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying `SubprocessFlowRunner()` as flow runner, means that this flow is executed locally, e.g. not on Kubernetes or Docker containers. Here we specify the local storage identifier that we have just created above. Note that we specify the flow by name (`'mlflow-staging'`). This is why we defined `name='mlflow-staging'` in the decorator of `mlflow_staging`. To push our deployment to Prefect, we execute the following in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prefect deployment create deployments.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/deploy-mlflow-staging.png\n",
    "---\n",
    "---\n",
    "100 runs are now scheduled in Prefect.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding workers to deployed workflows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flows are now scheduled in Prefect. Notice that there are late runs. This is because we haven't attached any workers that will run these tasks. Unlike CI/CD platforms, all compute happens outside of Prefect that users will have to provide to run the scheduled workflows. So we create a **work queue** and fire up a **Prefect agent** to execute our deployment with our local compute. Note that the following setting up can also be done with the help of the UI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prefect deployment ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the deployment ID above\n",
    "!prefect work-queue create \\\n",
    "    --deployment '271734ba-6e5e-4fa1-bf03-ab6801f7b44f' \\\n",
    "    --flow-runner subprocess \\\n",
    "    mlflow-deploy-runner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the scheduled runs for this worker. This also confirms that we chose the correct flow runner and deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the worker ID above\n",
    "!prefect work-queue preview 3f80fde3-4390-413f-be3a-3d8da1913163"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the worker now, so it picks up scheduled runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the worker ID above\n",
    "!prefect agent start 3f80fde3-4390-413f-be3a-3d8da1913163"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stop the run since this will take a while. But we already see that it has completed one flow, while some flows are still running, some are late, and many are scheduled. There are late flows because we had 1 minute deployments and it took a while for us to create a worker. Another nice thing to notice with the worker works concurrently on late flows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/deployment-1.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/deployment-2.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the log on `00:58:47.290` we see that the completed flow `gleaming-honeybee` created Version 4 of the model `NYCRideDurationModel`. Indeed, we have this version staged in the model registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_latest_versions(name='NYCRideDurationModel')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
