{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Nearest Neighbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{admonition} Attribution\n",
    "This notebook is based on [Lecture 2](https://github.com/rasbt/stat451-machine-learning-fs20/tree/master/L02) of [Sebastian Raschka](http://pages.stat.wisc.edu/~sraschka/)'s [STAT 451: Intro to Machine Learning @ UW-Madison (Fall 2020)](http://pages.stat.wisc.edu/~sraschka/teaching/stat451-fs2020/).\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "While KNN is a universal function approximator under certain conditions, the underlying\n",
    "concept is relatively simple. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. lazy learner, i.e. all processing of training examples occurs during inference. training = just store data.\n",
    "2. computing m(q) on a trained knn model m just looks for the k nearest neighbors of a query point q and uses properties of those k train examples to compute a class label &mdash; or a continuous target, in the case of regression.\n",
    "\n",
    "the overall idea is that instead of approximating a function globally during a prediction, m approximates the target function locally.\n",
    "\n",
    "put image knn-query.png\n",
    "\n",
    "Illustration of the nearest neighbor classification algorithm in two dimensions (features\n",
    "x1 and x2). In the left subpanel, the training examples are shown as blue dots, and a query point\n",
    "that we want to classify is shown as a question mark. In the right subpanel, the class labels are\n",
    "annotated via blue squares and red triangle symbols. The dashed line indicates the nearest neighbor\n",
    "of the query point, assuming a Euclidean distance metric. The predicted class label is the class\n",
    "label of the closest data point in the training set (here: class 0, i.e., blue square)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nearest neighbors in context"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the prediction is based on a\n",
    "comparison of a query point with data points in the training set (rather than a global\n",
    "model), kNN is also categorized as instance-based (or “memory-based”) method. In contrast, SVM is an eager instance-based learning algorithm. Lastly, because we do not make any assumption about the functional form of the kNN\n",
    "algorithm, a kNN model is also considered a nonparametric model.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Under certain assumptions, we can estimate the conditional probability that a given data\n",
    "point belongs to a given class as well as the marginal probability for a feature given a training\n",
    "dataset (more details are provided in the section on “kNN from a Bayesian Perspective”\n",
    "later). However, since kNN does not explicitly try to model the data generating process\n",
    "but models the posterior probabilities, p(f(x) = i|x), directly, kNN is usually considered a\n",
    "discriminative model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Common use cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "While neural networks are gaining popularity in the computer vision and pattern recognition\n",
    "field, one area where k-nearest neighbors models are still commonly and successfully being\n",
    "used is in the intersection between computer vision, pattern classification, and biometrics\n",
    "(e.g., to make predictions based on extracted geometrical features2).\n",
    "Other common use cases include recommender systems (via collaborative filtering3) and\n",
    "outlier detection4.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nearest Neighbors Algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NearestNeighbor:\n",
    "    \"\"\"Implementing the Nearest Neighbor algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, metric=\"Euclidean\"):\n",
    "        if metric == \"Euclidean\":\n",
    "            self.distance = lambda x, y: np.sqrt(((x - y)**2).sum())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model. Here X is a 2d numpy array and y is array-like.\"\"\"\n",
    "\n",
    "        self.data = X\n",
    "        self.targets = y\n",
    "\n",
    "    def predict(self, query_point):\n",
    "        \"\"\"Predict label of query point.\"\"\"\n",
    "\n",
    "        query_point = np.array(query_point)\n",
    "        t_min = None\n",
    "        min_distance = np.inf\n",
    "        \n",
    "        for i in range(len(self.data)):\n",
    "            x = self.data[i]\n",
    "            t = self.targets[i]\n",
    "            distance = self.distance(query_point, x)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                t_min = t\n",
    "\n",
    "        return t_min"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing with a simple dataset and query point."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Generate fake dataset\n",
    "X = np.array([\n",
    "    [1, 2],\n",
    "    [3, 3],\n",
    "    [5, 1]\n",
    "])\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Initialize NN model\n",
    "model = NearestNeighbor()\n",
    "\n",
    "# Fit and make inference\n",
    "model.fit(X, y)\n",
    "print(model.predict([1, 1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The default distance for KNN algorithm is the Euclidean distance\n",
    "\n",
    "$$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\sum_{j=1}^m {\\left(x_j - y_j\\right)}^2 }.$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nearest Neighbor Decision Boundary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('homl': conda)"
  },
  "interpreter": {
   "hash": "5015f56ad70b34caa323120b2da519fd7eb92ffe0301170069951d562b8794dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}