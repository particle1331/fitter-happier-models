{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=brightgreen)\n",
    "[![Source](https://img.shields.io/static/v1.svg?label=GitHub&message=Source&color=181717&logo=GitHub)](https://github.com/particle1331/inefficient-networks/blob/master/docs/notebooks/tensorflow/05-tensorflow-cnn.ipynb)\n",
    "[![Stars](https://img.shields.io/github/stars/particle1331/inefficient-networks?style=social)](https://github.com/particle1331/inefficient-networks)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the idea for using convolutions is that close pixels are related. But pixels far from each other have none. For sequences, this is not necessarily true. There can be long term dependencies between words in a at the beginning and end of a paragraph, for example. For modelling sequences, we will look at **recurrent connections** (i.e. cyclic dependencies) and its extensions. As with convolutional layers, recurrent units also use weight sharing but over time instead of over space.\n",
    "\n",
    "The key idea for sequence modelling is that while sequences $\\langle \\boldsymbol{\\mathsf{x}}_1, \\boldsymbol{\\mathsf{x}}_2, \\ldots, \\boldsymbol{\\mathsf{x}}_T \\rangle$ have arbitrary length $T \\in \\mathbb{N},$ each time step can be modelled with a state vector $\\boldsymbol{\\mathsf{x}}_t \\in \\mathbb{R}^d$ with fixed number of entries. For example, when modelling temperatures for each day, we can use a sequence of maximum, minimum, and average temperatures getting a 3-dimensional vector to represent the state of a day. For this to work well, each step in a sequence must be semantically equivalent, and that order matters. Recurrent connections are able to capture this information in the data by using a memory vector $\\boldsymbol{\\mathsf{h}}_t.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "DATASET_DIR = Path(\"./data\").absolute()\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent connections compute a hidden state vector $\\boldsymbol{\\mathsf{h}}_t$ which evolves based on the new system state $\\boldsymbol{\\mathsf{x}}_t$ and the existing hidden state $\\boldsymbol{\\mathsf{h}}_{t-1}.$ This has weights both for blending the past history to the current state of the system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\mathsf{h}}_t \n",
    "&= \\textsf{A}(\\boldsymbol{\\mathsf{h}}_{t-1}, \\boldsymbol{\\mathsf{x}}_{t}) \\\\\n",
    "&= \\tanh\\left( \\boldsymbol{\\mathsf{h}}_{t-1} \\boldsymbol{\\mathsf{W}}_{\\mathsf{h}} +  {\\boldsymbol{\\mathsf{x}}}_t\\, \\boldsymbol{\\mathsf{W}}_{\\mathsf{x}}  + \\boldsymbol{\\mathsf{b}}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "such that $\\boldsymbol{\\mathsf{h}}_0 = \\boldsymbol 0.$ Note that this is able to process an entire sequence regardless of its length. The choice of nonlinearity means that components of the hidden state saturate in the range $[-1, 1].$ \n",
    "\n",
    "Unrolling recurrent connections makes it look more familiar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "Source:<br>\n",
    "[`d2l.ai/ch9`](https://www.d2l.ai/chapter_recurrent-neural-networks/index.html)\n",
    "```\n",
    "```{figure} ../../img/unfolded-rnn.svg\n",
    "---\n",
    "width: 80%\n",
    "name: unfolded-rnn\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** Input order matters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h=\n",
      "[[0.7637734  0.74654836]]\n",
      "[[0.77538518 0.8957538 ]]\n",
      "[[0.79465479 0.91582463]]\n",
      "\n",
      "h=\n",
      "[[0.62483806 0.66089363]]\n",
      "[[0.85413636 0.91019435]]\n",
      "[[0.79689722 0.92114369]]\n"
     ]
    }
   ],
   "source": [
    "T = 3\n",
    "x  = np.random.random((T, 3))\n",
    "Wx = np.random.random((3, 2))\n",
    "Wh = np.random.random((2, 2))\n",
    "\n",
    "\n",
    "def run(order=[0, 1, 2]):\n",
    "    h  = np.zeros((1, 2))\n",
    "\n",
    "    print('\\nh=')\n",
    "    for i in order:\n",
    "        h = np.tanh(h @ Wh + x[[i]] @ Wx)\n",
    "        print(h)\n",
    "\n",
    "\n",
    "run([0, 1, 2])\n",
    "run([1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at how to use RNN units in PyTorch to predict the likely country of origin of a name. The code for this section is based on [this notebook](https://github.com/EdwardRaff/Inside-Deep-Learning/blob/main/Chapter_4.ipynb). For this task we will classify a name's source language by passing the characters of a name as sequence that is fed into the RNN unit. Each character updates the hidden state vector. Once the complete name has been processed, we get a final state vector, which is passed to a classification subnetwork for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "Source:<br>\n",
    "\n",
    "```\n",
    "```{figure} ../../img/rnn-names.png\n",
    "---\n",
    "width: 500px\n",
    "name: rnn-names\n",
    "---\n",
    "\n",
    "Classifying the source language for the name Frank. The characters of the name is sequentially passed to an RNN unit resulting in a final hidden state $\\boldsymbol{\\mathsf{h}}_5$ that is passed to  a linear layer.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "file_url = \"https://download.pytorch.org/tutorial/data.zip\"\n",
    "zip = zipfile.ZipFile(io.BytesIO(requests.get(file_url).content))\n",
    "zip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unicode (e.g. Ślusàrski to Slusarski):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic:       2000\n",
      "Chinese:        268\n",
      "Czech:        519\n",
      "Dutch:        297\n",
      "English:       3668\n",
      "French:        277\n",
      "German:        724\n",
      "Greek:        203\n",
      "Irish:        232\n",
      "Italian:        709\n",
      "Japanese:        991\n",
      "Korean:         94\n",
      "Polish:        139\n",
      "Portuguese:         74\n",
      "Russian:       9408\n",
      "Scottish:        100\n",
      "Spanish:        298\n",
      "Vietnamese:         73\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    alphabet = {}\n",
    "    for i, a in enumerate(string.ascii_letters + \" .,;'\"):\n",
    "        alphabet[a] = i\n",
    "\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn' and c in alphabet\n",
    "    )\n",
    "\n",
    "\n",
    "# Loop through every language:\n",
    "#   1. Open the zip file entry\n",
    "#   2. Get target language based on filename.\n",
    "#   3. Read text file, save all names to ascii to target language.\n",
    "data = {}\n",
    "for path in (p for p in zip.namelist() if \"names\" in p and p.endswith(\".txt\")):\n",
    "    lang = path.split('/')[-1].replace(\".txt\", \"\")\n",
    "    with zip.open(path) as f:\n",
    "        lang_names = [unicode_to_ascii(line).lower() for line in str(f.read(), encoding='utf-8').strip().split(\"\\n\")]\n",
    "        data[lang] = lang_names\n",
    "    \n",
    "for lang, lang_names in data.items():\n",
    "    print(f\"{lang}: {len(lang_names):>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/particle1331/code/inefficient-networks/docs/notebooks/tensorflow/data/names')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_DIR / \"names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(p for p in zip.namelist() if \"names\" in p and p.endswith(\".txt\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bb4561ca8d8b7b3a7bc7514080b6e7dab3824c9a0b3ef748f0e5ff42277ee64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
