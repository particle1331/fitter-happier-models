{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=brightgreen)\n",
    "[![Source](https://img.shields.io/static/v1.svg?label=GitHub&message=Source&color=181717&logo=GitHub)](https://github.com/particle1331/inefficient-networks/blob/master/docs/notebooks/tensorflow/05-tensorflow-cnn.ipynb)\n",
    "[![Stars](https://img.shields.io/github/stars/particle1331/inefficient-networks?style=social)](https://github.com/particle1331/inefficient-networks)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome to recurrent neural networks.\n",
    "\n",
    "So in this module, we're going to discuss recurrent neural networks and everything related to them,\n",
    "\n",
    "as well as their variants such as LSD and Joyeux.\n",
    "\n",
    "So first of all, we need to understand, why do we need Arnon's?\n",
    "\n",
    "Why can't we just use normal feed forward networks?\n",
    "\n",
    "And we're going to understand what our answer the general.\n",
    "\n",
    "So usually Arnon's are used for time series analysis.\n",
    "\n",
    "So when we have to make a prediction that requires remembering or it requires some memory, which is\n",
    "\n",
    "based on previous data, not only the current data, then we make use of an Anadan.\n",
    "\n",
    "So in a normal feed for that work, realize that we don't make use of previous data, we only make use\n",
    "\n",
    "of the current inputs.\n",
    "\n",
    "So Arnon's allow you to make use of previous data and to remember what happened in previous data.\n",
    "\n",
    "So basically they implement a memory.\n",
    "\n",
    "So they make use of sequential information, let me give you two examples to facilitate the explanation\n",
    "\n",
    "of Arnon's stock price prediction is one example so we can predict the price of the current month by\n",
    "\n",
    "just looking at one inputs, for example, the previous day.\n",
    "\n",
    "What we need to do is we need to observe the data and the trend previously and in previous days and\n",
    "\n",
    "previous months even in order to make a new prediction.\n",
    "\n",
    "So this requires remembering what happened previously in order to predict the current and which is the\n",
    "\n",
    "current price for the day.\n",
    "\n",
    "Another example is text generation so we can predict the next word in a sentence based on the previous\n",
    "\n",
    "word only.\n",
    "\n",
    "We need to understand the general context of the sentence.\n",
    "\n",
    "We do know what happened previously, the sentence in order to predict the next word.\n",
    "\n",
    "So we need to look at the whole sentence to understand its context.\n",
    "\n",
    "If you're predicting each word in a sentence.\n",
    "\n",
    "So let's say the sentence is she spoke to her husband and you're to due to this location right here\n",
    "\n",
    "and you want to put the word her, of course, you can't predict the words her only by looking at two.\n",
    "\n",
    "So you're going to look back and realize what this sentence is about.\n",
    "\n",
    "In this case, it's she spoke to.\n",
    "\n",
    "And then once you understand this whole sorry, the previous words in the sentence and especially the\n",
    "\n",
    "word she because she will allow you to predict her, then you can predict this this word right here.\n",
    "\n",
    "So you need to understand the general context in this case.\n",
    "\n",
    "It's she spoke to.\n",
    "\n",
    "So the context here is that somebody spoke to and you need to also look back at the word she to understand\n",
    "\n",
    "that the person that spoke is a female.\n",
    "\n",
    "OK, so in this case, you can predict the words here.\n",
    "\n",
    "So this is an example of using Arnett's.\n",
    "\n",
    "Another way to think about ordinances is that they have a memory which captures information about what\n",
    "\n",
    "has been calculated so far.\n",
    "\n",
    "So in this case, the hidden state where she spoke and to have been calculated and now we can predict\n",
    "\n",
    "the word her and.\n",
    "\n",
    "Another thing is that recurrent neural networks are called recurrent because they perform the same task\n",
    "\n",
    "for every element of a sequence.\n",
    "\n",
    "So another way to think about Arnon's, as I told you, is that they have a memory which which makes\n",
    "\n",
    "use of previous information and whatever has been calculated up until the point.\n",
    "\n",
    "Now, an answer is very simply several copies of the same neural network that are aligned together and\n",
    "\n",
    "each one passes its outputs to the next one.\n",
    "\n",
    "So the first neural network passes the output.\n",
    "\n",
    "The second neural network on the second network passes its output to the third neural network and so\n",
    "\n",
    "on.\n",
    "\n",
    "So neural network is called a timestep.\n",
    "\n",
    "OK, so let's say that you have five copies of the neural network, then you have five time steps.\n",
    "\n",
    "In this case, if I give you an example, then let's say we have we have a sequence and we care.\n",
    "\n",
    "We care about three words of the sentence or sequence.\n",
    "\n",
    "They'll say we want to encode these three words or predict them.\n",
    "\n",
    "And then what we have is three copies of the neural network and each copy is called a timestep.\n",
    "\n",
    "OK, so one time for each word, each word, each time step, which means each neural network receives\n",
    "\n",
    "one of these three words.\n",
    "\n",
    "So each copy, it's called the Times because it receives different inputs at different times.\n",
    "\n",
    "So we're going to see later on, that's the Arnon's not only receive the previous sorry, the current\n",
    "\n",
    "input, it also receives the previous hidden states.\n",
    "\n",
    "And now we're going to explain what is a hidden state.\n",
    "\n",
    "So that's why it's called the timestamp, because it receives different inputs at different times.\n",
    "\n",
    "So if we have three words, then each time step, each neural network receives one of these words.\n",
    "\n",
    "That's why it's called the timestep.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "\n",
    "DATASET_DIR = Path(\"./data\").absolute()\n",
    "\n",
    "warnings.simplefilter(action=\"once\")\n",
    "backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at the equations for an answer.\n",
    "\n",
    "So, as I told you, it's just a copy of Neural Network several times.\n",
    "\n",
    "And here we have four copies.\n",
    "\n",
    "So we have four time steps.\n",
    "\n",
    "And now let's have a look at the equations for an hour and now realize that each of these five steps\n",
    "\n",
    "here is just a neural network.\n",
    "\n",
    "So we have a neural network which consists of the input layer, the hidden layer and the outputs layer,\n",
    "\n",
    "the very simple neural network.\n",
    "\n",
    "Now, what we're going to do is pass the first inputs and get the first input to the employer and then\n",
    "\n",
    "pass the employer of the hidden layer.\n",
    "\n",
    "And now what we're going to do, we're going to take this output of the hidden layer and pass it to\n",
    "\n",
    "the next neural network.\n",
    "\n",
    "So this next timestep or this next neural network receives the previous output of the hidden state as\n",
    "\n",
    "well as the current inputs.\n",
    "\n",
    "OK, and again, you do the same thing.\n",
    "\n",
    "So you feed the current inputs, you get the representation, and now you feed the hidden representation\n",
    "\n",
    "to the third neural network third times.\n",
    "\n",
    "So the third time surpasses the previous state as well as the current inputs.\n",
    "\n",
    "And you continue on up until the end of the sequence.\n",
    "\n",
    "So what are the equations in this case?\n",
    "\n",
    "We're going to project each of X and the hidden states and the first step at zero is just a sensor or\n",
    "\n",
    "a matrix of zeros.\n",
    "\n",
    "So this is how we initialize the hidden state since we don't have anything.\n",
    "\n",
    "So we just initialize it to Jerome.\n",
    "\n",
    "So what we're going to do is do two linear projections.\n",
    "\n",
    "So realize they have different weights here.\n",
    "\n",
    "The first projection is for the inputs, which is this right here, and the second projection is for\n",
    "\n",
    "the hidden state.\n",
    "\n",
    "And now we're going to use the same exact weights.\n",
    "\n",
    "Remember, we're doing these are all just copies of the network.\n",
    "\n",
    "So they're not different networks.\n",
    "\n",
    "They're the same exact network working together, aligning together.\n",
    "\n",
    "So they have all the same weights right here.\n",
    "\n",
    "They just receive different.\n",
    "\n",
    "And so we're going to take the the second input, again, projected through W X and we're going to take\n",
    "\n",
    "the previous domestic and also projected through W each.\n",
    "\n",
    "And the activation function here is the Dunwich activation function that is used an ordinance and now\n",
    "\n",
    "we finish with our second that we're going to do the third time step again, Project X three and it's\n",
    "\n",
    "two.\n",
    "\n",
    "We're going to protect the privacy of the states and we're going to attach activation function we get\n",
    "\n",
    "at three and now finally we take X3.\n",
    "\n",
    "So we're going to pass it on the fourth time.\n",
    "\n",
    "So we're going to projected through A, through W, X and we're going to take the fourth and what's\n",
    "\n",
    "projected through W X running through a kind of activation function and we get Atwar.\n",
    "\n",
    "And now if we're doing some classification or prediction task, then of course we need an output layer.\n",
    "\n",
    "So in this case, we have an upper tier, which basically is a linear layer followed by a softmax in\n",
    "\n",
    "order to classify our input.\n",
    "\n",
    "So let's say we're doing text generation, then we're going to classify according to how many words\n",
    "\n",
    "we have in our vocabulary.\n",
    "\n",
    "So how what are the what is the word that we want to predict?\n",
    "\n",
    "So this is a classification task.\n",
    "\n",
    "So here, for example, I'm just making it very simple.\n",
    "\n",
    "We have two words to predict.\n",
    "\n",
    "So we want to predict whether it's the first word or the second word.\n",
    "\n",
    "So we do this usual as we do a normal fit for the words.\n",
    "\n",
    "We project the output, each one through a linear layer, which is called w w why in this case.\n",
    "\n",
    "And we take this softmax and we get our prediction.\n",
    "\n",
    "So this is nothing new here.\n",
    "\n",
    "This is just the output.\n",
    "\n",
    "And the most important thing to realize is that all of these steps share the same weight.\n",
    "\n",
    "So they all have W, W, X and Y, they just receive difference.\n",
    "\n",
    "And so they're working all together in order to understand the sequence.\n",
    "\n",
    "Now, let's talk about how we can turn feed forward networks into Arnon's.\n",
    "\n",
    "OK, now let's talk about how to visualize feed for networks and relationships to recurrent neural networks.\n",
    "\n",
    "So we have our feet forward here.\n",
    "\n",
    "This is the fourth neural network that we usually work with.\n",
    "\n",
    "It consists of an input layer and outputs.\n",
    "\n",
    "And now if we take this and flip it and then we just back it up so easily, we're just going to back\n",
    "\n",
    "up all the neurons in each layer.\n",
    "\n",
    "So the first layer has four neurons, the first layer has three neurons and the first layer has two\n",
    "\n",
    "neurons.\n",
    "\n",
    "And then we're just going to represent each of these multidimensional neurons in one neuron, OK?\n",
    "\n",
    "So here we're going to represent an aunt and this is a representation of an aunt.\n",
    "\n",
    "And why I'm doing this is because you're going to see this in many literature literature.\n",
    "\n",
    "So many of these scientific papers represents an art and in this form right here.\n",
    "\n",
    "But you should understand what they mean is not only one you're on.\n",
    "\n",
    "This is actually a multidimensional neuron.\n",
    "\n",
    "So here we have four neurons.\n",
    "\n",
    "As you can see here, we have three neurons and here we have two.\n",
    "\n",
    "They're just representing these multidimensional neurons into one.\n",
    "\n",
    "You're on for simplicity.\n",
    "\n",
    "So you should understand this because you're going to see this and many literature.\n",
    "\n",
    "So this is our.\n",
    "\n",
    "And remember, this is a multidimensional representation.\n",
    "\n",
    "So we're representing a multidimensional, multidimensional neurons and just one neuron.\n",
    "\n",
    "And now we're going to take this and copy it.\n",
    "\n",
    "So let's say that we want five, five steps.\n",
    "\n",
    "So we're going to copy this network five times.\n",
    "\n",
    "So we're going to copy the same network for the number of times.\n",
    "\n",
    "That's OK.\n",
    "\n",
    "And this is how you presenting art.\n",
    "\n",
    "And so this is how you're going to see and are represented in literature.\n",
    "\n",
    "OK, so remember, this is the same exact network, they all share the same ways, they just receive\n",
    "\n",
    "different inputs.\n",
    "\n",
    "So here we have the same network copied four, five times, which is the number of five steps.\n",
    "\n",
    "And again, remember, each of these is just this feeds forward neural network right here, OK?\n",
    "\n",
    "And again, I emphasize that each of these neurons is multidimensional.\n",
    "\n",
    "So this is what we have right here.\n",
    "\n",
    "This is an example of three steps.\n",
    "\n",
    "This is what you usually have.\n",
    "\n",
    "So we have a feed forward that we're copied for three times, and each of these receive the output of\n",
    "\n",
    "the previous neural network.\n",
    "\n",
    "OK, so these are the hidden units in an urn.\n",
    "\n",
    "So if you go back here, then these are basically the hidden neurons right here.\n",
    "\n",
    "We just have them up for visualisations.\n",
    "\n",
    "And each of these are cold attacks.\n",
    "\n",
    "So this is the input layer, the layer upon layer.\n",
    "\n",
    "So the first neuron receives the input of the first input of the first step and a vector of zeros since\n",
    "\n",
    "we're initializing the hidden state and then it generates an output representation of the hidden layer.\n",
    "\n",
    "So we're going to take the output of the hillier and pass it to the next timestep.\n",
    "\n",
    "So the next time step or the next neural network receives the second input and the previous hidden state.\n",
    "\n",
    "And then we were going to pass it to the third time step, so the third time step received the third\n",
    "\n",
    "and fourth and the previous events and so on.\n",
    "\n",
    "OK, so that's it for our own ends.\n",
    "\n",
    "And the next video we'll be discussing about back propagation through time.\n",
    "\n",
    "Thank you very much.\n",
    "\n",
    "And I'll see you next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e22d0483a6548b95afd8edd918b22c86e89936e118673cc5f8f537d08f54242e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
