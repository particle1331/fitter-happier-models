{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation on DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=brightgreen)\n",
    "[![Source](https://img.shields.io/static/v1.svg?label=GitHub&message=Source&color=181717&logo=GitHub)](https://github.com/particle1331/inefficient-networks/blob/master/docs/notebooks/fundamentals/backpropagation.ipynb)\n",
    "[![Stars](https://img.shields.io/github/stars/particle1331/inefficient-networks?style=social)](https://github.com/particle1331/inefficient-networks)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will look at **backpropagation** (BP) on computational **directed acyclic graphs** (DAG). Our main result is that a training step for a single data point (consisting of both a forward and a backward pass) has a time complexity that is linear in the number of edges of the graph. This result is interesting since it turns out that neural networks are fundamentally just computational DAGs.\n",
    "\n",
    "<!-- In the last section, we take a closer look at the implementation of `.backward` in PyTorch. -->\n",
    "\n",
    "**References**\n",
    "* [Evaluating $\\nabla f(x)$ is as fast as $f(x)$](https://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/)\n",
    "* [Back-propagation, an introduction](http://www.offconvex.org/2016/12/20/backprop/)\n",
    "* [PyTorch Autograd Explained - In-depth Tutorial](https://www.youtube.com/watch?v=MswxJw-8PvE)\n",
    "* [Topological Sort of Directed Acyclic Graphs](https://www.baeldung.com/cs/dag-topological-sort)\n",
    "* [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent on the loss surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every data point $(\\mathbf x, y)$, the loss function $\\mathcal L$ assigns a nonnegative number $\\mathcal L(y, f_{\\boldsymbol \\Theta}(\\mathbf x))$ that approaches zero whenever the predictions $f_{\\boldsymbol \\Theta}(\\mathbf x)$ approach the target values $y$. Given the current parameters $\\boldsymbol \\Theta \\in \\mathbb R^d$ of a neural network $f,$ we can imagine the network to be at a certain point $(\\boldsymbol \\Theta, \\mathcal L_{\\mathcal X}(\\boldsymbol \\Theta))$ on a surface in $\\mathbb R^d \\times \\mathbb R$ where $\\mathcal L_{\\mathcal X}(\\boldsymbol \\Theta)$ is the average loss over the dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal L_{\\mathcal X}(\\boldsymbol \\Theta) = \\frac{1}{|\\mathcal X|} \\sum_{(\\mathbf x, y) \\in \\mathcal X} \\mathcal L(y, f_{\\boldsymbol \\Theta}(\\mathbf x)).\n",
    "$$\n",
    "\n",
    "The empirical loss acts as an almost-everywhere differentiable surrogate to the true objective (e.g. a non-differentiable metric such as accuracy). This will generally vary for each sample, hence the subscript. Nevertheless, we except these surfaces to be very similar assuming the samples are drawn from the same distribution. Training a neural network is equivalent to finding the minimum of this surface. The first derivatives give us information about the local slope of the surface which we can use to make the first-order approximation \n",
    "\n",
    "$$\\Delta \\mathcal L_{\\mathcal X} \\approx  \\sum_k \\left(\\frac{\\partial \\mathcal L_{\\mathcal X}}{ \\partial \\Theta_k} \\right)  \\Delta \\Theta_k = \\left(\\nabla_{\\boldsymbol \\Theta}\\, \\mathcal L_{\\mathcal X}\\right) \\cdot \\Delta {\\boldsymbol\\Theta}.$$ \n",
    "\n",
    "Then, the direction of steepest descent at the current point in the surface is $-\\nabla_{\\boldsymbol \\Theta}\\, \\mathcal L_{\\mathcal X}.$ So the update rule used for network weights is $\\boldsymbol \\Theta \\leftarrow \\boldsymbol \\Theta - \\varepsilon \\nabla_{\\boldsymbol \\Theta}\\, \\mathcal L_\n",
    "{\\mathcal X}$ where $\\varepsilon > 0$ called the learning rate is a constant that controls the step size. Since this uses a fixed factor for all directions, the resulting weights can potentially overshoot the minimum in some directions. This explains why second-order methods which uses second derivatives, i.e. the [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix), to measure local surface curvature are used. Since this is impractical to compute for large number of weights, the approach used in practice is to instead use [adaptive learning rates](https://particle1331.github.io/inefficient-networks/notebooks/tensorflow/04-tensorflow-optim-init.html#optimization-algorithms).\n",
    "\n",
    "**Remark.** Note that since the loss is a sum over pointwise losses, it suffices to compute gradients for loss surfaces generated by a single input-output pair $(\\mathbf x, y) \\in \\mathcal X$ to compute the gradient of the empirical loss over $\\mathcal X$ by linearity of $\\nabla_{\\boldsymbol \\Theta}.$\n",
    "\n",
    "\n",
    "```{figure} ../../img/loss_surface_resnet.png\n",
    "---\n",
    "name: loss-surface-resnet\n",
    "width: 35em\n",
    "---\n",
    "Loss surface for ResNet-56 with or without skip connections. Much of deep learning research is dedicated to studying the geometry of loss surfaces and its effect on optimization. {cite}`arxiv.1712.09913`\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**The need for efficient BP**\n",
    "```\n",
    "\n",
    "Observe that $\\nabla_{\\boldsymbol \\Theta}\\, \\mathcal L_{\\mathcal X}$ consists of partial derivatives for each weight in the network which can easily number in millions. Thus, we will perform computation of derivatives in an efficient way for this training with gradient-based methods to be feasible. It turns out that we can perform this with time complexity that is linear with the number of weights. Furthermore, the development of sophisticated hardware for parallel computation (e.g. GPUs) has reduced training time by a significant factor, resulting in the viability of neural network models for pratical uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation on computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network can be modelled as a directed acyclic graph (DAG) of compute and parameter nodes that implements a function $f$ and can be extended to implement the calculation of the loss value for each training example and parameter values. Note that we can always perturb the value of the loss node at the current state of the graph (i.e. the current values of its nodes) by perturbing the values in each node. This results in perturbations flowing up to the final loss node. Assuming each computation is differentiable, we can compute the partial derivative of the loss with respect to each graph node. In this section, we will look at an algorithm for computing partial derivatives of computational graphs.\n",
    "\n",
    "Our computational DAGs will consist of **compute nodes** and **parameter nodes**. A compute node simply implements a function of values of nodes that are directed to it. This doesn't have to implement atomic functions. In fact, we will implement compute nodes at various levels of abstraction. The choice generally depends on design considerations. On the other hand, parameter nodes simply store values which ultimately determines the function $f.$ Hence, our goal is to modify the parameter node values that minimizes the empirical loss.\n",
    "\n",
    "We will use the fundamental property of DAGs that it has a **topological sorting**. Observe that a DAG always has a node that has no outgoing edges (just follow a path until the last node). For our purposes, we know that this is precisely the loss node of our graph. Then, we can perform [BFS](https://en.wikipedia.org/wiki/Breadth-first_search) starting with the loss node to determine the \"layers\" of the DAG, i.e. the order index of each node in the topological sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../img/compute.svg\n",
    "---\n",
    "width: 30%\n",
    "name: compute\n",
    "---\n",
    "Compute and parameter node as input to another compute node. Note that parameter nodes always have zero fan-in.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "To compute $f(\\mathbf x)$, the input $\\mathbf x$ is passed to the input nodes, then all compute nodes are executed up to the loss node following the directions and operations specified by the nodes and their edges. Note that node executions can occur asynchronously, e.g. in parallel. Moreover, each compute node is executed exactly once with the output value stored in the node. This will preserve the current network state for backward pass, and avoids any recomputation for nodes in the next layer. So assuming a node with $n$ inputs executes $n$ operations, then one forward pass takes $\\mathcal{O}(E)$ calculations were $E$ is the number of edges of the graph. For neural networks the number of edges is proportional to the number of weights and activation units of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "During backward pass gradients are categorized into two types: **local gradients** which are derivatives between adjacent nodes, and derivatives of the loss with respect to the node called **backpropagated gradients** since it is propagated backward from the loss node into the current node. Recall from above that our goal is to calculate the backpropagated gradient of the loss with respect to each parameter node.\n",
    "\n",
    "**BP proceeds recursively.** We will assume that the graph has been topologically sorted. For neural networks, there is no need to do this since we already have a natural ordering defined by the layers. \n",
    "For the base step, the gradient of the compute node for the loss is set to $1$ (i.e. its derivative with itself) and stored in memory. Iterating backwards in the sorted list of nodes, we know that the backpropagated gradient for each compute node $v$ that depends on $u$ is already stored. Moreover, all local gradients between $u$ and $v$ are [computable at runtime](https://en.wikipedia.org/wiki/Automatic_differentiation). Then, the backpropagated gradient with respect to node $u$ can be calculated using the chain rule:\n",
    "\n",
    "$$\n",
    "{\\frac{\\partial\\mathcal L}{\\partial u} } = \\sum_{ {v} } \\left( {{\\frac{\\partial\\mathcal L}{\\partial v}}} \\right) \\left( {{\\frac{\\partial{v}}{\\partial u}}} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that gradient type is distinguished by color in the figure below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../img/backward-1.svg\n",
    "---\n",
    "width: 80%\n",
    "name: backward-1\n",
    "---\n",
    "Computing the backpropagated gradient for a single node.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be visualized as gradients \"flowing\" to each network node from the loss node. Observe that the flow of gradients end on parameter nodes since these nodes have zero fan-in. Here the partial derivatives are evaluated on the current network state with values obtained during forward pass. Hence, forward pass should always precede backward pass. Moreover, as required by the algorithm all backpropagated gradients are stored in each compute node for use by the next layer. Memory can be released after the weights are updated. On the other hand, there is no need to store local gradients; these are computed as needed. \n",
    "    \n",
    "**Remark.** BP is a useful tool for understanding how derivatives flow through a model. This can be extremely helpful in reasoning about why some models are difficult to optimize. Classic examples are vanishing or exploding gradients as we go into deeper layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to compute each backpropagated gradient, we can implement this as a method `u.backward()` for each node `u` that sends the gradient of the loss with respect to `u` to all nodes that depend on it. Every backpropagated gradient that is sent to a node is accumulated in a sum. Hence, we have to zero it out at the start of every trainng step.\n",
    "\n",
    "```python\n",
    "class CompGraph:\n",
    "    # ...\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.graph.input_nodes.pass(inputs)\n",
    "        \n",
    "        for node in self.graph.nodes_toposorted():\n",
    "            node.forward()\n",
    "\n",
    "\n",
    "    def backward():\n",
    "        for node in self.nodes():\n",
    "            node.grad = 0\n",
    "\n",
    "        for node in reversed(self.graph.nodes_toposorted()):\n",
    "            node.backward()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of the algorithm which makes it the practical choice for training huge neural networks are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Modularity.** For neural networks, the dependence only on nodes belonging to the upper layer suggests a modularity in implementing neural networks, i.e. we can connect DAG subnetworks with possibly distinct network architectures by only connecting outermost nodes that are exposed between layers.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Efficiency.** Iterating over all nodes in the network during backward pass covers all the edges in the network with no edge counted twice. Assuming computing local gradients take constant time, then backward pass requires ${\\mathcal O}(E)$ computations. For neural networks this is proportional to the number of neurons and parameters (i.e. the network size). Furthermore, the chain rule between nodes on the same layer and nodes in the upper layer of a network can be implemented as matrix multiplication for which there exist [highly optimized implementations](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms). This is done in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training a neural net from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will attempt an implementation of an actual computational graph with nodes that implement `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, label='', _parents=(), _op=''):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.grad = 0\n",
    "        self._op = _op\n",
    "        self._parents = _parents\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Node(data={self.data}, label={self.label}, grad={self.grad})\"\n",
    "\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return BinaryOpNode(self, other, _op='+')\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return BinaryOpNode(self, other, _op='-')\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return BinaryOpNode(self, other, _op='*')\n",
    "\n",
    "    def __pow__(self, n):\n",
    "        assert isinstance(n, int)\n",
    "        assert n > 1\n",
    "        return PowOp(self, n)\n",
    "\n",
    "    def relu(self):\n",
    "        return ReLUNode(self)\n",
    "\n",
    "    def tanh(self):\n",
    "        return TanhNode(self)\n",
    "\n",
    "\n",
    "class BinaryOpNode(Node):\n",
    "    def __init__(self, x, y, _op='', label=None):\n",
    "\n",
    "        if not label:\n",
    "            label = f'({x.label}{_op}{y.label})'\n",
    "\n",
    "        super().__init__(..., label, (x, y), _op)\n",
    "        self.forward() # Note deferred initialization\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Pushing global gradient to child nodes \n",
    "        scaled with resp. local derivative (chain rule).\"\"\"\n",
    "        \n",
    "        if self._op == '+':\n",
    "            x, y = self._parents\n",
    "            x.grad += self.grad * 1.0\n",
    "            y.grad += self.grad * 1.0\n",
    "            x.backward()\n",
    "            y.backward()\n",
    "        \n",
    "        elif self._op == '-':\n",
    "            x, y = self._parents\n",
    "            x.grad += self.grad * 1.0\n",
    "            y.grad -= self.grad * 1.0\n",
    "            x.backward()\n",
    "            y.backward()\n",
    "        \n",
    "        elif self._op == '*':\n",
    "            x, y = self._parents\n",
    "            x.grad += self.grad * y.data\n",
    "            y.grad += self.grad * x.data\n",
    "            x.backward()\n",
    "            y.backward()\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Push values of parents to node.\"\"\"\n",
    "\n",
    "        if self._op == '+':\n",
    "            x, y = self._parents\n",
    "            self.data = x.data + y.data\n",
    "        \n",
    "        elif self._op == '-':\n",
    "            x, y = self._parents\n",
    "            self.data = x.data - y.data\n",
    "        \n",
    "        elif self._op == '*':\n",
    "            x, y = self._parents\n",
    "            self.data = x.data * y.data\n",
    "            \n",
    "\n",
    "class ReLUNode(Node):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(..., f'relu({x.label})', (x,), 'relu')\n",
    "        self.forward()\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Pushing global gradient to child node\n",
    "        scaled with local derivative (chain rule).\"\"\"\n",
    "        \n",
    "        x = self._parents[0]\n",
    "        x.grad += self.grad * float(x.data > 0)\n",
    "        x.backward()\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Push values of parents to node.\"\"\"\n",
    "\n",
    "        x = self._parents[0]\n",
    "        self.data = x.data * int(x.data > 0.0)\n",
    "\n",
    "\n",
    "class TanhNode(Node):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(..., f'tanh({x.label})', (x,), 'tanh')\n",
    "        self.forward()\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Pushing global gradient to child node\n",
    "        scaled with local derivative (chain rule).\"\"\"\n",
    "        \n",
    "        x = self._parents[0]\n",
    "        x.grad += self.grad * (1 - math.tanh(x.data)**2)\n",
    "        x.backward()\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Push values of parents to node.\"\"\"\n",
    "\n",
    "        x = self._parents[0]\n",
    "        self.data = math.tanh(x.data)\n",
    "        x.forward()\n",
    "\n",
    "\n",
    "class PowOp(Node):\n",
    "    def __init__(self, x, n):\n",
    "        self.n = n\n",
    "        super().__init__(..., f'({x.label})**{self.n}', (x,), 'pow')\n",
    "        self.forward()\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Pushing global gradient to child node\n",
    "        scaled with local derivative (chain rule).\"\"\"\n",
    "        \n",
    "        x = self._parents[0]\n",
    "        x.grad += self.grad * self.n * x.data ** (self.n-1)\n",
    "        x.backward()\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Push values of parents to node.\"\"\"\n",
    "\n",
    "        x = self._parents[0]\n",
    "        self.data = x.data ** self.n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize networks, we will use `graphviz`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\n",
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "def trace(root):\n",
    "    \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n",
    "\n",
    "    nodes, edges = set(), set()\n",
    "    \n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._parents:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def draw_graph(root):\n",
    "    \"\"\"Build diagram of computational graph.\"\"\"\n",
    "    \n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "\n",
    "        # Add node to graph    \n",
    "        uid = str(id(n))\n",
    "        dot.node(name=uid, label=f\"{n.label} | data={n.data:.3f} | grad={n.grad:.4f}\", shape='record')\n",
    "        \n",
    "        # If node is a result of computation, create op node for it\n",
    "        # Then, connect n to op node, \n",
    "        # e.g. if 5 = 2 + 3, then draw [5] as [+ -> 5].\n",
    "        if n._op:\n",
    "            dot.node(name=uid+n._op, label=n._op)\n",
    "            dot.edge(uid+n._op, uid)\n",
    "\n",
    "    for child, v in edges:\n",
    "        # Connect child to the op node of v\n",
    "        dot.edge(str(id(child)), str(id(v)) + v._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create graph for a dense unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 5.0.1 (20220820.1526)\n -->\n<!-- Pages: 1 -->\n<svg width=\"1128pt\" height=\"342pt\"\n viewBox=\"0.00 0.00 1128.00 342.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 338)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-338 1124,-338 1124,4 -4,4\"/>\n<!-- 4546178592 -->\n<g id=\"node1\" class=\"node\">\n<title>4546178592</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"672,-88.5 672,-157.5 818,-157.5 818,-88.5 672,-88.5\"/>\n<text text-anchor=\"middle\" x=\"745\" y=\"-142.3\" font-family=\"Times,serif\" font-size=\"14.00\">(((w₁*x₁)+(w₂*x₂))+b)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"672,-134.5 818,-134.5\"/>\n<text text-anchor=\"middle\" x=\"745\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"672,-111.5 818,-111.5\"/>\n<text text-anchor=\"middle\" x=\"745\" y=\"-96.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4546178736relu -->\n<g id=\"node4\" class=\"node\">\n<title>4546178736relu</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"881\" cy=\"-123\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"881\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">relu</text>\n</g>\n<!-- 4546178592&#45;&gt;4546178736relu -->\n<g id=\"edge10\" class=\"edge\">\n<title>4546178592&#45;&gt;4546178736relu</title>\n<path fill=\"none\" stroke=\"black\" d=\"M818.1,-123C826.97,-123 835.73,-123 843.73,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"843.77,-126.5 853.77,-123 843.77,-119.5 843.77,-126.5\"/>\n</g>\n<!-- 4546178592+ -->\n<g id=\"node2\" class=\"node\">\n<title>4546178592+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"609\" cy=\"-123\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"609\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 4546178592+&#45;&gt;4546178592 -->\n<g id=\"edge1\" class=\"edge\">\n<title>4546178592+&#45;&gt;4546178592</title>\n<path fill=\"none\" stroke=\"black\" d=\"M636.15,-123C643.82,-123 652.63,-123 661.85,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"661.98,-126.5 671.98,-123 661.98,-119.5 661.98,-126.5\"/>\n</g>\n<!-- 4546178736 -->\n<g id=\"node3\" class=\"node\">\n<title>4546178736</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"944,-88.5 944,-157.5 1120,-157.5 1120,-88.5 944,-88.5\"/>\n<text text-anchor=\"middle\" x=\"1032\" y=\"-142.3\" font-family=\"Times,serif\" font-size=\"14.00\">relu((((w₁*x₁)+(w₂*x₂))+b))</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"944,-134.5 1120,-134.5\"/>\n<text text-anchor=\"middle\" x=\"1032\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"944,-111.5 1120,-111.5\"/>\n<text text-anchor=\"middle\" x=\"1032\" y=\"-96.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4546178736relu&#45;&gt;4546178736 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4546178736relu&#45;&gt;4546178736</title>\n<path fill=\"none\" stroke=\"black\" d=\"M908.3,-123C915.84,-123 924.54,-123 933.76,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"933.93,-126.5 943.93,-123 933.93,-119.5 933.93,-126.5\"/>\n</g>\n<!-- 4545803984 -->\n<g id=\"node5\" class=\"node\">\n<title>4545803984</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-88.5 0,-157.5 86,-157.5 86,-88.5 0,-88.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-142.3\" font-family=\"Times,serif\" font-size=\"14.00\">w₁</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-134.5 86,-134.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=&#45;1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-111.5 86,-111.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-96.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4546178832* -->\n<g id=\"node9\" class=\"node\">\n<title>4546178832*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"149\" cy=\"-123\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"149\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 4545803984&#45;&gt;4546178832* -->\n<g id=\"edge9\" class=\"edge\">\n<title>4545803984&#45;&gt;4546178832*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M86,-123C94.48,-123 103.34,-123 111.61,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111.7,-126.5 121.7,-123 111.7,-119.5 111.7,-126.5\"/>\n</g>\n<!-- 4546178784 -->\n<g id=\"node6\" class=\"node\">\n<title>4546178784</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"212,-176.5 212,-245.5 298,-245.5 298,-176.5 212,-176.5\"/>\n<text text-anchor=\"middle\" x=\"255\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">(w₂*x₂)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"212,-222.5 298,-222.5\"/>\n<text text-anchor=\"middle\" x=\"255\" y=\"-207.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=6.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"212,-199.5 298,-199.5\"/>\n<text text-anchor=\"middle\" x=\"255\" y=\"-184.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4546179024+ -->\n<g id=\"node14\" class=\"node\">\n<title>4546179024+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"361\" cy=\"-167\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"361\" y=\"-163.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 4546178784&#45;&gt;4546179024+ -->\n<g id=\"edge13\" class=\"edge\">\n<title>4546178784&#45;&gt;4546179024+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M298,-193.23C308.13,-188.95 318.78,-184.44 328.34,-180.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"330,-183.49 337.84,-176.37 327.27,-177.05 330,-183.49\"/>\n</g>\n<!-- 4546178784* -->\n<g id=\"node7\" class=\"node\">\n<title>4546178784*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"149\" cy=\"-211\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"149\" y=\"-207.3\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 4546178784*&#45;&gt;4546178784 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4546178784*&#45;&gt;4546178784</title>\n<path fill=\"none\" stroke=\"black\" d=\"M176.24,-211C184.11,-211 193.04,-211 201.96,-211\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"201.98,-214.5 211.98,-211 201.98,-207.5 201.98,-214.5\"/>\n</g>\n<!-- 4546178832 -->\n<g id=\"node8\" class=\"node\">\n<title>4546178832</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"212,-88.5 212,-157.5 298,-157.5 298,-88.5 212,-88.5\"/>\n<text text-anchor=\"middle\" x=\"255\" y=\"-142.3\" font-family=\"Times,serif\" font-size=\"14.00\">(w₁*x₁)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"212,-134.5 298,-134.5\"/>\n<text text-anchor=\"middle\" x=\"255\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=&#45;1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"212,-111.5 298,-111.5\"/>\n<text text-anchor=\"middle\" x=\"255\" y=\"-96.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4546178832&#45;&gt;4546179024+ -->\n<g id=\"edge12\" class=\"edge\">\n<title>4546178832&#45;&gt;4546179024+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M298,-140.77C308.13,-145.05 318.78,-149.56 328.34,-153.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"327.27,-156.95 337.84,-157.63 330,-150.51 327.27,-156.95\"/>\n</g>\n<!-- 4546178832*&#45;&gt;4546178832 -->\n<g id=\"edge4\" class=\"edge\">\n<title>4546178832*&#45;&gt;4546178832</title>\n<path fill=\"none\" stroke=\"black\" d=\"M176.24,-123C184.11,-123 193.04,-123 201.96,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"201.98,-126.5 211.98,-123 201.98,-119.5 201.98,-126.5\"/>\n</g>\n<!-- 4442268432 -->\n<g id=\"node10\" class=\"node\">\n<title>4442268432</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-264.5 0,-333.5 86,-333.5 86,-264.5 0,-264.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-318.3\" font-family=\"Times,serif\" font-size=\"14.00\">w₂</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-310.5 86,-310.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-295.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=2.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-287.5 86,-287.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-272.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4442268432&#45;&gt;4546178784* -->\n<g id=\"edge7\" class=\"edge\">\n<title>4442268432&#45;&gt;4546178784*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M84.81,-264.47C97.9,-253.39 111.99,-241.47 123.54,-231.7\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"126.07,-234.14 131.44,-225.01 121.55,-228.8 126.07,-234.14\"/>\n</g>\n<!-- 4545803648 -->\n<g id=\"node11\" class=\"node\">\n<title>4545803648</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-69.5 86,-69.5 86,-0.5 0,-0.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\">x₁</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-46.5 86,-46.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-23.5 86,-23.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4545803648&#45;&gt;4546178832* -->\n<g id=\"edge11\" class=\"edge\">\n<title>4545803648&#45;&gt;4546178832*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M84.81,-69.53C97.9,-80.61 111.99,-92.53 123.54,-102.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"121.55,-105.2 131.44,-108.99 126.07,-99.86 121.55,-105.2\"/>\n</g>\n<!-- 4546178448 -->\n<g id=\"node12\" class=\"node\">\n<title>4546178448</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-176.5 0,-245.5 86,-245.5 86,-176.5 0,-176.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">x₂</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-222.5 86,-222.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-207.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=3.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-199.5 86,-199.5\"/>\n<text text-anchor=\"middle\" x=\"43\" y=\"-184.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4546178448&#45;&gt;4546178784* -->\n<g id=\"edge8\" class=\"edge\">\n<title>4546178448&#45;&gt;4546178784*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M86,-211C94.48,-211 103.34,-211 111.61,-211\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111.7,-214.5 121.7,-211 111.7,-207.5 111.7,-214.5\"/>\n</g>\n<!-- 4546179024 -->\n<g id=\"node13\" class=\"node\">\n<title>4546179024</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"424,-132.5 424,-201.5 546,-201.5 546,-132.5 424,-132.5\"/>\n<text text-anchor=\"middle\" x=\"485\" y=\"-186.3\" font-family=\"Times,serif\" font-size=\"14.00\">((w₁*x₁)+(w₂*x₂))</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"424,-178.5 546,-178.5\"/>\n<text text-anchor=\"middle\" x=\"485\" y=\"-163.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=5.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"424,-155.5 546,-155.5\"/>\n<text text-anchor=\"middle\" x=\"485\" y=\"-140.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4546179024&#45;&gt;4546178592+ -->\n<g id=\"edge6\" class=\"edge\">\n<title>4546179024&#45;&gt;4546178592+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M546.29,-145.26C556.32,-141.64 566.37,-138.01 575.37,-134.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"576.8,-137.97 585.02,-131.29 574.43,-131.39 576.8,-137.97\"/>\n</g>\n<!-- 4546179024+&#45;&gt;4546179024 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4546179024+&#45;&gt;4546179024</title>\n<path fill=\"none\" stroke=\"black\" d=\"M388.17,-167C395.8,-167 404.53,-167 413.53,-167\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"413.78,-170.5 423.78,-167 413.78,-163.5 413.78,-170.5\"/>\n</g>\n<!-- 4545803744 -->\n<g id=\"node15\" class=\"node\">\n<title>4545803744</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"442,-44.5 442,-113.5 528,-113.5 528,-44.5 442,-44.5\"/>\n<text text-anchor=\"middle\" x=\"485\" y=\"-98.3\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"442,-90.5 528,-90.5\"/>\n<text text-anchor=\"middle\" x=\"485\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=&#45;4.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"442,-67.5 528,-67.5\"/>\n<text text-anchor=\"middle\" x=\"485\" y=\"-52.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=0.0000</text>\n</g>\n<!-- 4545803744&#45;&gt;4546178592+ -->\n<g id=\"edge14\" class=\"edge\">\n<title>4545803744&#45;&gt;4546178592+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M528.33,-94.27C543.76,-99.83 560.96,-106.03 575.41,-111.25\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"574.34,-114.58 584.93,-114.68 576.71,-107.99 574.34,-114.58\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x10ef92dc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = Node(-1.0, label='w₁')\n",
    "w2 = Node( 2.0, label='w₂')\n",
    "b  = Node(-4.0, label='b' )\n",
    "\n",
    "x1 = Node( 1.0, label='x₁')\n",
    "x2 = Node( 3.0, label='x₂')\n",
    "\n",
    "z = w1 * x1 + w2 * x2 + b\n",
    "y = z.relu()\n",
    "draw_graph(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward pass.** Observe that all gradients check out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 5.0.1 (20220820.1526)\n -->\n<!-- Pages: 1 -->\n<svg width=\"1133pt\" height=\"342pt\"\n viewBox=\"0.00 0.00 1133.00 342.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 338)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-338 1129,-338 1129,4 -4,4\"/>\n<!-- 4546178592 -->\n<g id=\"node1\" class=\"node\">\n<title>4546178592</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"677,-88.5 677,-157.5 823,-157.5 823,-88.5 677,-88.5\"/>\n<text text-anchor=\"middle\" x=\"750\" y=\"-142.3\" font-family=\"Times,serif\" font-size=\"14.00\">(((w₁*x₁)+(w₂*x₂))+b)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"677,-134.5 823,-134.5\"/>\n<text text-anchor=\"middle\" x=\"750\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"677,-111.5 823,-111.5\"/>\n<text text-anchor=\"middle\" x=\"750\" y=\"-96.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=3.0000</text>\n</g>\n<!-- 4546178736relu -->\n<g id=\"node4\" class=\"node\">\n<title>4546178736relu</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"886\" cy=\"-123\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"886\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">relu</text>\n</g>\n<!-- 4546178592&#45;&gt;4546178736relu -->\n<g id=\"edge10\" class=\"edge\">\n<title>4546178592&#45;&gt;4546178736relu</title>\n<path fill=\"none\" stroke=\"black\" d=\"M823.1,-123C831.97,-123 840.73,-123 848.73,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"848.77,-126.5 858.77,-123 848.77,-119.5 848.77,-126.5\"/>\n</g>\n<!-- 4546178592+ -->\n<g id=\"node2\" class=\"node\">\n<title>4546178592+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"614\" cy=\"-123\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"614\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 4546178592+&#45;&gt;4546178592 -->\n<g id=\"edge1\" class=\"edge\">\n<title>4546178592+&#45;&gt;4546178592</title>\n<path fill=\"none\" stroke=\"black\" d=\"M641.15,-123C648.82,-123 657.63,-123 666.85,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"666.98,-126.5 676.98,-123 666.98,-119.5 666.98,-126.5\"/>\n</g>\n<!-- 4546178736 -->\n<g id=\"node3\" class=\"node\">\n<title>4546178736</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"949,-88.5 949,-157.5 1125,-157.5 1125,-88.5 949,-88.5\"/>\n<text text-anchor=\"middle\" x=\"1037\" y=\"-142.3\" font-family=\"Times,serif\" font-size=\"14.00\">relu((((w₁*x₁)+(w₂*x₂))+b))</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"949,-134.5 1125,-134.5\"/>\n<text text-anchor=\"middle\" x=\"1037\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"949,-111.5 1125,-111.5\"/>\n<text text-anchor=\"middle\" x=\"1037\" y=\"-96.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=3.0000</text>\n</g>\n<!-- 4546178736relu&#45;&gt;4546178736 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4546178736relu&#45;&gt;4546178736</title>\n<path fill=\"none\" stroke=\"black\" d=\"M913.3,-123C920.84,-123 929.54,-123 938.76,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"938.93,-126.5 948.93,-123 938.93,-119.5 938.93,-126.5\"/>\n</g>\n<!-- 4545803984 -->\n<g id=\"node5\" class=\"node\">\n<title>4545803984</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2.5,-88.5 2.5,-157.5 88.5,-157.5 88.5,-88.5 2.5,-88.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-142.3\" font-family=\"Times,serif\" font-size=\"14.00\">w₁</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2.5,-134.5 88.5,-134.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=&#45;1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2.5,-111.5 88.5,-111.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-96.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=3.0000</text>\n</g>\n<!-- 4546178832* -->\n<g id=\"node9\" class=\"node\">\n<title>4546178832*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"154\" cy=\"-123\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"154\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 4545803984&#45;&gt;4546178832* -->\n<g id=\"edge9\" class=\"edge\">\n<title>4545803984&#45;&gt;4546178832*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M88.59,-123C97.87,-123 107.62,-123 116.65,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"116.84,-126.5 126.84,-123 116.84,-119.5 116.84,-126.5\"/>\n</g>\n<!-- 4546178784 -->\n<g id=\"node6\" class=\"node\">\n<title>4546178784</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"217,-176.5 217,-245.5 303,-245.5 303,-176.5 217,-176.5\"/>\n<text text-anchor=\"middle\" x=\"260\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">(w₂*x₂)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"217,-222.5 303,-222.5\"/>\n<text text-anchor=\"middle\" x=\"260\" y=\"-207.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=6.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"217,-199.5 303,-199.5\"/>\n<text text-anchor=\"middle\" x=\"260\" y=\"-184.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=3.0000</text>\n</g>\n<!-- 4546179024+ -->\n<g id=\"node14\" class=\"node\">\n<title>4546179024+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"366\" cy=\"-167\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"366\" y=\"-163.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 4546178784&#45;&gt;4546179024+ -->\n<g id=\"edge13\" class=\"edge\">\n<title>4546178784&#45;&gt;4546179024+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M303,-193.23C313.13,-188.95 323.78,-184.44 333.34,-180.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"335,-183.49 342.84,-176.37 332.27,-177.05 335,-183.49\"/>\n</g>\n<!-- 4546178784* -->\n<g id=\"node7\" class=\"node\">\n<title>4546178784*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"154\" cy=\"-211\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"154\" y=\"-207.3\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 4546178784*&#45;&gt;4546178784 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4546178784*&#45;&gt;4546178784</title>\n<path fill=\"none\" stroke=\"black\" d=\"M181.24,-211C189.11,-211 198.04,-211 206.96,-211\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"206.98,-214.5 216.98,-211 206.98,-207.5 206.98,-214.5\"/>\n</g>\n<!-- 4546178832 -->\n<g id=\"node8\" class=\"node\">\n<title>4546178832</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"217,-88.5 217,-157.5 303,-157.5 303,-88.5 217,-88.5\"/>\n<text text-anchor=\"middle\" x=\"260\" y=\"-142.3\" font-family=\"Times,serif\" font-size=\"14.00\">(w₁*x₁)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"217,-134.5 303,-134.5\"/>\n<text text-anchor=\"middle\" x=\"260\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=&#45;1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"217,-111.5 303,-111.5\"/>\n<text text-anchor=\"middle\" x=\"260\" y=\"-96.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=3.0000</text>\n</g>\n<!-- 4546178832&#45;&gt;4546179024+ -->\n<g id=\"edge12\" class=\"edge\">\n<title>4546178832&#45;&gt;4546179024+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M303,-140.77C313.13,-145.05 323.78,-149.56 333.34,-153.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"332.27,-156.95 342.84,-157.63 335,-150.51 332.27,-156.95\"/>\n</g>\n<!-- 4546178832*&#45;&gt;4546178832 -->\n<g id=\"edge4\" class=\"edge\">\n<title>4546178832*&#45;&gt;4546178832</title>\n<path fill=\"none\" stroke=\"black\" d=\"M181.24,-123C189.11,-123 198.04,-123 206.96,-123\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"206.98,-126.5 216.98,-123 206.98,-119.5 206.98,-126.5\"/>\n</g>\n<!-- 4442268432 -->\n<g id=\"node10\" class=\"node\">\n<title>4442268432</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2.5,-264.5 2.5,-333.5 88.5,-333.5 88.5,-264.5 2.5,-264.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-318.3\" font-family=\"Times,serif\" font-size=\"14.00\">w₂</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2.5,-310.5 88.5,-310.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-295.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=2.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2.5,-287.5 88.5,-287.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-272.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=9.0000</text>\n</g>\n<!-- 4442268432&#45;&gt;4546178784* -->\n<g id=\"edge7\" class=\"edge\">\n<title>4442268432&#45;&gt;4546178784*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M88.29,-264.47C101.92,-253.21 116.61,-241.07 128.54,-231.21\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"130.84,-233.85 136.32,-224.79 126.38,-228.46 130.84,-233.85\"/>\n</g>\n<!-- 4545803648 -->\n<g id=\"node11\" class=\"node\">\n<title>4545803648</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-69.5 91,-69.5 91,-0.5 0,-0.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\">x₁</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-46.5 91,-46.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=1.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-23.5 91,-23.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=&#45;3.0000</text>\n</g>\n<!-- 4545803648&#45;&gt;4546178832* -->\n<g id=\"edge11\" class=\"edge\">\n<title>4545803648&#45;&gt;4546178832*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M88.29,-69.53C101.92,-80.79 116.61,-92.93 128.54,-102.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"126.38,-105.54 136.32,-109.21 130.84,-100.15 126.38,-105.54\"/>\n</g>\n<!-- 4546178448 -->\n<g id=\"node12\" class=\"node\">\n<title>4546178448</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2.5,-176.5 2.5,-245.5 88.5,-245.5 88.5,-176.5 2.5,-176.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">x₂</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2.5,-222.5 88.5,-222.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-207.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=3.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2.5,-199.5 88.5,-199.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-184.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=6.0000</text>\n</g>\n<!-- 4546178448&#45;&gt;4546178784* -->\n<g id=\"edge8\" class=\"edge\">\n<title>4546178448&#45;&gt;4546178784*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M88.59,-211C97.87,-211 107.62,-211 116.65,-211\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"116.84,-214.5 126.84,-211 116.84,-207.5 116.84,-214.5\"/>\n</g>\n<!-- 4546179024 -->\n<g id=\"node13\" class=\"node\">\n<title>4546179024</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"429,-132.5 429,-201.5 551,-201.5 551,-132.5 429,-132.5\"/>\n<text text-anchor=\"middle\" x=\"490\" y=\"-186.3\" font-family=\"Times,serif\" font-size=\"14.00\">((w₁*x₁)+(w₂*x₂))</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"429,-178.5 551,-178.5\"/>\n<text text-anchor=\"middle\" x=\"490\" y=\"-163.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=5.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"429,-155.5 551,-155.5\"/>\n<text text-anchor=\"middle\" x=\"490\" y=\"-140.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=3.0000</text>\n</g>\n<!-- 4546179024&#45;&gt;4546178592+ -->\n<g id=\"edge6\" class=\"edge\">\n<title>4546179024&#45;&gt;4546178592+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M551.29,-145.26C561.32,-141.64 571.37,-138.01 580.37,-134.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"581.8,-137.97 590.02,-131.29 579.43,-131.39 581.8,-137.97\"/>\n</g>\n<!-- 4546179024+&#45;&gt;4546179024 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4546179024+&#45;&gt;4546179024</title>\n<path fill=\"none\" stroke=\"black\" d=\"M393.17,-167C400.8,-167 409.53,-167 418.53,-167\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"418.78,-170.5 428.78,-167 418.78,-163.5 418.78,-170.5\"/>\n</g>\n<!-- 4545803744 -->\n<g id=\"node15\" class=\"node\">\n<title>4545803744</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"447,-44.5 447,-113.5 533,-113.5 533,-44.5 447,-44.5\"/>\n<text text-anchor=\"middle\" x=\"490\" y=\"-98.3\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"447,-90.5 533,-90.5\"/>\n<text text-anchor=\"middle\" x=\"490\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">data=&#45;4.000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"447,-67.5 533,-67.5\"/>\n<text text-anchor=\"middle\" x=\"490\" y=\"-52.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad=3.0000</text>\n</g>\n<!-- 4545803744&#45;&gt;4546178592+ -->\n<g id=\"edge14\" class=\"edge\">\n<title>4545803744&#45;&gt;4546178592+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M533.33,-94.27C548.76,-99.83 565.96,-106.03 580.41,-111.25\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"579.34,-114.58 589.93,-114.68 581.71,-107.99 579.34,-114.58\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x10ef37040>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad = 3.0\n",
    "y.backward()\n",
    "draw_graph(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a neural network which computes a function $f,$ we can think of wrapping the original network in a computational graph that adds an additional input node for the target values, and an additional compute node for the loss function which takes in the network output $f(\\mathbf x)$ and target $y$ given an input output pair $(\\mathbf x, y)$ in a training step. Note that we can use any algorithm to update network weights. The following training loop implements [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) with $\\epsilon = 0.01.$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 200\n",
    "x = np.linspace(-1, 1, N)\n",
    "y = np.sin(np.pi*x) + 0.1*np.random.normal(size=N)\n",
    "plt.plot(x, y);\n",
    "\n",
    "dataset = list(zip(list(x), list(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a fully-connected neural network with 1-3-2-1 units with ReLU activations in the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod, abstractproperty\n",
    "\n",
    "\n",
    "class Module(ABC):\n",
    "    def __init__(self, module_name):\n",
    "        self.module_name = module_name\n",
    "\n",
    "    @abstractproperty\n",
    "    def parameters(self):\n",
    "        self.parameters = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def _init_params(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: list):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            param.grad = 0\n",
    "\n",
    "    \n",
    "class DenseLayer(Module):\n",
    "    def __init__(self, n_in, n_out, layer_index=0, activation=''):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.i = layer_index\n",
    "        self.act = activation\n",
    "        super().__init__('DenseLayer')\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self._init_params()\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [w for layer in self.weights for w in layer] + self.biases\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = []\n",
    "        for k in range(self.n_out):\n",
    "            w = self.weights[k]\n",
    "            o = self.biases[k]\n",
    "            for j in range(self.n_in):\n",
    "                o += x[j] * w[j]; o.label = f'o[{self.i},{k}]'\n",
    "            if self.act == 'relu':\n",
    "                o = o.relu() \n",
    "            elif self.act == 'tanh':\n",
    "                o = o.tanh()\n",
    "            else:\n",
    "                pass # Identity activation\n",
    "            out.append(o)\n",
    "        return out\n",
    "\n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize network weights randomly.\"\"\"\n",
    "\n",
    "        for k in range(self.n_out):\n",
    "            b = Node(0.0, label=f\"b[{self.i},{k}]\")\n",
    "            self.biases.append(b)\n",
    "\n",
    "            w = []\n",
    "            for j in range(self.n_in):\n",
    "                w_init = np.random.random()\n",
    "                w.append(Node(w_init, f\"w[{self.i},{j},{k}]\"))\n",
    "            self.weights.append(w)\n",
    "            \n",
    "\n",
    "class MLPRegressor(Module):\n",
    "    def __init__(self, shape, activation='tanh'):\n",
    "        self.shape = shape\n",
    "        self.activation = activation\n",
    "        super().__init__('MLP')\n",
    "        \n",
    "        self.layers = []\n",
    "        self.pred_node = None\n",
    "        self._init_params()\n",
    "\n",
    "    def __call__(self, x: list):\n",
    "        x = [Node(x[j], label=f\"x[{j}]\") for j in range(len(x))]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        self.pred_node = x[0]\n",
    "        return x[0].data\n",
    "\n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize layers and layer parameters.\"\"\" \n",
    "\n",
    "        # Each layer calls _init_params() internally.\n",
    "        for i in range(1, len(self.shape)):\n",
    "            activation = '' if i == len(self.shape) - 1 else self.activation\n",
    "            layer = DenseLayer(\n",
    "                self.shape[i-1], \n",
    "                self.shape[i], \n",
    "                i, activation\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sgd(model, eps, bs=1):\n",
    "    for p in model.parameters:\n",
    "        p.data -= (eps / bs) * p.grad \n",
    "\n",
    "def loss_fn(out, y):\n",
    "    return (out - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c5542f004944dda234a2a0eaa969b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21280751270376333\n",
      "0.09142369741773052\n",
      "0.07809698958828946\n",
      "0.0748317983843161\n",
      "0.07282472928448087\n",
      "0.06949245913195917\n",
      "0.06064416446360525\n",
      "0.04966760009795683\n",
      "0.04427226220746054\n",
      "0.04089875814018116\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = MLPRegressor([1, 3, 2, 1])\n",
    "\n",
    "hist = []\n",
    "for _ in tqdm(range(10000)):\n",
    "    shuffled = random.sample(dataset, N)\n",
    "    avg_loss = 0.0\n",
    "    for i in range(N // 10):\n",
    "        minibatch = shuffled[i * 10: (i + 1) * 10]  # Get minibatch of size 10\n",
    "        batch_loss = 0\n",
    "        for x_, y_ in minibatch:\n",
    "            y = Node(y_, label='y')\n",
    "\n",
    "            model([x_])\n",
    "            out = model.pred_node\n",
    "            model.zero_grad()\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.grad = 1.0\n",
    "            loss.backward()\n",
    "            apply_sgd(model, eps=0.001, bs=10)\n",
    "            batch_loss += loss.data\n",
    "        avg_loss += batch_loss / 10\n",
    "    avg_loss = avg_loss / (N // 10)\n",
    "    hist.append(avg_loss)\n",
    "    if (_ % 1000) == 0:\n",
    "        print(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP equations for MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "Source:<br>**Figure 1** of {cite}`0483bd9444a348c8b59d54a190839ec9`\n",
    "```\n",
    "```{figure} ../../img/deep-nns.png\n",
    "---\n",
    "width: 40em\n",
    "---\n",
    "\n",
    "**Figure 1** of {cite}`0483bd9444a348c8b59d54a190839ec9` should hopefully make sense after reading this article. This figure shows (left) forward pass for a multilayer neural network, and (right) backward pass for the same network.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we solve the BP equations for a densely connected neural network. This can be useful for looking gradient flow analytically. As shown in the above figure, a multilayer neural network can be modelled as a computational DAG. Note that we use row vectors to represent input and output in the equations below. Computation performed by the network at layer $t$ can be written in two steps as:\n",
    "\n",
    "* ${\\boldsymbol{\\mathsf{y}}}^{[l]} = \\boldsymbol{{\\mathsf{x}}}^{[l-1]}{\\boldsymbol{\\mathsf{W}}}^{[l]} + {\\boldsymbol{\\mathsf{b}}}^{[l]}$ \n",
    "* ${\\boldsymbol{\\mathsf{x}}}^{[l]} = \\varphi({\\boldsymbol{\\mathsf{y}}}^{[l]})$ \n",
    "\n",
    "Here we use row vectors for layer inputs and outputs. The following equations are obtained by simply matching input and output shapes, then trying to figure out the entries of the right hand side matrix by tracking node dependencies. For the compute nodes in the current layer, the BP equations are:\n",
    "    \n",
    "$$\n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {\\boldsymbol{\\mathsf{x}}}^{[l]}} \n",
    "= \n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {\\boldsymbol{\\mathsf{y}}}^{[l+1]}} \n",
    "\\dfrac{\\partial {\\boldsymbol{\\mathsf{y}}}^{[l+1]}}{\\partial {\\boldsymbol{\\mathsf{x}}}^{[l]}} \n",
    "=\n",
    "\\dfrac{\\partial \\mathcal L}{{\\partial \\boldsymbol{\\mathsf{y}}}^{[l+1]}}\n",
    "\\boldsymbol{\\mathsf{W}}^{[l+1]\\top}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {{\\boldsymbol{\\mathsf{y}}}}^{[l]}} \n",
    "= \n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {\\boldsymbol{\\mathsf{x}}}^{[l]}} \n",
    "\\dfrac{\\partial {\\boldsymbol{\\mathsf{x}}}^{[l]}}{\\partial {\\boldsymbol{\\mathsf{y}}}^{[l]}} \n",
    "= \n",
    "\\dfrac{\\partial \\mathcal L}{{\\partial \\boldsymbol{\\mathsf{x}}}^{[l]}}\n",
    "{{\\boldsymbol{\\mathsf{J}}}_\\varphi}{^{[l]}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the Jacobian matrix is defined as $\\left({\\boldsymbol{\\mathsf{J}}}_{\\varphi}\\right)_{ij} = \\frac{\\partial {\\mathsf x}_i}{\\partial {\\mathsf y}_j}$. For commonly used activations, i.e. those which are applied entrywise on vector inputs, the Jacobian reduces to a diagonal matrix. The backpropagated gradients for parameter nodes are:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {\\boldsymbol {\\mathsf W}}^{[l]}} \n",
    "= \n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {{\\boldsymbol{\\mathsf y}}}^{[l]}} \n",
    "\\dfrac{\\partial {{\\boldsymbol{\\mathsf y}}}^{[l]}}{\\partial {\\boldsymbol {\\mathsf W}}^{[l]}} \n",
    "= \n",
    "\\boldsymbol{\\mathsf{x}}^{[l-1]\\top}\n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {{\\boldsymbol{\\mathsf y}}}^{[l]}} \n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {\\boldsymbol{\\mathsf{b}}_{j}}^{[l]}} \n",
    "= \n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {{\\boldsymbol{\\mathsf y}}}^{[l]}}\n",
    "\\dfrac{\\partial {{\\boldsymbol{\\mathsf y}}}^{[l]}}{\\partial {\\boldsymbol{\\mathsf{b}}}^{[l]}} \n",
    "= \n",
    "\\dfrac{\\partial \\mathcal L}{\\partial {{\\boldsymbol{\\mathsf y}}}^{[l]}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagated gradients for compute nodes have to be stored until weights are updated. Observe that derivatives of the compute nodes of layer $l+1$ are retrieved to compute gradients in layer $l.$ On the other hand, the local gradients ${\\boldsymbol {\\mathsf J}}_\\varphi$ are computed using autodifferentiation and are evaluated at the current network state based on values obtained at forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../img/jacobian.svg\n",
    "---\n",
    "name: jacobian\n",
    "width: 75%\n",
    "---\n",
    "Deriving the equations by matching shapes of derivatives as matrices. Trick is to put the incoming gradients as input (blue), and current gradients as output (gray). Figure out the transformation in between containing the local gradients (red).\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Autodiff\n",
    "\n",
    "The above equations and process is demonstrated in the following computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.2\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 01:49:58.433892: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-28 01:49:58.434006: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# Input and weight init.\n",
    "x0 = tf.Variable(tf.random.normal(shape=(1, 4)))\n",
    "W1 = tf.Variable(tf.random.normal(shape=(4, 3)))\n",
    "b1 = tf.Variable(tf.random.normal(shape=(1, 3)))\n",
    "W2 = tf.Variable(tf.random.normal(shape=(3, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(weights)\n",
      "TF autodiff:\n",
      "[[-0.02551604  0.00109216  0.02442426]\n",
      " [ 0.01623343 -0.00069484 -0.01553884]\n",
      " [ 0.01366194 -0.00058477 -0.01307738]\n",
      " [-0.02030785  0.00086924  0.01943892]]\n",
      "\n",
      "BP equations:\n",
      "[[-0.02551604  0.00109216  0.02442409]\n",
      " [ 0.01623344 -0.00069484 -0.01553873]\n",
      " [ 0.01366195 -0.00058477 -0.01307729]\n",
      " [-0.02030786  0.00086924  0.01943879]]\n",
      "\n",
      "(biases)\n",
      "TF autodiff:  [[-0.02342275  0.00100256  0.02242054]]\n",
      "BP equations: [[-0.02342275  0.00100256  0.02242038]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 01:49:59.067667: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-08-28 01:49:59.068169: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Forward:\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y1 = tf.matmul(x0, W1) + b1\n",
    "    x1 = tf.keras.activations.softmax(y1)\n",
    "\n",
    "    y2 = tf.matmul(x1, W2)\n",
    "    L = tf.reduce_sum(y2**2)\n",
    "\n",
    "# Backward:\n",
    "# (1) ∂L/∂x[t] = ∂L/∂y[t+1] W[t+1]\n",
    "# Fetch ∂L/∂y[t+1] from upper layer.\n",
    "L_x1 = tf.matmul(tape.gradient(L, y2), tf.transpose(W2))\n",
    "\n",
    "# (2) ∂L/∂y[t] = ∂L/∂x[t] J[t]\n",
    "# Compute local gradients using autodiff.\n",
    "L_y1 = tf.matmul(L_x1, tf.reshape(tape.jacobian(x1, y1), (3, 3)))\n",
    "\n",
    "# (3) ∂L/∂W[t] = x[t-1]T ∂L/∂y[t]\n",
    "L_W1 = tf.matmul(tf.transpose(x0), L_y1)\n",
    "\n",
    "# (4) ∂L/∂b[t] = ∂L/∂y[t]\n",
    "L_b1 = L_y1\n",
    "\n",
    "print(\"(weights)\")\n",
    "print(\"TF autodiff:\")\n",
    "print(tape.gradient(L, W1).numpy())\n",
    "print(\"\\nBP equations:\")\n",
    "print(L_W1.numpy())\n",
    "\n",
    "print(\"\\n(biases)\")\n",
    "print(\"TF autodiff: \", tape.gradient(L, b1).numpy())\n",
    "print(\"BP equations:\", L_b1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Autodifferentiation with PyTorch `autograd`\n",
    "\n",
    "The `autograd` package allows automatic differentiation by building computational graphs on the fly every time we pass data through our model. Autograd tracks which data combined through which operations to produce the output. This allows us to take derivatives over ordinary imperative code. This functionality is consistent with the memory and time requirements outlined in above for BP.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Backward for scalars.** Let $y = \\mathbf x^\\top \\mathbf x = \\sum_i {x_i}^2.$ In this example, we initialize a tensor `x` which initially has no gradient. Calling backward on `y` results in gradients being stored on the leaf tensor `x`.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- x = torch.arange(4, dtype=torch.float, requires_grad=True)\n",
    "y = x.T @ x \n",
    "\n",
    "y.backward() \n",
    "(x.grad == 2*x).all() -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Backward for vectors.** Let $\\mathbf y = g(\\mathbf x)$ and let $\\mathbf v$ be a vector having the same length as $\\mathbf y.$ Then `y.backward(v)` implements   \n",
    "\n",
    "$$\\sum_i v_i \\left(\\frac{\\partial y_i}{\\partial x_j}\\right)$$ \n",
    "  \n",
    "resulting in a vector of same length as `x` that is stored in `x.grad`. Note that the terms on the right are the local gradients in backprop. Hence, if `v` contains backpropagated gradients of nodes that depend on `y`, then this operation gives us the backpropagated gradients with respect to `x`, i.e. setting $v_i = \\frac{\\partial \\mathcal{L} }{\\partial y_i}$ gives us the vector $\\frac{\\partial \\mathcal{L} }{\\partial x_j}.$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- x = torch.rand(size=(4,), dtype=torch.float, requires_grad=True)\n",
    "v = torch.rand(size=(2,), dtype=torch.float)\n",
    "y = x[:2]\n",
    "\n",
    "# Computing the Jacobian by hand\n",
    "J = torch.tensor(\n",
    "    [[1, 0, 0, 0],\n",
    "    [0, 1, 0, 0]], dtype=torch.float\n",
    ")\n",
    "\n",
    "# Confirming the above formula\n",
    "y.backward(v)\n",
    "(x.grad == v @ J).all() -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Locally disabling gradient tracking.** Disabling gradient computation is useful when computing values, e.g. accuracy, whose gradients will not be backpropagated into the network. To stop PyTorch from building computational graphs, we can put the code inside a `torch.no_grad()` context or inside a function with a `@torch.no_grad()` decorator.\n",
    "\n",
    "Another technique is to use the `.detach()` method which returns a new tensor detached from the current graph but shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bb4561ca8d8b7b3a7bc7514080b6e7dab3824c9a0b3ef748f0e5ff42277ee64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
