{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deployment\n",
    "\n",
    "batch (offline)\n",
    "- run regularly\n",
    "\n",
    "online\n",
    "- up running all the time\n",
    " - web service (http requests, get back prediction)\n",
    " - streaming (stream of events, model services listening for events on the stream and react to this)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Attribution: These are notes for [Module 4](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/04-deployment) of the [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp). The MLOps Zoomcamp is a free course from [DataTalks.Club](https://github.com/DataTalksClub).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch mode\n",
    "- regularly (every day, hourly, monthly, etc)\n",
    "- database of all data (pull data from db -> apply model)\n",
    "- scoring job\n",
    "- daily: get all data from yesterday\n",
    "- hourly: get all data from previous hour\n",
    "- and so on\n",
    "- write to predictions db\n",
    "- something can read from predictions db and react on this predictions, e.g. report\n",
    "- marketing related tasks, e.g. churn prediction\n",
    "- no need to know that user is about to churn immediately\n",
    "- (i.e. churning does not occur at a small time interval)\n",
    "\n",
    "web service\n",
    "- contains model\n",
    "- ride duration prediction\n",
    "- backend sends info to service (pu location id, dropoff, time of day, etc)\n",
    "- send back prediction, then passed to user\n",
    "- needs to be up all the time.\n",
    "- user uses app, checks ride duration, and decides whether to hire taxi or not.\n",
    "- for this decision we can't wait five minutes, need this immediately.\n",
    "\n",
    "streaming\n",
    "- producer and consumers\n",
    "- producer pushes event to event stream and consumers wil read from this stream.\n",
    "- and react to these events. \n",
    "- recall web service: 1-1 relationship (explicit connection between user and service)\n",
    "- 1-many  / many - many. \n",
    "- user -> producer=backend -> send event containing all info about ride ->\n",
    "     services will react on this event\n",
    "\n",
    "- e.g. one consuming service predict tip -> send push notif to user asking for tip.\n",
    "- duration prediction (web service) = okay pred\n",
    "- streaming service, better ride duration prediction -> update prediction. \n",
    "- only implicit connection, we dont know which consumer will react, how many\n",
    "- example: content moderation\n",
    "    - user -> video -> event -> C1 (copyright)\n",
    "                             -> C2 (NSFW)        -> prediction stream -> decision service\n",
    "                             -> C2 (violence)          \n",
    "\n",
    "- can be scaled to infinitely many services or models (in principle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a model as a web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pipenv install scikit-learn==1.0.2 flask --python=3.9\n",
    "pipenv shell\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"duration\": 12.265893072879651\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "ride = [{\n",
    "    'VendorID': 2,\n",
    "    'store_and_fwd_flag': 'N',\n",
    "    'RatecodeID': 1.0,\n",
    "    'PULocationID': 130,\n",
    "    'DOLocationID': 205,\n",
    "    'passenger_count': 5.0,\n",
    "    'trip_distance': 3.66,\n",
    "    'fare_amount': 14.0,\n",
    "    'extra': 0.5,\n",
    "    'mta_tax': 0.5,\n",
    "    'tip_amount': 10.0,\n",
    "    'tolls_amount': 0.0,\n",
    "    'ehail_fee': None,\n",
    "    'improvement_surcharge': 0.3,\n",
    "    'total_amount': 25.3,\n",
    "    'payment_type': 1.0,\n",
    "    'trip_type': 1.0,\n",
    "    'congestion_surcharge': 0.0\n",
    "}]\n",
    "\n",
    "\n",
    "host = 'http://192.168.254.180:9696'\n",
    "url = f'{host}/predict'\n",
    "response = requests.post(url, json=ride)\n",
    "result = response.json()\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pipenv install --dev requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get model from mlflow registry using run id\n",
    "problematic if server goes down\n",
    "we become dependent on tracking server.\n",
    "-> go directly to artifact root. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipenv install --dev pipenv-setup\n",
    "pipenv-setup sync"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('prefect')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "345667e5aa1587f3f99ee9b59f54516e1e87c18189e42869047387925519f8a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
