{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration and ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Ongoing&color=orange)\n",
    "\n",
    "<!-- Place this tag where you want the button to render. -->\n",
    "<a class=\"github-button\" href=\"https://github.com/particle1331/steepest-ascent\" data-color-scheme=\"no-preference: dark; light: light; dark: dark;\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star particle1331/steepest-ascent on GitHub\">Star</a>\n",
    "<!-- Place this tag in your head or just before your close body tag. -->\n",
    "<script async defer src=\"https://buttons.github.io/buttons.js\"></script> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous module, we learned about experiment tracking and model registry.\n",
    "In particular, we discussed how to get a candidate model and promote it from staging to production.\n",
    "In this module, we learn how to automate this process, and having this scheduled with workflow orchestration &mdash; specifically, with [Prefect 2.0](https://orion-docs.prefect.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect allows us to programatically author, schedule, and monitor workflows. Prefect allows us to minimize time on **negative engineering**, i.e. coding against all possible causes of failure. This is a Sisyphean task as there are endless ways that elements of a data pipeline can fail. In practical terms, Prefect provides tools such as retries, concurrency, logging, a nice UI, tracking dependencies, a database, caching and serialization, parameterization of scheduled tasks, and more. As we shall see later, this adds observability to the whole data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "⚠️ **Attribution:** These are notes for [Module 3](https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/03-orchestration) of the [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp). The MLOps Zoomcamp is a free course from [DataTalks.Club](https://github.com/DataTalksClub).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefect flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **flow** in Prefect is simply a Python function. This consists of **tasks** which can be thought of as the atom of observability in Prefect. In practice, to create a flow, you simply convert functions that make it up into tasks. Consider the following example from the [*Getting Started with Prefect 2.0*](https://www.prefect.io/guide/blog/getting-started-prefect-2/#Makingourflowsbetterwithtasks) blog post. Here, we simulate getting data from an unreliable API, augmenting the fetched data, and writing the resulting data into a database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:26:54.662 | INFO    | prefect.engine - Created flow run 'juicy-platypus' for flow 'pipeline'\n",
      "17:26:54.664 | INFO    | Flow run 'juicy-platypus' - Using task runner 'ConcurrentTaskRunner'\n",
      "17:26:54.671 | WARNING | Flow run 'juicy-platypus' - No default storage is configured on the server. Results from this flow run will be stored in a temporary directory in its runtime environment.\n",
      "17:26:54.708 | INFO    | Flow run 'juicy-platypus' - Created task run 'call_unreliable_api-48f93715-0' for task 'call_unreliable_api'\n",
      "17:26:54.729 | INFO    | Flow run 'juicy-platypus' - Created task run 'augment_data-505b3e0c-0' for task 'augment_data'\n",
      "17:26:54.741 | ERROR   | Task run 'call_unreliable_api-48f93715-0' - Encountered exception during execution:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/engine.py\", line 798, in orchestrate_task_run\n",
      "    result = await run_sync_in_worker_thread(task.fn, *args, **kwargs)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/utilities/asyncio.py\", line 54, in run_sync_in_worker_thread\n",
      "    return await anyio.to_thread.run_sync(call, cancellable=True)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/9vsvd9252_349lsng_5gc_jw0000gn/T/ipykernel_38107/3061218592.py\", line 10, in call_unreliable_api\n",
      "    raise Exception(\"Our unreliable service failed.\")\n",
      "Exception: Our unreliable service failed.\n",
      "17:26:54.766 | INFO    | Flow run 'juicy-platypus' - Created task run 'write_to_database-d58974ba-0' for task 'write_to_database'\n",
      "17:26:54.782 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Received non-final state 'AwaitingRetry' when proposing final state 'Failed' and will attempt to run again...\n",
      "17:26:54.804 | ERROR   | Task run 'call_unreliable_api-48f93715-0' - Encountered exception during execution:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/engine.py\", line 798, in orchestrate_task_run\n",
      "    result = await run_sync_in_worker_thread(task.fn, *args, **kwargs)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/utilities/asyncio.py\", line 54, in run_sync_in_worker_thread\n",
      "    return await anyio.to_thread.run_sync(call, cancellable=True)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/9vsvd9252_349lsng_5gc_jw0000gn/T/ipykernel_38107/3061218592.py\", line 10, in call_unreliable_api\n",
      "    raise Exception(\"Our unreliable service failed.\")\n",
      "Exception: Our unreliable service failed.\n",
      "17:26:54.822 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Received non-final state 'AwaitingRetry' when proposing final state 'Failed' and will attempt to run again...\n",
      "17:26:54.857 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Finished in state Completed()\n",
      "17:26:54.885 | INFO    | Task run 'augment_data-505b3e0c-0' - Finished in state Completed()\n",
      "17:26:54.915 | INFO    | Task run 'write_to_database-d58974ba-0' - Finished in state Completed()\n",
      "17:26:54.925 | INFO    | Flow run 'juicy-platypus' - Finished in state Completed('All states completed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote {'data': 42, 'message': '0'} to database successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Completed(message='All states completed.', type=COMPLETED, result=[Completed(message=None, type=COMPLETED, result={'data': 42, 'message': '0'}, task_run_id=72365071-5085-4158-a391-eeddede5ca75), Completed(message=None, type=COMPLETED, result={'data': 42, 'message': '0'}, task_run_id=e7028889-7db0-435f-896c-03e6ff7bb733), Completed(message=None, type=COMPLETED, result='Success!', task_run_id=9ec022e6-39e8-41ea-98b2-b0a14df9eaff)], flow_run_id=be507fa9-289c-4ff1-a7eb-fd2aa29ab969)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from prefect import flow, task \n",
    "\n",
    "\n",
    "@task(retries=3)\n",
    "def call_unreliable_api():\n",
    "    choices = [{\"data\": 42}, \"Failure\"]\n",
    "    res = random.choice(choices)\n",
    "    if res == \"Failure\":\n",
    "        raise Exception(\"Our unreliable service failed.\")\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "@task\n",
    "def augment_data(data: dict, msg: str):\n",
    "    data[\"message\"] = msg\n",
    "    return data\n",
    "\n",
    "@task\n",
    "def write_to_database(data: dict):\n",
    "    print(f\"Wrote {data} to database successfully!\")\n",
    "    return \"Success!\"\n",
    "\n",
    "@flow \n",
    "def pipeline(msg: str):\n",
    "    api_result = call_unreliable_api()\n",
    "    augmented_data = augment_data(data=api_result, msg=msg)\n",
    "    write_to_database(augmented_data)\n",
    "\n",
    "\n",
    "pipeline(0) # Augment data with zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefect Orion UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this failed before pushing through. We can start the UI by calling `prefect orion start` in any directory (`.prefect` is saved in the system's root directory). This starts the Prefect Orion server in port 4200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "❯ prefect orion start\n",
    "Starting...\n",
    "\n",
    " ___ ___ ___ ___ ___ ___ _____    ___  ___ ___ ___  _  _\n",
    "| _ \\ _ \\ __| __| __/ __|_   _|  / _ \\| _ \\_ _/ _ \\| \\| |\n",
    "|  _/   / _|| _|| _| (__  | |   | (_) |   /| | (_) | .` |\n",
    "|_| |_|_\\___|_| |___\\___| |_|    \\___/|_|_\\___\\___/|_|\\_|\n",
    "\n",
    "Configure Prefect to communicate with the server with:\n",
    "\n",
    "    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n",
    "\n",
    "Check out the dashboard at http://127.0.0.1:4200\n",
    "\n",
    "\n",
    "\n",
    "INFO:     Started server process [20557]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://127.0.0.1:4200 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We navigate around the UI to find the `pipeline` flow and its most recent which, as we have seen in the logs, was able to complete its execution. Here we see that this flow started on `2022/06/10 11:05:51 PM` and ended on `2022/06/10 11:05:52 PM`. We also see the logs has the details of the exception when the API call failed. In the second tab, we can see the tasks that make up this flow. There is also the subflow tab which shows that we can call flows from a parent flow.\n",
    "\n",
    "```{figure} ../../../img/hello-world-2.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more interesting features of the dashboard is **Radar** on the right. This shows the dependence between tasks. Notice the linear dependence of the tasks, e.g. `write_to_database` depends on `augment_data` task but not on `call_unreliable_api`. Hovering on the tasks show the backward and forward data dependencies. Having tasks arranged in concentric circles allow for a heirarchy of dependence. Note that the runtime for each task is also conveniently displayed.\n",
    "\n",
    "```{figure} ../../../img/hello-world-1.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "Finally, let us look at a flow which failed to complete all its tasks. Here all calls to the API failed despite the retries. The radar plot nicely shows where the flow has failed. This is really useful, especially when we have a dozens task and multiple subflows happening in our data pipeline.\n",
    "\n",
    "\n",
    "```{figure} ../../../img/hello-world-3.png\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Remark.** Note also that geometrically there is more space available to grow the dependence tree due to nodes being farther apart as we move radially with a fixed angle, this also allows Radar to minimize edge crossing by combining radial and circumferential movement for the edges between task nodes. This is in comparison to traditional top-down or left-right approaches of drawing graphs. Furthermore, it turns out that Radar dynamically updates as tasks complete (or fails) its execution. And the mini-map, edge tracing, and node selection tools make workflow inspection doable even for highly complex graphs. See [*Introducing Radar*](https://www.prefect.io/guide/blog/introducing-radar/) by Bill Palombi for further reading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow runs as flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will write our code from the previous module for running modelling experiments as a flow in Prefect. Our idea is to have two flows: one for preprocessing the dataset such that the preprocessed datasets will be used by all experiment runs which will be the second flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`utils.py`](https://github.com/particle1331/inefficient-networks/blob/57e38c5eb06ac3323035fb9f8d714870e397a39a/docs/notebooks/mlops/3-prefect/utils.py)\n",
    "```\n",
    "```python\n",
    "@task\n",
    "def load_training_dataframe(file_path, y_min=1, y_max=60):\n",
    "    \"\"\"Load data from disk and preprocess for training.\"\"\"\n",
    "    \n",
    "    # Load data from disk\n",
    "    data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Create target column and filter outliers\n",
    "    data['duration'] = data.lpep_dropoff_datetime - data.lpep_pickup_datetime\n",
    "    data['duration'] = data.duration.dt.total_seconds() / 60\n",
    "    data = data[(data.duration >= y_min) & (data.duration <= y_max)]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "@task\n",
    "def fit_preprocessor(train_data):\n",
    "    \"\"\"Fit and save preprocessing pipeline.\"\"\"\n",
    "\n",
    "    # Unpack passed data\n",
    "    y_train = train_data.duration.values\n",
    "    X_train = train_data.drop('duration', axis=1)    \n",
    "\n",
    "    # Initialize pipeline\n",
    "    cat_features = ['PU_DO']\n",
    "    num_features = ['trip_distance']\n",
    "\n",
    "    preprocessor = make_pipeline(\n",
    "        AddPickupDropoffPair(),\n",
    "        SelectFeatures(cat_features + num_features),\n",
    "        ConvertToString(cat_features),\n",
    "        ConvertToDict(),\n",
    "        DictVectorizer(),\n",
    "    )\n",
    "\n",
    "    # Fit only on train set\n",
    "    preprocessor.fit(X_train, y_train)\n",
    "    joblib.dump(preprocessor, artifacts / 'preprocessor.pkl')\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "@task\n",
    "def create_model_features(preprocessor, train_data, valid_data):\n",
    "    \"\"\"Fit feature engineering pipeline. Transform training dataframes.\"\"\"\n",
    "\n",
    "    # Unpack passed data\n",
    "    y_train = train_data.duration.values\n",
    "    y_valid = valid_data.duration.values\n",
    "    X_train = train_data.drop('duration', axis=1)\n",
    "    X_valid = valid_data.drop('duration', axis=1)\n",
    "    \n",
    "    # Feature engineering\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_valid = preprocessor.transform(X_valid)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "@flow\n",
    "def preprocess_data(train_data_path, valid_data_path):\n",
    "    \"\"\"Preprocess data for model training.\"\"\"\n",
    "\n",
    "    train_data = load_training_dataframe(train_data_path)\n",
    "    valid_data = load_training_dataframe(valid_data_path)\n",
    "    \n",
    "    preprocessor = fit_preprocessor(train_data)\n",
    "    \n",
    "    return create_model_features(preprocessor, train_data, valid_data).result()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the `preprocess_data` flow loads the datasets from disk, fits and saves a preprocessor, and then creates transformed features and targets for training the machine learning model. Note that we have to be careful here to make sure we don't use concurrent execution if using multiple since we may log different preprocessors. In this case, we only use on preprocessor for all experiments so this concern does not materialize.\n",
    "\n",
    "Next, we will create a flow for executing experiment runs. Note that in the `main` flow we are passing around a [`PrefectFuture`](https://orion-docs.prefect.io/api-ref/prefect/futures/) object instead of Python objects. Futures represent the execution of a task and allow retrieval of the task run's state. This so that Prefect is able to track data dependency between tasks &mdash; converting to Python objects, i.e. using `.result()`, breaks this lineage. Note that once a future has been passed into the function, then we can treat this as a usual Python object. This is because the `task` decorator has done work to unpack the Future object into Python objects. For example, instead of defining:\n",
    "\n",
    "```python\n",
    "@task\n",
    "def f(X, y):\n",
    "    ...\n",
    "```\n",
    "\n",
    "We do:\n",
    "\n",
    "```python\n",
    "@task\n",
    "def f(future):\n",
    "    X, y = future\n",
    "```\n",
    "\n",
    "You will see notice this in the `xgboost_runs` and `lr_runs` tasks below. For the `main` flow, we execute the following sequentially:\n",
    "setting up the connection to the experiment (not a task), a subflow run for preprocessing the datasets for modelling, one run of the linear regression baseline model, and multiple runs of XGBoost hyperparameter optimization using the TPE algorithm. Sequential execution ensures that all resources are allocated to a single learning algorithm at each point in the flow run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`main.py`](https://github.com/particle1331/inefficient-networks/blob/fd937c097b9f59e171f263f0208b2407bb22efde/docs/notebooks/mlops/3-prefect/main.py)\n",
    "```\n",
    "```python\n",
    "def objective(params, xgb_train, y_train, xgb_valid, y_valid):\n",
    "    \"\"\"Compute validation RMSE (one trial = one run).\"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=xgb_train,\n",
    "            num_boost_round=100,\n",
    "            evals=[(xgb_valid, 'validation')],\n",
    "            early_stopping_rounds=5\n",
    "        )\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "\n",
    "    return {'loss': rmse_valid, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "@task\n",
    "def xgboost_runs(num_runs, training_packet):\n",
    "    \"\"\"Run TPE algorithm on search space to minimize objective.\"\"\"\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = training_packet\n",
    "    Xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    Xgb_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "\n",
    "    search_space = {\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "        'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "        'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "        'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    best_result = fmin(\n",
    "        fn=partial(\n",
    "            objective, \n",
    "            xgb_train=Xgb_train, y_train=y_train, \n",
    "            xgb_valid=Xgb_valid, y_valid=y_valid,\n",
    "        ),\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_runs,\n",
    "        trials=Trials()\n",
    "    )\n",
    "\n",
    "\n",
    "@task\n",
    "def linreg_runs(training_packet):\n",
    "    \"\"\"Run linear regression training.\"\"\"\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = training_packet\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "\n",
    "        \n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main(train_data_path, valid_data_path, num_xgb_runs=1):\n",
    "\n",
    "    # Set and run experiment\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "\n",
    "    future = preprocess_data(train_data_path, valid_data_path)\n",
    "    linreg_runs(future)\n",
    "    xgboost_runs(num_xgb_runs, future)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the dashboard, we can see a `utopian-rat` run of the `main` flow. As expected, this consists of 3 tasks (one of which is a subflow) that are executed sequentially as indicated in the timeline. If we look at the preprocessing subflow, we see that it has concurrent execution from overlapping lines in the timeline graph. This subflow consists of four tasks.\n",
    "\n",
    "```{figure} ../../../img/mlflow-runs-dashboard.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/radar_preprocessing.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check out the radar of the `main` flow, we see the following. Here in an earlier screenshot, we see that the XGBoost run is still running. The `xgboost_runs` task has been running for 1 minute and 14 seconds. Both runs depend on the preprocessing subflow. We can go down on the radar for the preprocessing subflow by clicking on the `4 task runs` button.\n",
    "\n",
    "\n",
    "```{figure} ../../../img/radar_xgb.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "Here we see the radar plot. You might want to open this image in a new tab to see better. Hovering on each task shows its data dependence on other tasks. For each task, we show the dependency lines in the figure below:\n",
    "\n",
    "```{figure} ../../../img/radar.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "The `load_training_dataframe` task on the left loads the validation dataset since it only has `create_model_features` as the only forward dependence. The `fit_preprocessor` task trains the preprocessor and therefore depends on the task that loads the training dataframes. Next, we see the dependencies of the `load_training_dataframe` task that loads the train dataset. This sends data to the preprocessor and to the final task `create_model_features` which returns all processed data for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we deploy a workflow that transitions a performant model to staging in MLflow's model registry. This can be useful for regularly staging candidate models models trained on new data. The staged models can then be further checked if it should be deployed into production. Refer to the code in [Module 2](https://particle1331.github.io/inefficient-networks/notebooks/mlops/2-mlflow/2-mlflow.html#api-workflows) to understand the next few code cells. Checking connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experiment)\n",
      "    experiment_id=0\n",
      "    name='Default'\n",
      "    artifact_location='./mlruns/0'\n",
      "\n",
      "(Experiment)\n",
      "    experiment_id=1\n",
      "    name='nyc-taxi-experiment'\n",
      "    artifact_location='./mlruns/1'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "\n",
    "def print_experiment(experiment):\n",
    "    print(f\"(Experiment)\")\n",
    "    print(f\"    experiment_id={experiment.experiment_id}\")\n",
    "    print(f\"    name='{experiment.name}'\")\n",
    "    print(f\"    artifact_location='{experiment.artifact_location}'\")\n",
    "    print()\n",
    "\n",
    "for experiment in client.list_experiments():\n",
    "    print_experiment(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall we run a flow which performs HPO for XGBoost with 10 runs. So we expect we have experiments in our tracker. We will filter out runs with valid RMSE less than `6.5` and inference time less than `2e-5`. This can be done through the client as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 4659603b9b674df59319ae6d4b67890a   rmse_valid: 6.404   inference_time: 1.1171e-05\n",
      "run_id: d2baeedede3545c7a12f858c86e01605   rmse_valid: 6.415   inference_time: 7.7727e-06\n",
      "run_id: 4a627ff6420549e5a0dbaf41fef5795b   rmse_valid: 6.440   inference_time: 7.2689e-06\n",
      "run_id: 391212c00c67495fbfcf6e5abc0c8a9d   rmse_valid: 6.442   inference_time: 6.0649e-06\n",
      "run_id: f3fc839c1036469290c309afa47e3d3b   rmse_valid: 6.470   inference_time: 8.2583e-06\n"
     ]
    }
   ],
   "source": [
    "candidates = client.search_runs(\n",
    "    experiment_ids=1,\n",
    "    filter_string='metrics.rmse_valid < 6.5 and metrics.inference_time < 20e-6',\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.rmse_valid ASC\"]\n",
    ")\n",
    "\n",
    "for run in candidates:\n",
    "    print(f\"run_id: {run.info.run_id}   rmse_valid: {run.data.metrics['rmse_valid']:.3f}   inference_time: {run.data.metrics['inference_time']:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4659603b9b674df59319ae6d4b67890a', 6.40444573825302)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_stage = candidates[0]\n",
    "model_to_stage.info.run_id, model_to_stage.data.metrics['rmse_valid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, we register this to `Staging`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'NYCRideDurationModel'.\n",
      "2022/06/11 23:25:29 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: NYCRideDurationModel, version 1\n",
      "Created version '1' of model 'NYCRideDurationModel'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1654961129078, current_stage='Staging', description=None, last_updated_timestamp=1654961129083, name='NYCRideDurationModel', run_id='4659603b9b674df59319ae6d4b67890a', run_link=None, source='./mlruns/1/4659603b9b674df59319ae6d4b67890a/artifacts/model', status='READY', status_message=None, tags={}, user_id=None, version=1>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_model = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{model_to_stage.info.run_id}/model\", \n",
    "    name='NYCRideDurationModel'\n",
    ")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name='NYCRideDurationModel',\n",
    "    version=registered_model.version, \n",
    "    stage='Staging',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/mlflow-automatic-staging.png\n",
    "---\n",
    "---\n",
    "Staged model from code cells above.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, so we now collect this into a workflow which will create a new experiment, perform the experiment runs, and filters the best model for staging. We will then schedule this workflow to be run at fixed intervals using Prefect. To create a deployment in Prefect we simply run the following specifications:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from prefect.deployments import DeploymentSpec\n",
    "from prefect.orion.schemas.schedules import IntervalSchedule\n",
    "from prefect.flow_runners import SubprocessFlowRunner\n",
    "from datetime import timedelta\n",
    "\n",
    "...\n",
    "\n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def deploy_main(train_data_path, valid_data_path, num_xgb_runs=1):\n",
    "\n",
    "    # Set and run experiment\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(f\"nyc-taxi-experiment-{str(datetime.datetime.now())}\")\n",
    "\n",
    "    future = preprocess_data(train_data_path, valid_data_path)\n",
    "    linreg_runs(future)\n",
    "    xgboost_runs(num_xgb_runs, future)\n",
    "\n",
    "\n",
    "DeploymentSpec(\n",
    "    flow=deploy_main,\n",
    "    name=\"mlflow_staging\",\n",
    "    schedule=IntervalSchedule(interval=timedelta(minutes=5)),\n",
    "    flow_runner=SubprocessFlowRunner(),\n",
    "    parameters={\n",
    "        \"train_data_path\": data_path / 'green_tripdata_2021-01.parquet',\n",
    "        \"valid_data_path\": data_path / 'green_tripdata_2021-02.parquet',\n",
    "        \"num_xgb_runs\": 10\n",
    "    },\n",
    "    tags=[\"ml\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark.** Relative imports don't work so we had to paste everything in the `mlflow_deploy.py` script for lack of time. But if we are to do this properly, we have to create a package for the project so that imports for our own scripts work everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "345667e5aa1587f3f99ee9b59f54516e1e87c18189e42869047387925519f8a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('prefect')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
