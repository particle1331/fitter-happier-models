{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration and ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Ongoing&color=orange)\n",
    "\n",
    "<!-- Place this tag where you want the button to render. -->\n",
    "<a class=\"github-button\" href=\"https://github.com/particle1331/steepest-ascent\" data-color-scheme=\"no-preference: dark; light: light; dark: dark;\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star particle1331/steepest-ascent on GitHub\">Star</a>\n",
    "<!-- Place this tag in your head or just before your close body tag. -->\n",
    "<script async defer src=\"https://buttons.github.io/buttons.js\"></script> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous module, we learned about experiment tracking and model registry.\n",
    "In particular, we discussed how to get a candidate model and promote it from staging to production.\n",
    "In this module, we learn how to automate this process, and having this scheduled with workflow orchestration &mdash; specifically, with [Prefect 2.0](https://orion-docs.prefect.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect allows us to programatically author, schedule, and monitor workflows. Prefect allows us to minimize time on **negative engineering**, i.e. coding against all possible causes of failure. This is a Sisyphean task as there are endless ways that elements of a data pipeline can fail. In practical terms, Prefect provides tools such as retries, concurrency, logging, a nice UI, tracking dependencies, a database, caching and serialization, parameterization of scheduled tasks, and more. As we shall see later, this adds observability to the whole data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "⚠️ **Attribution:** These are notes for [Module 3](https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/03-orchestration) of the [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp). The MLOps Zoomcamp is a free course from [DataTalks.Club](https://github.com/DataTalksClub).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefect basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **flow** in Prefect is simply a Python function. This consists of **tasks** which can be thought of as the atom of observability in Prefect. In practice, to create a flow, you simply convert functions that make it up into tasks. Consider [the following example](https://www.prefect.io/guide/blog/getting-started-prefect-2/#Makingourflowsbetterwithtasks) where we get data from an unreliable API, augment this data, then write the resulting data into a database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:05:51.772 | INFO    | prefect.engine - Created flow run 'ancient-sturgeon' for flow 'pipeline'\n",
      "23:05:51.804 | INFO    | Flow run 'ancient-sturgeon' - Using task runner 'ConcurrentTaskRunner'\n",
      "23:05:51.870 | WARNING | Flow run 'ancient-sturgeon' - No default storage is configured on the server. Results from this flow run will be stored in a temporary directory in its runtime environment.\n",
      "23:05:51.967 | INFO    | Flow run 'ancient-sturgeon' - Created task run 'call_unreliable_api-48f93715-0' for task 'call_unreliable_api'\n",
      "23:05:52.000 | INFO    | Flow run 'ancient-sturgeon' - Created task run 'augment_data-505b3e0c-0' for task 'augment_data'\n",
      "23:05:52.010 | ERROR   | Task run 'call_unreliable_api-48f93715-0' - Encountered exception during execution:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/engine.py\", line 798, in orchestrate_task_run\n",
      "    result = await run_sync_in_worker_thread(task.fn, *args, **kwargs)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/utilities/asyncio.py\", line 54, in run_sync_in_worker_thread\n",
      "    return await anyio.to_thread.run_sync(call, cancellable=True)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/9vsvd9252_349lsng_5gc_jw0000gn/T/ipykernel_20240/3061218592.py\", line 10, in call_unreliable_api\n",
      "    raise Exception(\"Our unreliable service failed.\")\n",
      "Exception: Our unreliable service failed.\n",
      "23:05:52.025 | INFO    | Flow run 'ancient-sturgeon' - Created task run 'write_to_database-d58974ba-0' for task 'write_to_database'\n",
      "23:05:52.036 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Received non-final state 'AwaitingRetry' when proposing final state 'Failed' and will attempt to run again...\n",
      "23:05:52.059 | ERROR   | Task run 'call_unreliable_api-48f93715-0' - Encountered exception during execution:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/engine.py\", line 798, in orchestrate_task_run\n",
      "    result = await run_sync_in_worker_thread(task.fn, *args, **kwargs)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/utilities/asyncio.py\", line 54, in run_sync_in_worker_thread\n",
      "    return await anyio.to_thread.run_sync(call, cancellable=True)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/9vsvd9252_349lsng_5gc_jw0000gn/T/ipykernel_20240/3061218592.py\", line 10, in call_unreliable_api\n",
      "    raise Exception(\"Our unreliable service failed.\")\n",
      "Exception: Our unreliable service failed.\n",
      "23:05:52.071 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Received non-final state 'AwaitingRetry' when proposing final state 'Failed' and will attempt to run again...\n",
      "23:05:52.083 | ERROR   | Task run 'call_unreliable_api-48f93715-0' - Encountered exception during execution:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/engine.py\", line 798, in orchestrate_task_run\n",
      "    result = await run_sync_in_worker_thread(task.fn, *args, **kwargs)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/utilities/asyncio.py\", line 54, in run_sync_in_worker_thread\n",
      "    return await anyio.to_thread.run_sync(call, cancellable=True)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/9vsvd9252_349lsng_5gc_jw0000gn/T/ipykernel_20240/3061218592.py\", line 10, in call_unreliable_api\n",
      "    raise Exception(\"Our unreliable service failed.\")\n",
      "Exception: Our unreliable service failed.\n",
      "23:05:52.097 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Received non-final state 'AwaitingRetry' when proposing final state 'Failed' and will attempt to run again...\n",
      "23:05:52.128 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Finished in state Completed()\n",
      "23:05:52.182 | INFO    | Task run 'augment_data-505b3e0c-0' - Finished in state Completed()\n",
      "23:05:52.346 | INFO    | Task run 'write_to_database-d58974ba-0' - Finished in state Completed()\n",
      "23:05:52.390 | INFO    | Flow run 'ancient-sturgeon' - Finished in state Completed('All states completed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote {'data': 42, 'message': '0'} to database successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Completed(message='All states completed.', type=COMPLETED, result=[Completed(message=None, type=COMPLETED, result={'data': 42, 'message': '0'}, task_run_id=abbf8182-dc8c-4e25-8169-baa5d141e288), Completed(message=None, type=COMPLETED, result={'data': 42, 'message': '0'}, task_run_id=fb891af2-e287-43bc-ae6e-3f6b2aed7a17), Completed(message=None, type=COMPLETED, result='Success!', task_run_id=eb4015c0-bed5-4ef9-b6b1-8f3ade94b2c5)], flow_run_id=53796cbf-187d-45c6-9a0d-c58b111e4a60)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from prefect import flow, task \n",
    "\n",
    "\n",
    "@task(retries=3)\n",
    "def call_unreliable_api():\n",
    "    choices = [{\"data\": 42}, \"Failure\"]\n",
    "    res = random.choice(choices)\n",
    "    if res == \"Failure\":\n",
    "        raise Exception(\"Our unreliable service failed.\")\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "@task\n",
    "def augment_data(data: dict, msg: str):\n",
    "    data[\"message\"] = msg\n",
    "    return data\n",
    "\n",
    "@task\n",
    "def write_to_database(data: dict):\n",
    "    print(f\"Wrote {data} to database successfully!\")\n",
    "    return \"Success!\"\n",
    "\n",
    "@flow \n",
    "def pipeline(msg: str):\n",
    "    api_result = call_unreliable_api()\n",
    "    augmented_data = augment_data(data=api_result, msg=msg)\n",
    "    write_to_database(augmented_data)\n",
    "\n",
    "\n",
    "pipeline(0) # Augment data with zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this failed multiple times before pushing through. We can start the UI by calling `prefect orion start` in any directory (`.prefect` is saved in the system's root directory). This starts the Prefect Orion server in port 4200. Then, we can navigate around to find the `ancient-sturgeon` flow which as we have seen in the logs, was able to complete its execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "❯ prefect orion start\n",
    "Starting...\n",
    "\n",
    " ___ ___ ___ ___ ___ ___ _____    ___  ___ ___ ___  _  _\n",
    "| _ \\ _ \\ __| __| __/ __|_   _|  / _ \\| _ \\_ _/ _ \\| \\| |\n",
    "|  _/   / _|| _|| _| (__  | |   | (_) |   /| | (_) | .` |\n",
    "|_| |_|_\\___|_| |___\\___| |_|    \\___/|_|_\\___\\___/|_|\\_|\n",
    "\n",
    "Configure Prefect to communicate with the server with:\n",
    "\n",
    "    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n",
    "\n",
    "Check out the dashboard at http://127.0.0.1:4200\n",
    "\n",
    "\n",
    "\n",
    "INFO:     Started server process [20557]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://127.0.0.1:4200 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that this flow started on `2022/06/10 11:05:51 PM` and ended on `2022/06/10 11:05:52 PM`. We also see the logs has the details of the exception when the API call failed. In the second tab, we can see the tasks that make up this flow. There is also the subflow tab which shows that we can call flows from a parent flow. One of the more interesting features of the UI is **Radar** on the right.\n",
    "\n",
    "```{figure} ../../../img/hello-world-2.png\n",
    "```\n",
    "\n",
    "This shows the dependence between tasks. Notice the linear dependence, the `write_to_database` task depends on the `augment_data` task but not on the `call_unreliable_api` task. Having tasks arranged in concentric circles allow for a heirarchy of dependence. \n",
    "\n",
    "```{figure} ../../../img/hello-world-1.png\n",
    "```\n",
    "\n",
    "Finally, we have a flow which failed to complete all its tasks. Here all calls to the API failed despite the retries. We can clearly see from Radar where the flow has failed. This is super cool. Especially, when we have a dozens task and multiple subflows happening in our data pipeline.\n",
    "\n",
    "\n",
    "```{figure} ../../../img/hello-world-3.png\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Remark.** Note also that geometrically there is more space available to grow the dependence tree due to nodes being farther apart as we move radially with a fixed angle, this also allows Radar to minimize edge crossing by combining radial and circumferential movement for the edges between task nodes. This is in comparison to traditional top-down / left-right approaches of drawing graphs. Furthermore, it turns out that Radar dynamically updates as tasks complete (or fails) its execution:\n",
    "\n",
    "> Orion’s Radar is based on a structured, radial canvas upon which tasks are rendered as they are orchestrated. We developed a new layout algorithm that updates Radar as tasks run. It’s not just designed to handle simple workflows, but also those with massive dynamic fan-out, fan-in, sidecar tasks, and complex references. The algorithm optimizes readability through consistent node placement and minimal edge crossings. Users can zoom and pan across the canvas to discover and inspect tasks of interest. The mini-map, edge tracing, and node selection tools make workflow inspection a breeze. &mdash; [*Introducing Radar*](https://www.prefect.io/guide/blog/introducing-radar/) by Bill Palombi, Head of Product at Prefect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow runs as flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will write our code from the previous module for running modelling experiments as a flow in Prefect. Our idea is to have two flows: one for preprocessing the dataset such that the preprocessed datasets will be used by all experiment runs which will be the second flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`utils.py`](https://github.com/particle1331/inefficient-networks/blob/57e38c5eb06ac3323035fb9f8d714870e397a39a/docs/notebooks/mlops/3-prefect/utils.py)\n",
    "```\n",
    "```python\n",
    "@task\n",
    "def load_training_dataframe(file_path, y_min=1, y_max=60):\n",
    "    \"\"\"Load data from disk and preprocess for training.\"\"\"\n",
    "    \n",
    "    # Load data from disk\n",
    "    data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Create target column and filter outliers\n",
    "    data['duration'] = data.lpep_dropoff_datetime - data.lpep_pickup_datetime\n",
    "    data['duration'] = data.duration.dt.total_seconds() / 60\n",
    "    data = data[(data.duration >= y_min) & (data.duration <= y_max)]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "@task\n",
    "def fit_preprocessor(train_data):\n",
    "    \"\"\"Fit and save preprocessing pipeline.\"\"\"\n",
    "\n",
    "    # Unpack passed data\n",
    "    y_train = train_data.duration.values\n",
    "    X_train = train_data.drop('duration', axis=1)    \n",
    "\n",
    "    # Initialize pipeline\n",
    "    cat_features = ['PU_DO']\n",
    "    num_features = ['trip_distance']\n",
    "\n",
    "    preprocessor = make_pipeline(\n",
    "        AddPickupDropoffPair(),\n",
    "        SelectFeatures(cat_features + num_features),\n",
    "        ConvertToString(cat_features),\n",
    "        ConvertToDict(),\n",
    "        DictVectorizer(),\n",
    "    )\n",
    "\n",
    "    # Fit only on train set\n",
    "    preprocessor.fit(X_train, y_train)\n",
    "    joblib.dump(preprocessor, artifacts / 'preprocessor.pkl')\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "@task\n",
    "def create_model_features(preprocessor, train_data, valid_data):\n",
    "    \"\"\"Fit feature engineering pipeline. Transform training dataframes.\"\"\"\n",
    "\n",
    "    # Unpack passed data\n",
    "    y_train = train_data.duration.values\n",
    "    y_valid = valid_data.duration.values\n",
    "    X_train = train_data.drop('duration', axis=1)\n",
    "    X_valid = valid_data.drop('duration', axis=1)\n",
    "    \n",
    "    # Feature engineering\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_valid = preprocessor.transform(X_valid)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "@flow\n",
    "def preprocess_data(train_data_path, valid_data_path):\n",
    "    \"\"\"Preprocess data for model training.\"\"\"\n",
    "\n",
    "    train_data = load_training_dataframe(train_data_path)\n",
    "    valid_data = load_training_dataframe(valid_data_path)\n",
    "    \n",
    "    preprocessor = fit_preprocessor(train_data)\n",
    "    \n",
    "    return create_model_features(preprocessor, train_data, valid_data).result()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the `preprocess_data` flow loads the datasets from disk, fits and saves a preprocessor, and then creates transformed features and targets for training the machine learning model. Note that we have to be careful here to make sure we don't use concurrent execution if using multiple since we may log different preprocessors. In this case, we only use on preprocessor for all experiments so this concern does not materialize.\n",
    "\n",
    "Next, we will create a flow for executing experiment runs. Note that in the `main` flow we are passing around a [`PrefectFuture`](https://orion-docs.prefect.io/api-ref/prefect/futures/) object instead of Python objects. Futures represent the execution of a task and allow retrieval of the task run's state. This so that Prefect is able to track data dependency between tasks &mdash; converting to Python objects, e.g. using `.result()`, results in breaking this lineage. Note that once a future has been passed into the function, then we can treat this as a usual Python object. This is because the `task` decorator has done work to unpack the Future object into Python objects. For example, instead of defining:\n",
    "\n",
    "```python\n",
    "@task\n",
    "def f(X, y):\n",
    "    ...\n",
    "```\n",
    "\n",
    "We do:\n",
    "\n",
    "```python\n",
    "@task\n",
    "def f(future):\n",
    "    X, y = future\n",
    "```\n",
    "\n",
    "You will see notice this in the `xgboost_runs` and `lr_runs` tasks below. For the `main` flow, we execute the following sequentially:\n",
    "setting up the connection to the experiment (not a task), subflow run for preprocessing the datasets for modelling, one run of the linear regression baseline model, and `num_xgb_runs` many runs of XGBoost hyperparameter optimization using the TPE algorithm. This ensures all resources are allocated to a single learning algorithm at each point in the flow run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`main.py`](https://github.com/particle1331/inefficient-networks/blob/fd937c097b9f59e171f263f0208b2407bb22efde/docs/notebooks/mlops/3-prefect/main.py)\n",
    "```\n",
    "```python\n",
    "def objective(params, xgb_train, y_train, xgb_valid, y_valid):\n",
    "    \"\"\"Compute validation RMSE (one trial = one run).\"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=xgb_train,\n",
    "            num_boost_round=100,\n",
    "            evals=[(xgb_valid, 'validation')],\n",
    "            early_stopping_rounds=5\n",
    "        )\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "\n",
    "    return {'loss': rmse_valid, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "@task\n",
    "def xgboost_runs(num_runs, training_packet):\n",
    "    \"\"\"Run TPE algorithm on search space to minimize objective.\"\"\"\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = training_packet\n",
    "    Xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    Xgb_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "\n",
    "    search_space = {\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "        'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "        'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "        'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    best_result = fmin(\n",
    "        fn=partial(\n",
    "            objective, \n",
    "            xgb_train=Xgb_train, y_train=y_train, \n",
    "            xgb_valid=Xgb_valid, y_valid=y_valid,\n",
    "        ),\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_runs,\n",
    "        trials=Trials()\n",
    "    )\n",
    "\n",
    "\n",
    "@task\n",
    "def linreg_runs(training_packet):\n",
    "    \"\"\"Run linear regression training.\"\"\"\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = training_packet\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "\n",
    "        \n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main(train_data_path, valid_data_path, num_xgb_runs=1):\n",
    "\n",
    "    # Set and run experiment\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "\n",
    "    future = preprocess_data(train_data_path, valid_data_path)\n",
    "    linreg_runs(future)\n",
    "    xgboost_runs(num_xgb_runs, future)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "345667e5aa1587f3f99ee9b59f54516e1e87c18189e42869047387925519f8a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('prefect')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
