{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Ongoing&color=orange)\n",
    "\n",
    "<!-- Place this tag where you want the button to render. -->\n",
    "<a class=\"github-button\" href=\"https://github.com/particle1331/steepest-ascent\" data-color-scheme=\"no-preference: dark; light: light; dark: dark;\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star particle1331/steepest-ascent on GitHub\">Star</a>\n",
    "<!-- Place this tag in your head or just before your close body tag. -->\n",
    "<script async defer src=\"https://buttons.github.io/buttons.js\"></script> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we will look into deploying the ride duration model which has been our working example in the modules. Deploying means that other applications can get predictions from our model. We will look at three modes of deployment: **online** deployment, **offline** or batch deployment, and **streaming**. \n",
    "\n",
    "In online mode, our service must be up all the time. To do this, we implement a web service which takes in HTTP requests and sends out predictions. In offline or mode, we have a service running regularly, but not necessarily all the time. This can make predictions for a batch of examples that runs periodically using workflow orchestration. Finally, we look at how to implement a streaming service, i.e. a machine learning service that listens to a stream of events and reacts to it using AWS Kinesis and AWS Lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "⚠️ **Attribution:** These are notes for [Module 4: Model Deployment](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/04-deployment) of the [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp). The MLOps Zoomcamp is a free course from [DataTalks.Club](https://github.com/DataTalksClub).\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying models with Flask and Docker\n",
    "\n",
    "In this section, we develop a web server using Flask for serving model predictions. The model is obtained from an S3 artifacts store and predicts on data sent to the service by the backend. We will containerize the application using Docker. This container can be deployed anywhere where Docker is supported such as Kubernetes and Elastic Beanstalk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging modeling code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will collect code around our models into a `ride_duration` package. Having a model package ensures smooth integration with the Flask API since we have the same code for training, testing, and inference. Ideally, we should also use the same environment for each of these phases and this can be done by using Pipenv. \n",
    "\n",
    "Also, packaging makes imports just work. For consistency, we will also use the `ride_duration` package for our batch scoring workflow. The directory structure of our project would look like the following. Notice that there is a nice separation between model code, application code, and deployment code.\n",
    "\n",
    "```\n",
    "deployment/\n",
    "├── app/\n",
    "│   └── main.py\n",
    "├── ride_duration/\n",
    "│   ├── __init__.py\n",
    "│   ├── predict.py\n",
    "│   ├── utils.py\n",
    "│   └── VERSION\n",
    "├── .env\n",
    "├── Dockerfile\n",
    "├── Pipfile\n",
    "├── MANIFEST.in\n",
    "├── Pipfile.lock\n",
    "├── setup.py\n",
    "├── test.py\n",
    "├── train.py\n",
    "└── pyproject.toml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create [`setup.py`](https://github.com/particle1331/inefficient-networks/blob/92b7232fbbba4e17a10ce6a55a37725d32371f0b/docs/notebooks/mlops/04-deployment/setup.py) and [`pyproject.toml`](https://github.com/particle1331/inefficient-networks/blob/217134c84bb323452bf0dc3e8b6a6a04fea8f06b/docs/notebooks/mlops/04-deployment/pyproject.toml) for packaging. Refer to the links to see the complete code. For `setup.py` you only have to change the package metadata (or just leave them blank) and set `install_requires` to `[]`. This list will be later filled using a tool that integrates with Pipenv which we will use for package management.\n",
    "\n",
    "```{margin}\n",
    "[`setup.py`](https://github.com/particle1331/inefficient-networks/blob/92b7232fbbba4e17a10ce6a55a37725d32371f0b/docs/notebooks/mlops/04-deployment/setup.py)\n",
    "```\n",
    "```python\n",
    "from pathlib import Path\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "\n",
    "# Package meta-data.\n",
    "NAME = \"ride-duration-prediction\"\n",
    "DESCRIPTION = \"\"\n",
    "URL = \"\"\n",
    "EMAIL = \"\"\n",
    "AUTHOR = \"\"\n",
    "REQUIRES_PYTHON = \">=3.9.0\"\n",
    "\n",
    "\n",
    "# The rest you shouldn't have to touch too much. Except for install_requires=[]. \n",
    "# Perhaps also the License and Trove Classifiers if publishing to PyPI (public).\n",
    "...\n",
    "\n",
    "setup(\n",
    "    ...\n",
    "    install_requires=[],             \n",
    "    ...\n",
    "    license=\"MIT\",\n",
    "    classifiers=[\n",
    "        # Trove classifiers\n",
    "        \"License :: OSI Approved :: MIT License\",\n",
    "        \"Programming Language :: Python :: 3.9\",\n",
    "        \"Programming Language :: Python :: Implementation :: CPython\",\n",
    "        \"Programming Language :: Python :: Implementation :: PyPy\",\n",
    "    ],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can include [`MANIFEST.in`](https://github.com/particle1331/inefficient-networks/blob/217134c84bb323452bf0dc3e8b6a6a04fea8f06b/docs/notebooks/mlops/04-deployment/MANIFEST.in) file to specify the files included in the source distribution of the package. The full list can be viewed in the `SOURCES.txt` file of the generated `egg-info` folder after building the package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`MANIFEST.in`](https://github.com/particle1331/inefficient-networks/blob/217134c84bb323452bf0dc3e8b6a6a04fea8f06b/docs/notebooks/mlops/04-deployment/MANIFEST.in)\n",
    "```\n",
    "```\n",
    "include ride_duration/*.py\n",
    "include ride_duration/VERSION\n",
    "\n",
    "recursive-exclude * __pycache__\n",
    "recursive-exclude * *.py[co]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage our projects package dependencies, we will use [Pipenv](https://pipenv.pypa.io/en/latest/). Notice that we get `Pipfile` which supersedes the usual requirements file, and also a `Pipfile.lock` containing hashes of downloaded packages that ensure reproducible builds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pipenv install scikit-learn==1.0.2 flask pandas mlflow boto3 --python=3.9\n",
    "pipenv install --dev requests\n",
    "pipenv install --dev pipenv-setup\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we install the model package locally. Here we use `pipenv-setup sync` to update `install_requires` in the `setup` script according to the packages installed using Pipenv. This makes sure there are no dependency conflicts when using the package.\n",
    "\n",
    "```bash\n",
    "pipenv-setup sync\n",
    "pipenv install -e .\n",
    "```\n",
    "\n",
    "Our `Pipfile` should now look like the following. Note that `ride-duration-prediction` is installed in editable mode which is okay since the underlying code is still in development.\n",
    "\n",
    "```{margin}\n",
    "[`Pipfile`](https://github.com/particle1331/inefficient-networks/blob/32bb7aecb5b6c4999becba323d6695eb97f3cd3a/docs/notebooks/mlops/04-deployment/Pipfile)\n",
    "```\n",
    "```ini\n",
    "[[source]]\n",
    "url = \"https://pypi.org/simple\"\n",
    "verify_ssl = true\n",
    "name = \"pypi\"\n",
    "\n",
    "[packages]\n",
    "scikit-learn = \"==1.0.2\"\n",
    "flask = \"*\"\n",
    "pandas = \"*\"\n",
    "mlflow = \"*\"\n",
    "boto3 = \"*\"\n",
    "ride-duration-prediction = {editable = true, path = \".\"}\n",
    "\n",
    "[dev-packages]\n",
    "requests = \"*\"\n",
    "pipenv-setup = \"*\"\n",
    "\n",
    "[requires]\n",
    "python_version = \"3.9\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS credentials and other environmental variables that we will use later are saved in a `.env` file in the same directory as Pipfile. These are automatically detected and loaded by Pipenv when calling `pipenv shell`. However, the shell must be restarted whenever the `.env` file is modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# .env\n",
    "EXPERIMENT_ID=1\n",
    "MODEL_RUN_ID=f4e2242a53a3410d89c061d1958ae70a\n",
    "AWS_ACCESS_KEY_ID=A*************LI\n",
    "AWS_SECRET_ACCESS_KEY=N*********************+9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following module, define helper functions for model training and inference. This includes the usual `load_training_dataframe` function which creates the target features (ride duration in minutes) and filters it to some range, i.e. `[1, 60]`. This function is used for creating training and validation datasets. The other function `prepare_features` is used for feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`utils.py`](https://github.com/particle1331/inefficient-networks/blob/217134c84bb323452bf0dc3e8b6a6a04fea8f06b/docs/notebooks/mlops/04-deployment/ride_duration/utils.py)\n",
    "```\n",
    "```python\n",
    "def load_training_dataframe(file_path, y_min=1, y_max=60):\n",
    "    \"\"\"Load data from disk and preprocess for training.\"\"\"\n",
    "    \n",
    "    # Load data from disk\n",
    "    data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Create target column and filter outliers\n",
    "    data['duration'] = data.lpep_dropoff_datetime - data.lpep_pickup_datetime\n",
    "    data['duration'] = data.duration.dt.total_seconds() / 60\n",
    "    data = data[(data.duration >= y_min) & (data.duration <= y_max)]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_features(input_data):\n",
    "    \"\"\"Prepare features for dict vectorizer.\"\"\"\n",
    "\n",
    "    X = pd.DataFrame(input_data)\n",
    "    X['PU_DO'] = X['PULocationID'].astype(str) + '_' + X['DOLocationID'].astype(str)\n",
    "    X = X[['PU_DO', 'trip_distance']].to_dict(orient='records')\n",
    "    \n",
    "    return X\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the `predict` module of our package. This contains two functions: one for loading the model and another for making predictions with the model. The `load_model` function loads a model directly from the S3 artifacts store:\n",
    "\n",
    "```{margin}\n",
    "[`predict.py`](https://github.com/particle1331/inefficient-networks/blob/eae7ab0c4fac9dde75a7510ff9a705cabc256e64/docs/notebooks/mlops/04-deployment/ride_duration/predict.py#L10-L16)\n",
    "```\n",
    "```python\n",
    "def load_model(experiment_id, run_id):\n",
    "    \"\"\"Get model from our S3 artifacts store.\"\"\"\n",
    "\n",
    "    source = f\"s3://mlflow-models-ron/{experiment_id}/{run_id}/artifacts/model\"\n",
    "    model = mlflow.pyfunc.load_model(source)\n",
    "\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to load the preprocessor separately from the artifacts store, we train models that are pipelines of the following form (see **Appendix** below):\n",
    "\n",
    "```python\n",
    "pipeline = make_pipeline(\n",
    "    DictVectorizer(), \n",
    "    RandomForestRegressor(**params, n_jobs=-1)\n",
    ")\n",
    "```\n",
    "\n",
    "Our models expect as input `prepare_features(data)` where `data` can be a `DataFrame` with rows containing ride or a list of ride features dictionaries (e.g obtained as a JSON payload). So we define the following function to help with inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`predict.py`](https://github.com/particle1331/inefficient-networks/blob/eae7ab0c4fac9dde75a7510ff9a705cabc256e64/docs/notebooks/mlops/04-deployment/ride_duration/predict.py#L19-L25)\n",
    "```\n",
    "```python\n",
    "def make_prediction(model, input_data: Union[list[dict], pd.DataFrame]):\n",
    "    \"\"\"Make prediction from features dict or DataFrame.\"\"\"\n",
    "    \n",
    "    X = prepare_features(input_data)\n",
    "    preds = model.predict(X)\n",
    "\n",
    "    return preds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out the `load_model()` function:\n",
    "\n",
    "```bash\n",
    "❯ python\n",
    "Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:01:00)\n",
    "[Clang 13.0.1 ] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import os\n",
    ">>> from ride_duration.predict import load_model\n",
    ">>> model = load_model(os.getenv(\"EXPERIMENT_ID\"), os.getenv(\"MODEL_RUN_ID\"))\n",
    ">>> model\n",
    "mlflow.pyfunc.loaded_model:\n",
    "  artifact_path: model\n",
    "  flavor: mlflow.sklearn\n",
    "  run_id: f4e2242a53a3410d89c061d1958ae70a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/s3-artifacts-ss.png\n",
    "---\n",
    "---\n",
    "Artifacts store for model runs of experiment 1.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, we can also load the latest **production version** directly from the model registry (no need to specify a run and experiment ID) using the tracking server. One issue is that starting of the Flask server can fail whenever the request the tracking server is down.\n",
    "\n",
    "```python\n",
    "TRACKING_URI = f\"http://{TRACKING_SERVER_HOST}:5000\"\n",
    "\n",
    "# Fetch production model from client\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "client = MlflowClient(tracking_uri=TRACKING_URI)\n",
    "prod_model = client.get_latest_versions(name='NYCRideDurationModel', stages=['Production'])[0]\n",
    "\n",
    "# Load model from S3 artifacts store\n",
    "run_id = prod_model.run_id\n",
    "source = prod_model.source\n",
    "model = mlflow.pyfunc.load_model(source)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving predictions using Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model loads when the Flask server starts (not every time we make a prediction). Then, we define a single `predict_endpoint` which expects a singleton JSON payload, otherwise it only predicts on the first entry. For observability, each prediction is returned along with the `run_id` of the model.\n",
    "\n",
    "```{margin}\n",
    "[`app/main.py`](https://github.com/particle1331/inefficient-networks/blob/d3126c33f6a12e06203a6772d79eb0bbf4d10e2d/docs/notebooks/mlops/04-deployment/app/main.py)\n",
    "```\n",
    "```python\n",
    "import os\n",
    "from ride_duration.predict import load_model, make_prediction\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "\n",
    "# Load model with run ID and experiment ID defined in the env.\n",
    "RUN_ID = os.getenv(\"MODEL_RUN_ID\")\n",
    "EXPERIMENT_ID = os.getenv(\"EXPERIMENT_ID\")\n",
    "model = load_model(run_id=RUN_ID, experiment_id=EXPERIMENT_ID)\n",
    "\n",
    "app = Flask('duration-prediction')\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_endpoint():\n",
    "    \"\"\"Predict duration of a single ride using NYCRideDurationModel.\"\"\"\n",
    "    \n",
    "    ride = request.get_json()\n",
    "    preds = make_prediction(model, [ride])\n",
    "\n",
    "    return jsonify({\n",
    "        'duration': float(preds[0]),\n",
    "        'model_version': RUN_ID,\n",
    "    })\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=9696)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing the prediction endpoint, we have the following script. This can be used without modification to test remote hosts using port forwarding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`test.py`](https://github.com/particle1331/inefficient-networks/blob/d3126c33f6a12e06203a6772d79eb0bbf4d10e2d/docs/notebooks/mlops/04-deployment/test.py)\n",
    "```\n",
    "```python\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "ride = {\n",
    "    \"PULocationID\": 130,\n",
    "    \"DOLocationID\": 205,\n",
    "    \"trip_distance\": 3.66,\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    host = \"http://0.0.0.0:9696\"\n",
    "    url = f\"{host}/predict\"\n",
    "    response = requests.post(url, json=ride)\n",
    "    result = response.json()\n",
    "    \n",
    "    print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containerizing the application with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our `Dockerfile`, we start by installing Pipenv. Then, we copy all files relevant for running the application. So these are the requirements files, the model package files, and the files for the Flask app. Next, we install everything using Pipenv, expose the `9696` endpoint, and configure the entrypoint which serves the main app on `0.0.0.0:9696`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`Dockerfile`](https://github.com/particle1331/inefficient-networks/blob/69fc955d08a8bbb8d214cc753af35250c34f4a27/docs/notebooks/mlops/04-deployment/Dockerfile)\n",
    "```\n",
    "```Dockerfile\n",
    "FROM python:3.9.13-slim\n",
    "\n",
    "RUN pip install -U pip\n",
    "RUN pip install pipenv\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\",  \"./\"]\n",
    "COPY [ \"setup.py\", \"pyproject.toml\", \"MANIFEST.in\",  \"./\"]\n",
    "COPY [ \"ride_duration\",  \"./ride_duration\"]\n",
    "COPY [ \"app\",  \"./app\"]\n",
    "\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "EXPOSE 9696\n",
    "\n",
    "# https://stackoverflow.com/a/71092624/1091950\n",
    "ENTRYPOINT [ \"gunicorn\", \"--bind=0.0.0.0:9696\", \"--timeout=600\", \"app.main:app\" ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker build -t ride-duration-prediction-service:v1 .\n",
    "\n",
    "[+] Building 72.3s (13/13) FINISHED\n",
    " => [internal] load build definition from Dockerfile               0.1s\n",
    " => => transferring dockerfile: 388B                               0.0s\n",
    " => [internal] load .dockerignore                                  0.1s\n",
    " => => transferring context: 2B                                    0.0s\n",
    " => [internal] load metadata for docker.io/library/python:3.9.13-  3.4s\n",
    " => [internal] load build context                                  0.1s\n",
    " => => transferring context: 80.02kB                               0.1s\n",
    " => [1/8] FROM docker.io/library/python:3.9.13-slim@sha256:451ccc  0.0s\n",
    " => CACHED [2/8] RUN pip install -U pip                            0.0s\n",
    " => CACHED [3/8] RUN pip install pipenv                            0.0s\n",
    " => CACHED [4/8] WORKDIR /app                                      0.0s\n",
    " => [5/8] COPY [ Pipfile, Pipfile.lock, setup.py, pyproject.toml,  0.1s\n",
    " => [6/8] COPY [ ride_duration,  ./ride_duration]                  0.1s\n",
    " => [7/8] COPY [ app,  ./app]                                      0.0s\n",
    " => [8/8] RUN pipenv install --system --deploy                    64.8s\n",
    " => exporting to image                                             3.6s\n",
    " => => exporting layers                                            3.6s\n",
    " => => writing image sha256:1b2da34ca1b3504d45df527049f788a485713  0.0s\n",
    " => => naming to docker.io/library/ride-duration-prediction-servi  0.0s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the container:\n",
    "\n",
    "```bash\n",
    "docker run --env-file .env -it --rm -p 9696:9696 ride-duration-prediction-service:v1\n",
    "\n",
    "[2022-06-20 11:12:08 +0000] [1] [INFO] Starting gunicorn 20.1.0\n",
    "[2022-06-20 11:12:08 +0000] [1] [INFO] Listening at: http://0.0.0.0:9696 (1)\n",
    "[2022-06-20 11:12:08 +0000] [1] [INFO] Using worker: sync\n",
    "[2022-06-20 11:12:08 +0000] [9] [INFO] Booting worker with pid: 9\n",
    "Downloading model f4e2242a53a3410d89c061d1958ae70a from S3...\n",
    "2022/06/20 11:22:28 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
    " - psutil (current: uninstalled, required: psutil==5.9.1)\n",
    "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `--env-file .env` loads the environmental variables into the container. From the logs, we see that there is a mismatch between the training environment and the current one. In another terminal, let us test if prediction works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "❯ python test.py\n",
    "{'duration': 18.210770674183355, 'model_version': 'f4e2242a53a3410d89c061d1958ae70a'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial loading time, the next predictions are returned instantaneously. This confirms that the model is loaded only once when the server starts (or we can also log model loading to be really sure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming: Deploying models with Kinesis and Lambda\n",
    "\n",
    "A streaming service consists of **producers** and **consumers**. Producers push events to the event stream which are consumed by consuming services that react to this stream. Recall that a web service exhibits a 1-1 relationship so that there is explicit connection between user and service. On the other hand, the relationship between producing and consuming services can be 1-many or many-many. There is only implicit connection since we don't know which consumers will react or how many. Streaming services can be scaled to many services or models.\n",
    "\n",
    "For example, when a user uses our ride hailing app, the backend can send an event to the stream containing all information about this ride. Then, services will react on this event, e.g. one consuming service predicts tip and sends a push notification to user asking for the tip. And consuming services which makes better ride duration prediction but takes more time to make a prediction can update the prediction that was initially given to the user by the online web service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a serverless function for serving our model, we will use [AWS Lambda](https://aws.amazon.com/lambda/). The advantage of this is that we do not have to worry about owning a server that runs our function, we just know that the function is being executed somewhere in AWS. For the sake of demonstration, we will pretend that we are serving better model predictions, although we are actually deploying the same Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "1. Open the roles page in the IAM console and choose **Create role**. Choose Lambda as trusted entity type.\n",
    "    ```{figure} ../../../img/create-role.png\n",
    "    ---\n",
    "    ---\n",
    "    ```\n",
    "\n",
    "2. Add `AWSLambdaKinesisExecutionRole` to permissions. This role can read from the Kinesis stream and write logs. Resource `\"*\"` means that the role can do this to any Kinesis stream or log group.\n",
    "    ```{figure} ../../../img/create-role-2.png\n",
    "    ---\n",
    "    ---\n",
    "    ```\n",
    "\n",
    "3. Set name to `lambda-kinesis-role` and proceed to create role.\n",
    "    ```{figure} ../../../img/create-role-3.png\n",
    "    ---\n",
    "    ---\n",
    "    ```\n",
    "\n",
    "4. Go to Lambda and create `ride-duration-prediction-test` for testing. Later we will create the actual function. In the permissions, choose the role that we have just created.\n",
    "    ```{figure} ../../../img/create-function.png\n",
    "    ---\n",
    "    ---\n",
    "    ```\n",
    "\n",
    "    ```{figure} ../../../img/create-function-2.png\n",
    "    ---\n",
    "    ---\n",
    "    The `ride-duration-prediction-test` function.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify this to look more like our machine learning function. Note that unlike for a web service, there is no 1-1 relationship between inputs and outputs. We have to include a `ride_id` to tie an input event to its corresponding event in the output stream. In the code section write and deploy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# lambda_function.py\n",
    "import json\n",
    "\n",
    "def prepare_features(ride):\n",
    "    features = {}\n",
    "    features['PU_DO'] = f\"{ride['PULocationID']}_{ride['DOLocationID']}\"\n",
    "    features['trip_distance'] = ride['trip_distance']\n",
    "    return features\n",
    "\n",
    "def predict(features):\n",
    "    return 10.0\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    ride = event['ride']\n",
    "    ride_id = event['ride_id']\n",
    "\n",
    "    features = prepare_features(ride)\n",
    "    prediction = predict(features)\n",
    "\n",
    "    return {\n",
    "        'ride_duration': prediction,\n",
    "        'ride_id': ride_id\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the **test event** to:\n",
    "\n",
    "```{margin}\n",
    "**Toy test event**\n",
    "```\n",
    "```JSON\n",
    "{\n",
    "    \"ride\": {\n",
    "        \"PULocationID\": 130,\n",
    "        \"DOLocationID\": 205,\n",
    "        \"trip_distance\": 3.66\n",
    "    }, \n",
    "    \"ride_id\": 123\n",
    "}\n",
    "```\n",
    "\n",
    "Running the test. The model predicts a constant `10` minutes for each ride:\n",
    "\n",
    "```{figure} ../../../img/lambda-2.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching a Kinesis input data stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we attach a Kinesis data stream to our function. Go to Kinesis to create a data stream. We will call it `ride_events` and set its capacity mode to **Provisioned** with 1 shard. AWS provides the write and read capacity for the chosen number of shards. Note that we have to pay per hour for each shard.\n",
    "\n",
    "```{figure} ../../../img/kinesis-stream-1.png\n",
    "---\n",
    "width: 30em\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the Lambda function, we add `ride_events` as trigger:\n",
    "\n",
    "```{figure} ../../../img/kinesis-stream-2.png\n",
    "---\n",
    "width: 30em\n",
    "---\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the trigger is enabled, we execute the following the command line which puts a test event to the stream. Our lambda should read from the stream and we should see this in logs. We comment out our code, and only print out what a Kinesis event looks like:\n",
    "\n",
    "```python\n",
    "# lambda_function.py\n",
    "import json\n",
    "\n",
    "def prepare_features(ride):\n",
    "    features = {}\n",
    "    features['PU_DO'] = f\"{ride['PULocationID']}_{ride['DOLocationID']}\"\n",
    "    features['trip_distance'] = ride['trip_distance']\n",
    "    return features\n",
    "\n",
    "def predict(features):\n",
    "    return 10.0\n",
    "\n",
    "def lambda_handler(event, context):  \n",
    "    event_json = json.dumps(event)\n",
    "    print(event_json)\n",
    "\n",
    "    # ride = event['ride']\n",
    "    # ride_id = event['ride_id']\n",
    "\n",
    "    # features = prepare_features(ride)\n",
    "    # prediction = predict(features)\n",
    "\n",
    "    # return {\n",
    "    #     'ride_duration': prediction,\n",
    "    #     'ride_id': ride_id\n",
    "    # }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the above test event into the input stream:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Put record**\n",
    "<br>\n",
    "v. `aws-cli/1.22.34`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws kinesis put-record \\\n",
    "    --stream-name ride_events \\\n",
    "    --partition-key 1 \\\n",
    "    --data '{\n",
    "        \"ride\": {\n",
    "            \"PULocationID\": 130,\n",
    "            \"DOLocationID\": 205,\n",
    "            \"trip_distance\": 3.66\n",
    "        },\n",
    "        \"ride_id\": 123\n",
    "    }'\n",
    "```\n",
    "```bash\n",
    "{\n",
    "    \"ShardId\": \"shardId-000000000000\",\n",
    "    \"SequenceNumber\": \"49630706038424016596026506533783680704960320015674900482\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `SequenceNumber` of this event. This event can be found on the logs by looking at the \"Monitor\" tab of the lambda function and clicking on \"View logs in CloudWatch\". We update the test event in the lambda function with this record. In particular, this means we don't have to push events to the input stream when testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Actual test event**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"kinesis\": {\n",
    "                \"kinesisSchemaVersion\": \"1.0\",\n",
    "                \"partitionKey\": \"1\",\n",
    "                \"sequenceNumber\": \"49630706038424016596026506533782471779140474214180454402\",\n",
    "                \"data\": \"eyAgICAgICAgICAicmlkZSI6IHsgICAgICAgICAgICAgICJQVUxvY2F0aW9uSUQiOiAxMzAsICAgICAgICAgICAgICAiRE9Mb2NhdGlvbklEIjogMjA1LCAgICAgICAgICAgICAgInRyaXBfZGlzdGFuY2UiOiAzLjY2ICAgICAgICAgIH0sICAgICAgICAgICJyaWRlX2lkIjogMTIzICAgICAgfQ==\",\n",
    "                \"approximateArrivalTimestamp\": 1655944485.718\n",
    "            },\n",
    "            \"eventSource\": \"aws:kinesis\",\n",
    "            \"eventVersion\": \"1.0\",\n",
    "            \"eventID\": \"shardId-000000000000:49630706038424016596026506533782471779140474214180454402\",\n",
    "            \"eventName\": \"aws:kinesis:record\",\n",
    "            \"invokeIdentityArn\": \"arn:aws:iam::241297376613:role/lambda-kinesis-role\",\n",
    "            \"awsRegion\": \"us-east-1\",\n",
    "            \"eventSourceARN\": \"arn:aws:kinesis:us-east-1:241297376613:stream/ride_events\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/kinesis-stream-5.png\n",
    "---\n",
    "---\n",
    "Record with sequence number `496...402` printed in the logs.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that Kinesis encodes event data in `base64`, i.e. as `\"data\": \"eyAgI...\"`. So we have to decode this to be able to use it. Since the batch size is set to 100, we need to iterate over records to access each of them. Our function is modified as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# lambda_function.py\n",
    "...\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    for record in event['Records']:\n",
    "        encoded_data = record['kinesis']['data']\n",
    "        decoded_data = base64.b64decode(encoded_data).decode('utf-8')\n",
    "        ride_event = json.loads(decoded_data)\n",
    "\n",
    "        ride = ride_event['ride']\n",
    "        ride_id = ride_event['ride_id']\n",
    "    \n",
    "        features = prepare_features(ride)\n",
    "        prediction = predict(features)\n",
    "    \n",
    "        return {\n",
    "            'ride_duration': prediction,\n",
    "            'ride_id': ride_id\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing on the Kinesis test event. Observe that the data has been properly decoded:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/kinesis-stream-6.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing predictions to an output stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# lambda_function.py\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "kinesis_client = boto3.client('kinesis')\n",
    "PREDICTIONS_STREAM_NAME = os.getenv('PREDICTIONS_STREAM_NAME', 'ride_predictions')\n",
    "\n",
    "\n",
    "def prepare_features(ride):\n",
    "    features = {}\n",
    "    features['PU_DO'] = f\"{ride['PULocationID']}_{ride['DOLocationID']}\"\n",
    "    features['trip_distance'] = ride['trip_distance']\n",
    "    return features\n",
    "    \n",
    "    \n",
    "def predict(features):\n",
    "    return 10.0\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    prediction_events = []\n",
    "\n",
    "    for record in event['Records']:\n",
    "        encoded_data = record['kinesis']['data']\n",
    "        decoded_data = base64.b64decode(encoded_data).decode('utf-8')\n",
    "        \n",
    "        ride_event = json.loads(decoded_data)\n",
    "        ride = ride_event['ride']\n",
    "        ride_id = ride_event['ride_id']\n",
    "    \n",
    "        features = prepare_features(ride)\n",
    "        prediction = predict(features)\n",
    "        \n",
    "        prediction_event = {\n",
    "            'model': 'ride_duration_prediction_model',\n",
    "            'version': 123,\n",
    "            'prediction': {\n",
    "                'ride_duration': prediction,\n",
    "                'ride_id': ride_id\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/kinesis.html#Kinesis.Client.put_record\n",
    "        kinesis_client.put_record(\n",
    "            StreamName=PREDICTIONS_STREAM_NAME,\n",
    "            Data=json.dumps(prediction_event),\n",
    "            PartitionKey=str(ride_id)\n",
    "        )\n",
    "        \n",
    "        prediction_events.append(prediction_event)\n",
    "\n",
    "    return {\n",
    "        'predictions': prediction_events\n",
    "    } \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create ride_predictions kinesis stream\n",
    "\n",
    "multiple consumers push data to this output stream, \n",
    "so this lambda function kind of gives it's signature on the event.\n",
    "\n",
    "note that put_records which can support up to 500 records is cheaper, \n",
    "since you pay for each api call. but to make it simpler, we use put_record. \n",
    "\n",
    "update permissions to write to stream\n",
    "```\n",
    "  \"errorMessage\": \"An error occurred (AccessDeniedException) when calling the PutRecord operation: User: arn:aws:sts::241297376613:assumed-role/lambda-kinesis-role/ride-duration-prediction-test is not authorized to perform: kinesis:PutRecord on resource: arn:aws:kinesis:us-east-1:241297376613:stream/ride_predictions because no identity-based policy allows the kinesis:PutRecord action\",\n",
    "\n",
    "\n",
    "copy: arn:aws:kinesis:us-east-1:241297376613:stream/ride_predictions\n",
    "goto IAM lambda-kinesis-role\n",
    "add permission -> attach policies -> create policy\n",
    "\"lambda_kinesis_write_to_ride_predictions\"\n",
    "\n",
    "kinesis\n",
    "put record, put records\n",
    "\n",
    "very restrictive, it only has two write permissions on a particular resource.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from output stream:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Get records**\n",
    "<br>\n",
    "v. `aws-cli/1.22.34`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "KINESIS_STREAM_OUTPUT='ride_predictions'\n",
    "SHARD='shardId-000000000000'\n",
    "\n",
    "SHARD_ITERATOR=$(aws kinesis \\\n",
    "    get-shard-iterator \\\n",
    "        --shard-id ${SHARD} \\\n",
    "        --shard-iterator-type TRIM_HORIZON \\\n",
    "        --stream-name ${KINESIS_STREAM_OUTPUT} \\\n",
    "        --query 'ShardIterator' \\\n",
    ")\n",
    "\n",
    "RESULT=$(aws kinesis get-records --shard-iterator $SHARD_ITERATOR)\n",
    "\n",
    "echo ${RESULT} | jq -r '.Records[-1].Data' | base64 --decode | jq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "{\n",
    "  \"model\": \"ride_duration_prediction_model\",\n",
    "  \"version\": 123,\n",
    "  \"prediction\": {\n",
    "    \"ride_duration\": 10,\n",
    "    \"ride_id\": 123\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we only have one shard. if we have multiple shards we have to know which shard to read to.\n",
    "shard iterator gives us an id of an iterator -> that gives us a way to retrieve records from a stream. \n",
    "\n",
    "here we are reading without lambda so it's a bit more complex.\n",
    "later lambda hides all of these details. we only have to do this once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/kinesis-out-stream-1.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gallery.ecr.aws/lambda/python\n",
    "\n",
    "public.ecr.aws/lambda/python:3.9\n",
    "\n",
    "\n",
    "no need to set workdir already a workdir set\n",
    "\n",
    "```bash\n",
    "export $(cat .env | xargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker build -t stream-model-duration:v1 .\n",
    "docker run -it --rm -p 8080:8080 --env-file .env stream-model-duration:v1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing we want to do is to publish the container to ECR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "aws ecr create-repository --repository-name duration-model\n",
    "\n",
    "{\n",
    "    \"repository\": {\n",
    "        \"repositoryArn\": \"arn:aws:ecr:us-east-1:241297376613:repository/duration-model\",\n",
    "        \"registryId\": \"241297376613\",\n",
    "        \"repositoryName\": \"duration-model\",\n",
    "        \"repositoryUri\": \"241297376613.dkr.ecr.us-east-1.amazonaws.com/duration-model\",\n",
    "        \"createdAt\": 1656099951.0,\n",
    "        \"imageTagMutability\": \"MUTABLE\",\n",
    "        \"imageScanningConfiguration\": {\n",
    "            \"scanOnPush\": false\n",
    "        },\n",
    "        \"encryptionConfiguration\": {\n",
    "            \"encryptionType\": \"AES256\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "241297376613.dkr.ecr.us-east-1.amazonaws.com/duration-model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging in\n",
    "```\n",
    "$(aws ecr get-login --no-include-email)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "REMOTE_URI=241297376613.dkr.ecr.us-east-1.amazonaws.com/duration-model\n",
    "REMOTE_TAG=v1\n",
    "REMOTE_IMAGE=${REMOTE_URI}:${REMOTE_TAG}\n",
    "LOCAL_IMAGE=stream-model-duration:v1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker tag ${LOCAL_IMAGE} ${REMOTE_IMAGE}\n",
    "docker push ${REMOTE_IMAGE}\n",
    "\n",
    "The push refers to repository [241297376613.dkr.ecr.us-east-1.amazonaws.com/duration-model]\n",
    "d633cfbf6042: Pushed\n",
    "1e16d3c3a5e4: Pushing  124.5MB/657MB\n",
    "593e5b91fe04: Pushed\n",
    "ed5e98d9c477: Pushed\n",
    "a5a2488932a6: Pushed\n",
    "8071867dc313: Pushed\n",
    "39978c3cb375: Pushing  223.5MB\n",
    "6ea38db36806: Pushed\n",
    "f92fb29958b6: Pushed\n",
    "f1c31f6b2603: Pushed\n",
    "fe1bfb0e592a: Pushing  115.7MB/333.9MB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create new lambda function container\n",
    "- attach ride events kinesis\n",
    "- edit env variables for the docker container\n",
    "- give access to s3 (attach new policy)\n",
    "- All `List` and `Read`\n",
    "- Resource: bucket name, object bucket name = mlflow-models-ron, object name = *. `mlflow-models-ron/*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor0\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:Get*\",\n",
    "                \"s3:List*\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::mlflow-models-ron\",\n",
    "                \"arn:aws:s3:::mlflow-models-ron/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "aws kinesis put-record \\\n",
    "    --stream-name ride_events \\\n",
    "    --partition-key 1 \\\n",
    "    --data '{\n",
    "        \"ride\": {\n",
    "            \"PULocationID\": 130,\n",
    "            \"DOLocationID\": 205,\n",
    "            \"trip_distance\": 3.66\n",
    "        },\n",
    "        \"ride_id\": 999\n",
    "    }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    \"ShardId\": \"shardId-000000000000\",\n",
    "    \"SequenceNumber\": \"49630770830830710445310740585403183998225550034421678082\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "KINESIS_STREAM_OUTPUT='ride_predictions'\n",
    "SHARD='shardId-000000000000'\n",
    "\n",
    "SHARD_ITERATOR=$(aws kinesis \\\n",
    "    get-shard-iterator \\\n",
    "        --shard-id ${SHARD} \\\n",
    "        --shard-iterator-type TRIM_HORIZON \\\n",
    "        --stream-name ${KINESIS_STREAM_OUTPUT} \\\n",
    "        --query 'ShardIterator' \\\n",
    ")\n",
    "\n",
    "RESULT=$(aws kinesis get-records --shard-iterator $SHARD_ITERATOR)\n",
    "\n",
    "echo ${RESULT} | jq -r '.Records[-1].Data' | base64 --decode | jq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "{\n",
    "  \"model\": \"ride_duration_prediction_model\",\n",
    "  \"version\": 123,\n",
    "  \"prediction\": {\n",
    "    \"ride_duration\": 10,\n",
    "    \"ride_id\": 999\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying batch predictions\n",
    "\n",
    "For use cases that do not require the responsiveness of a web service, we can implement an offline service that makes batch predictions. Typically, offline services are expected to be done between fixed time periods, e.g. daily, weekly, or monthly. A critical element of this is **workflow orchestration** where we regularly pull from a database, make predictions on that data, then write the predictions on a database, or to a file that is uploaded to S3, or it can be pushed to an analytics dashboard thereby refreshing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do batch scoring, we simply load the model and use it to make prediction on a list of examples instead of a single example as in the web service. This is quite easily done using the `ride_duration` package. Note that the input file has no natural `id` column, so we randomly generate a [`uuid`](https://datatracker.ietf.org/doc/html/rfc4122.html) for each example which is a string that is expected to be unique across space and time. Our general workflow would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ride_duration.utils import load_training_dataframe\n",
    "from ride_duration.predict import load_model, make_prediction\n",
    "from uuid import uuid4\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv() # Load variables in .env\n",
    "\n",
    "input_file = f'https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.parquet'\n",
    "df = load_training_dataframe(input_file)\n",
    "\n",
    "model = load_model(os.getenv(\"EXPERIMENT_ID\"), os.getenv(\"MODEL_RUN_ID\"))\n",
    "preds = make_prediction(model, df)\n",
    "uuids = [str(uuid4()) for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output file includes all information relevant to modelling (features, targets, and predictions) that can be useful for analytics and monitoring. For observability, we include the generated `uuid` for each row and also include the `run_id` of the model used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>actual_duration</th>\n",
       "      <th>predicted_duration</th>\n",
       "      <th>diff</th>\n",
       "      <th>model_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84f54b9f-ac47-419c-a16e-765a6513dd17</td>\n",
       "      <td>2021-01-01 00:15:56</td>\n",
       "      <td>43</td>\n",
       "      <td>151</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>6.674317</td>\n",
       "      <td>-2.740983</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2a9413f-4130-434e-8a00-c3a45bdcf56a</td>\n",
       "      <td>2021-01-01 00:25:59</td>\n",
       "      <td>166</td>\n",
       "      <td>239</td>\n",
       "      <td>2.53</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>13.791950</td>\n",
       "      <td>-5.041950</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c572886d-afda-45ac-844c-37da663a1819</td>\n",
       "      <td>2021-01-01 00:45:57</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>1.12</td>\n",
       "      <td>5.966667</td>\n",
       "      <td>6.965782</td>\n",
       "      <td>-0.999115</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0133a7d3-3344-45e3-a7c5-a1e667da8cce</td>\n",
       "      <td>2020-12-31 23:57:51</td>\n",
       "      <td>168</td>\n",
       "      <td>75</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7.083333</td>\n",
       "      <td>11.486757</td>\n",
       "      <td>-4.403424</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37f16f1b-405f-4eab-a772-5eef2d32434c</td>\n",
       "      <td>2021-01-01 00:26:31</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2.316667</td>\n",
       "      <td>3.498446</td>\n",
       "      <td>-1.181780</td>\n",
       "      <td>f4e2242a53a3410d89c061d1958ae70a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ride_id lpep_pickup_datetime  PULocationID  \\\n",
       "0  84f54b9f-ac47-419c-a16e-765a6513dd17  2021-01-01 00:15:56            43   \n",
       "1  f2a9413f-4130-434e-8a00-c3a45bdcf56a  2021-01-01 00:25:59           166   \n",
       "2  c572886d-afda-45ac-844c-37da663a1819  2021-01-01 00:45:57            41   \n",
       "3  0133a7d3-3344-45e3-a7c5-a1e667da8cce  2020-12-31 23:57:51           168   \n",
       "7  37f16f1b-405f-4eab-a772-5eef2d32434c  2021-01-01 00:26:31            75   \n",
       "\n",
       "   DOLocationID  trip_distance  actual_duration  predicted_duration      diff  \\\n",
       "0           151           1.01         3.933333            6.674317 -2.740983   \n",
       "1           239           2.53         8.750000           13.791950 -5.041950   \n",
       "2            42           1.12         5.966667            6.965782 -0.999115   \n",
       "3            75           1.99         7.083333           11.486757 -4.403424   \n",
       "7            75           0.45         2.316667            3.498446 -1.181780   \n",
       "\n",
       "                      model_version  \n",
       "0  f4e2242a53a3410d89c061d1958ae70a  \n",
       "1  f4e2242a53a3410d89c061d1958ae70a  \n",
       "2  f4e2242a53a3410d89c061d1958ae70a  \n",
       "3  f4e2242a53a3410d89c061d1958ae70a  \n",
       "7  f4e2242a53a3410d89c061d1958ae70a  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = df[['lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance', 'duration']].copy()\n",
    "\n",
    "out['model_version']= os.getenv(\"MODEL_RUN_ID\")\n",
    "out['actual_duration'] = df.duration\n",
    "out['predicted_duration'] = preds\n",
    "out['diff'] = out.actual_duration - out.predicted_duration\n",
    "out['ride_id'] = uuids\n",
    "\n",
    "out = out[['ride_id', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance', 'actual_duration', 'predicted_duration', 'diff', 'model_version']]\n",
    "out.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying a batch scoring workflow in Prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "=== TODO (waiting for new video with orchestration :) ===\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Scoring script\n",
    "\n",
    "```{margin}\n",
    "[`score.py`](https://github.com/particle1331/inefficient-networks/blob/217134c84bb323452bf0dc3e8b6a6a04fea8f06b/docs/notebooks/mlops/04-deployment/score.py)\n",
    "```\n",
    "```python\n",
    "from ride_duration.utils import load_training_dataframe\n",
    "from ride_duration.predict import load_model, make_prediction\n",
    "\n",
    "\n",
    "def generate_uuids(n):\n",
    "    ride_ids = []\n",
    "    for i in range(n):\n",
    "        ride_ids.append(str(uuid.uuid4()))\n",
    "    return ride_ids\n",
    "\n",
    "\n",
    "def apply_model(\n",
    "    input_file: str, \n",
    "    run_id: str, \n",
    "    output_file: str\n",
    ") -> None:\n",
    "    \n",
    "    print(f'Reading the data from {input_file}...')\n",
    "    df = load_training_dataframe(input_file)\n",
    "    df['ride_id'] = generate_uuids(len(df))\n",
    "\n",
    "    print(f'Loading the model with RUN_ID={run_id}...')\n",
    "    model = load_model()\n",
    "\n",
    "    print(f'Applying the model...')\n",
    "    preds = make_prediction(model, df)\n",
    "\n",
    "    print(f'Saving the result to {output_file}...')\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['ride_id'] = df['ride_id']\n",
    "    df_result['lpep_pickup_datetime'] = df['lpep_pickup_datetime']\n",
    "    df_result['PULocationID'] = df['PULocationID']\n",
    "    df_result['DOLocationID'] = df['DOLocationID']\n",
    "    df_result['actual_duration'] = df['duration']\n",
    "    df_result['predicted_duration'] = preds\n",
    "    df_result['diff'] = df_result['actual_duration'] - df_result['predicted_duration']\n",
    "    df_result['model_version'] = run_id\n",
    "    df_result.to_parquet(output_file, index=False)\n",
    "\n",
    "\n",
    "def run(taxi_type: str, year: int, month: int, run_id: str) -> None:\n",
    "\n",
    "    source_url = 'https://s3.amazonaws.com/nyc-tlc/trip+data'\n",
    "    input_file = f'{source_url}/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    output_file = f'output/{taxi_type}/{year:04d}-{month:02d}.parquet'\n",
    "\n",
    "    apply_model(\n",
    "        input_file=input_file,\n",
    "        run_id=run_id,\n",
    "        output_file=output_file\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--taxi_type\", default='green', type=str)\n",
    "    parser.add_argument(\"--year\", default=2021, type=int)\n",
    "    parser.add_argument(\"--month\", default=1, type=int)\n",
    "    parser.add_argument(\"--run_id\", type=str)\n",
    "    parser.add_argument(\"--experiment_id\", type=int)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    run(\n",
    "        taxi_type=args.taxi_type,\n",
    "        year=args.year,\n",
    "        month=args.month,\n",
    "        run_id=args.run_id\n",
    "    )\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ```bash\n",
    "pipenv install --dev python-dotenv\n",
    "python score.py\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Model train script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training models that we use to serve predictions in our API, we use the following script. This trains a model using the `ride_duration` package (which ensures smooth integration with the Flask API) and logs the trained model to a remote MLflow tracking server. The tracking server host is provided as a command line argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`train.py`](https://github.com/particle1331/inefficient-networks/blob/217134c84bb323452bf0dc3e8b6a6a04fea8f06b/docs/notebooks/mlops/04-deployment/train.py)\n",
    "```\n",
    "```python\n",
    "import mlflow \n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from ride_duration.utils import load_training_dataframe, prepare_features\n",
    "\n",
    "\n",
    "def setup(tracking_server_host):\n",
    "    TRACKING_URI = f\"http://{tracking_server_host}:5000\"\n",
    "    mlflow.set_tracking_uri(TRACKING_URI)\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "\n",
    "\n",
    "def run_training(X_train, y_train, X_valid, y_valid):\n",
    "    with mlflow.start_run():\n",
    "        params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 20\n",
    "        }\n",
    "        \n",
    "        pipeline = make_pipeline(\n",
    "            DictVectorizer(), \n",
    "            RandomForestRegressor(**params, n_jobs=-1)\n",
    "        )\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_valid)\n",
    "        rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "        \n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"rmse_valid\", rmse)\n",
    "        mlflow.sklearn.log_model(pipeline, artifact_path='model')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--tracking-server-host\", type=str)\n",
    "    parser.add_argument(\"--train_path\", type=str)\n",
    "    parser.add_argument(\"--valid_path\", type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Getting data from disk\n",
    "    train_data = load_training_dataframe(args.train_path)\n",
    "    valid_data = load_training_dataframe(args.valid_path)\n",
    "\n",
    "    # Preprocessing dataset\n",
    "    X_train = prepare_features(train_data.drop(['duration'], axis=1))\n",
    "    X_valid = prepare_features(valid_data.drop(['duration'], axis=1))\n",
    "    y_train = train_data.duration.values\n",
    "    y_valid = valid_data.duration.values\n",
    "\n",
    "    # Push training to server\n",
    "    setup(args.tracking_server_host)\n",
    "    run_training(X_train, y_train, X_valid, y_valid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a55a0d1272a360f93e747858d443ec26da69f69eac36db3e567a961ca624a861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
