{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking and Model Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
    "\n",
    "<!-- Place this tag where you want the button to render. -->\n",
    "<a class=\"github-button\" href=\"https://github.com/particle1331/steepest-ascent\" data-color-scheme=\"no-preference: dark; light: light; dark: dark;\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star particle1331/steepest-ascent on GitHub\">Star</a>\n",
    "<!-- Place this tag in your head or just before your close body tag. -->\n",
    "<script async defer src=\"https://buttons.github.io/buttons.js\"></script> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we will look **experiment tracking** and **model management**. A machine learning experiment is defined as a session or process of making machine learning models. Experiment tracking is the process of keeping track of all relevant information in an experiments. This includes source code, environment, data, models, hyperparameters, and so on which are important for reproducing the experiment as well as for making actual predictions. \n",
    "\n",
    "From experience, we know that manual tracking, e.g. with spreadsheets, is error prone, not standardized, has low visibility, and difficult for teams to collaborate over. As an alternative, we will experiment tracking platforms such as [MLFlow](https://mlflow.org/). MLFlow has four main components: tracking, models, model registry, and projects. In this course, we only be cover the first three. \n",
    "\n",
    "```{margin}\n",
    "⚠️ **Attribution:** These are notes for [Module 2](https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/02-experiment-tracking) of the [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp). The MLOps Zoomcamp is a free course from [DataTalks.Club](https://github.com/DataTalksClub).\n",
    "```\n",
    "\n",
    "As we have seen with our previous prototyping, having the ability to reproduce results is important since we want to have the same results when deploying the model in different environments. Using experiment tracking and model management platforms allows us to have better chance at reproducing our results, as well as aid in organization (staging and deploying models) and optimization (finding the best models). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started: MLFlow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the MLFlow UI with an SQLite backend as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "$ mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "\n",
    "[2022-05-26 19:35:22 +0800] [92498] [INFO] Starting gunicorn 20.1.0\n",
    "[2022-05-26 19:35:22 +0800] [92498] [INFO] Listening at: http://127.0.0.1:5000 (92498)\n",
    "[2022-05-26 19:35:22 +0800] [92498] [INFO] Using worker: sync\n",
    "[2022-05-26 19:35:22 +0800] [92499] [INFO] Booting worker with pid: 92499\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our experiment, we will use our code and data from [Module 1](https://particle1331.github.io/inefficient-networks/notebooks/mlops/1-intro.html). So before doing any run, we either create an **experiment** or connect a run to it if the experiment already exists. This also sets the experiment tracking backend. The same one that is visualized in the UI above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`experiment_lr.py`](https://github.com/particle1331/inefficient-networks/blob/mlops/docs/notebooks/mlops/2-mlflow/experiment_lr.py#L23-L25)\n",
    "```\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section of the code executes a **single run** of the experiment. Note the logging at the end of the script. Everything that runs inside the following context is a single run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`experiment_lr.py`](https://github.com/particle1331/inefficient-networks/blob/mlops/docs/notebooks/mlops/2-mlflow/experiment_lr.py#L29-L52)\n",
    "```\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Logging\n",
    "    rmse_train = mean_squared_error(y_train, model.predict(X_train), squared=False)\n",
    "    rmse_valid = mean_squared_error(y_valid, model.predict(X_valid), squared=False)\n",
    "\n",
    "    fig = plot_duration_distribution(model, X_train, y_train, X_valid, y_valid)\n",
    "    fig.savefig('plot.svg')\n",
    "\n",
    "    mlflow.set_tag('author', 'particle')\n",
    "    mlflow.set_tag('model', 'baseline')\n",
    "    \n",
    "    mlflow.log_param('train_data_path', train_data_path)\n",
    "    mlflow.log_param('valid_data_path', valid_data_path)\n",
    "    \n",
    "    mlflow.log_metric('rmse_train', rmse_train)\n",
    "    mlflow.log_metric('rmse_valid', rmse_valid)\n",
    "    \n",
    "    mlflow.log_artifact(artifacts / 'plot.svg')\n",
    "    mlflow.log_artifact(artifacts / 'preprocessor.pkl', artifact_path='preprocessing')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this script, we see in the UI as a single run under the `nyc-tax-experiment` experiment. MLFlow is able to obtain the version from `git` and the user from the system, i.e. the user that is currently logged in. The other values are obtained from the logs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/single-run-mlflow.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we click on the run, we can see all details about it that were logged. Shown here are the date of the run, the user that executed it, total runtime, the source code used, as well as the git commit hash. Hence, it is best practice to always commit before running experiments. This ties your runs with a specific version of the code. Status `FINISHED` indicates that the script successfully ran. These are useful metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/mlflow-single-run.png\n",
    "---\n",
    "---\n",
    "Logged run of the baseline model. MLFlow allows us to preview saved artifacts. Shown here is a plot of distribution of true and predicted target values.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the details of the trained model, we have parameters which here include only the path of the dataset for training and validation (only paths, no versioning). Most importantly, we can see the logged RMSEs `5.7` (train) and `7.759` (valid). The plot of the distributions of the true and predicted distributions which we logged as a training artifact is also conveniently displayed here. Finally, we log the preprocessor as **artifact** which will be needed for preprocessing test data during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we go deeper into experiment tracking with MLFlow. We show how to iterate over different models and different parameters. This really just involves wrapping the run function in a loop. The nontrivial part is how to construct the sequence of parameters to loop over. For this we use [Hyperopt](https://hyperopt.github.io/hyperopt/) which implements the [TPE algorithm](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf). We also look at the **autologging** feature which makes logging easier for supported frameworks (e.g. also automates things like logging intermediate values for models trained incrementally such as XGBoost or neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate different models, we will define a `run` function that takes in parameters that define and configure a run of the experiment. Then, we define a `main` function that controls the runs that will be executed. Here the `model_class` parameter controls which scikit-learn model is used to model the dataset. Note that this connects to the same tracking URI and same experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`experiment_sklearn.py`](https://github.com/particle1331/inefficient-networks/blob/mlops/docs/notebooks/mlops/2-mlflow/experiment_sklearn.py)\n",
    "```\n",
    "```python\n",
    "def setup():\n",
    "    \n",
    "    [...]\n",
    "\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "\n",
    "def run(model_class):\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        model = model_class()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    [...]\n",
    "\n",
    "\n",
    "def main():\n",
    "    for model_class in [\n",
    "        Ridge,\n",
    "        RandomForestRegressor, \n",
    "        GradientBoostingRegressor,\n",
    "    ]:\n",
    "        run(model_class)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup()\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GradientBoostingRegressor` has the best validation score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/sklearn-results.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that autologging is available for scikit-learn models. Using this, parameters (even default ones) are automatically logged. Also, this generates the `MLModel` file along with the environments files. We can also see the `estimator_class` of the model as well as its input and output signature which is really convenient to know. Note that autologging can be activated by calling `mlflow.<framework>.autolog()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/autolog-sk1.png\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/autolog-sk2.png\n",
    "---\n",
    "---\n",
    "Autologging of a Random Forest model in scikit-learn.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous runs, `GradientBoostingRegressor` has the best performance on this task. So we try out XGBoost next. Moreover, we find the best parameter of XGBoost using Hyperopt. Here the parameters are sampled using the TPE algorithm over the search space defined in the `search_space` dictionary. We look at functionalities MLFlow provides to assist with hyperparameter optimization.\n",
    "\n",
    "As before, we will define a `setup` function which sets up the required datasets and connections, define a run function (here named `objective`), and a `main` function to facilitate the runs. Finally, we define command line arguments, so we can easily control the details of the runs in the command line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`experiment_xgboost.py`](https://github.com/particle1331/inefficient-networks/blob/mlops/docs/notebooks/mlops/2-mlflow/experiment_xgboost.py)\n",
    "```\n",
    "```python\n",
    "def setup(autolog):\n",
    "\n",
    "    [...]\n",
    "\n",
    "    xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    xgb_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    # Set experiment\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    mlflow.xgboost.autolog(disable=not(autolog))\n",
    "\n",
    "\n",
    "def objective(params, autolog):\n",
    "    \"\"\"Compute validation RMSE (one trial = one run).\"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=xgb_train,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(xgb_valid, 'validation')],\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "\n",
    "        # Logging\n",
    "        [...]\n",
    "\n",
    "        if not autolog:\n",
    "            mlflow.xgboost.log_model(model, 'model')\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "    return {'loss': rmse_valid, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'reg:squarederror',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "\n",
    "def main(autolog, num_runs):\n",
    "    best_result = fmin(\n",
    "        fn=partial(objective, autolog=autolog),\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_runs,\n",
    "        trials=Trials()\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--autolog\", choices=[\"False\", \"True\"])\n",
    "    parser.add_argument(\"--num_runs\", default=1, type=int)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Experiment runs\n",
    "    autolog = (args.autolog == 'True')\n",
    "    setup(autolog=autolog)\n",
    "    main(autolog=autolog, num_runs=args.num_runs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the details of a single XGBoost run with autologging. Here all 14 parameters of XGBoost are logged (much more than what we have in our search space). Also, we have feature importances and the `MLModel` file along with the environments files. Moreover, we get metrics that are relevant for models trained incrementally, such as the best and stopped iterations (early stopped), as well as a plot of the intermediate values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/xgboost-run.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/xgboost-intermediate.png\n",
    "---\n",
    "---\n",
    "Intermediate values plot for an autologged XGBoost model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We executed 30 runs of the XGBoost model (+1 with autologging). This can be analyzed using the compare button after selecting the results with the search query `tags.model = 'xgboost'`. Below we analyze the runs with the parallel coordinate plot. First, we have the specify the parameters and the metric. Then, we can use filters to filter out ranges that result in low objective values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/xgboost-parallel.png\n",
    "```\n",
    "\n",
    "In the other tabs, we also have scatter and contour interactive [plotly](https://plotly.com/) plots:\n",
    "\n",
    "```{figure} ../../../img/xgboost-scatter.png\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/xgboost-contour.png\n",
    "---\n",
    "---\n",
    "Plots for analyzing the hyperparameter search space of an XGBoost model on this dataset.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the best model, first we filter with `tags.model = 'xgboost' and metrics.rmse_valid < 6.4` and order by ascending validation RMSE by clicking the column header. Then, we can look at times such as train duration and inference time (not logged). The tradeoffs should be weighed against production requirements. Next step is to analyze the subsets of the data where these top models are wrong, perhaps create an ensemble of them, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/xgboost-select.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we take a deeper look at **model management**. In addition to experiment tracking, part of model management is to do model versioning and model deployment. Model management using file systems, typically involving only a file name and date modified, is error prone. There is no versioning and no [model lineage](https://aws.amazon.com/blogs/machine-learning/model-and-data-lineage-in-machine-learning-experimentation/). Model lineage refers to all associations between a model and all components involved in its creation.\n",
    "Having no model lineage makes it difficult to track results and progress, not to mention reproducing them. \n",
    "\n",
    "For each run a `run_uuid` key is assigned which maps to information such as metrics, source version, and other metadata in the database. If properly done, this takes care of versioning. Also, each run corresponds to an `artifacts_uri` which simply corresponds to a directory in disk containing logged artifacts. These can be viewed by opening the database file in a database explorer.\n",
    "\n",
    "Autologging also generates a [`MLModel` file](https://www.mlflow.org/docs/latest/models.html) which provides a standard format for packaging models that can be used in a variety of downstream tools (e.g. serving through a REST API or batch inference on Apache Spark). This includes package dependencies and Python version used for training, as well as the input and output signature of the model.\n",
    "\n",
    "Both of these should allow you to reproduce results of experiment runs with relative ease. Indeed, to demonstrate this, we perform inference below using the stored preprocessing pipeline and load the model from the `MLModel` file. Both can be retrieved using the `run_uuid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/autologging-1.png\n",
    "---\n",
    "---\n",
    "In addition to run metadata, the `MLModel` file obtained with autologging preserves model lineage.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we try to perform inference on test data using the logged model and artifacts. Note that new data has no labels when processed for inference. In reality, we will use the validation dataset as our test dataset. Also, for the sake of checking correctness, we try to reproduce the valid RMSE of `6.656`. Recall that the model is validated only for rides between 1 and 60 minutes in duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.655558028101635\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import joblib\n",
    "from utils import runs, data_path, compute_targets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Configure paths to run\n",
    "run_path = runs / '1' / 'f46f9fedb22c4411b6e265f8e65edbe3'\n",
    "model_path = run_path / 'artifacts' / 'model'\n",
    "model_artifacts_path = run_path / 'artifacts' / 'preprocessing'\n",
    "valid_data_path = data_path / 'green_tripdata_2021-02.parquet'\n",
    "\n",
    "# Preprocessing test data\n",
    "feature_pipe = joblib.load(model_artifacts_path / 'preprocessor.pkl')\n",
    "X = pd.read_parquet(valid_data_path)\n",
    "y = compute_targets(X)\n",
    "\n",
    "X = X[(y >= 1) & (y <= 60)]\n",
    "y = y[(y >= 1) & (y <= 60)]\n",
    "X = feature_pipe.transform(X)\n",
    "\n",
    "# Inference using MLModel file\n",
    "loaded_model = mlflow.pyfunc.load_model(model_path)\n",
    "\n",
    "# Reproducing metric: expected 6.656\n",
    "print(mean_squared_error(y, loaded_model.predict(X), squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://databricks.com/blog/2020/04/15/databricks-extends-mlflow-model-registry-with-enterprise-features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New model -> good performance -> plz deploy. \n",
    "\n",
    "what has changed? is there any preprocessing needed? what is the environment, what are the dependencies? \n",
    "\n",
    "back and fort communication with data scientist.\n",
    "\n",
    "what if incident in prod, need rollback, need to go to email inbox. imagine its not possible to run this model again, have to retrain from scratch. but what if retraining information is lost, no idea of env used, which dataset, and so on. \n",
    "\n",
    "recall we have tracking server, which stored experiment runs. model registry has stages. staging -> prod -> archive. DS only decides which models are ready for prod. deployment engineer not responsible. \n",
    "\n",
    "then if we want rollback, we can go back from archive to prod. note that model registry not deployment. it just labels models. need to compliment with ci/cd. in order to do actual deployment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing models: \n",
    "- look at duration, and model size, rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "information you need to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hjg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class ConvertToString(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Convert columns of DataFrame to type string.\"\"\"\n",
    "\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[self.features] = X[self.features].astype(str)\n",
    "        return X\n",
    "\n",
    "\n",
    "class AddPickupDropoffPair(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Add product of pickup and dropoff locations.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X['PU_DO'] = X['PULocationID'].astype(str) + '_' + X['DOLocationID'].astype(str)\n",
    "        return X\n",
    "\n",
    "\n",
    "class ConvertToDict(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Convert tabular data to feature dictionaries.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.to_dict(orient='records')\n",
    "\n",
    "\n",
    "class SelectFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Convert tabular data to feature dictionaries.\"\"\"\n",
    "\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.features]\n",
    "\n",
    "\n",
    "def preprocess_train_targets(data):\n",
    "    \"\"\"Derive target and filter outliers.\"\"\"\n",
    "\n",
    "    # Create target column and filter outliers\n",
    "    data['duration'] = data.lpep_dropoff_datetime - data.lpep_pickup_datetime\n",
    "    data['duration'] = data.duration.dt.total_seconds() / 60\n",
    "    \n",
    "    X = data[(data.duration >= 1) & (data.duration <= 60)]\n",
    "    y = X.duration.values\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<73908x13221 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 147816 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categorical = ['PU_DO']\n",
    "numerical = ['trip_distance']\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    AddPickupDropoffPair(),\n",
    "    ConvertToString(categorical),\n",
    "    SelectFeatures(categorical + numerical),\n",
    "    ConvertToDict(),\n",
    "    DictVectorizer(),\n",
    ")\n",
    "\n",
    "train_data_path =  '../data/green_tripdata_2021-01.parquet'\n",
    "X_train, y_train = preprocess_train_targets(pd.read_parquet(train_data_path))\n",
    "\n",
    "pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13221"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AddPickupDropoffPair().transform(X_train)['PU_DO'].nunique() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13221"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe.named_steps['dictvectorizer'].feature_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Utility code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c69054bf84d6ad36c63a02b926ed0729b159d9b327d6020e8e7aee9c3ae1ac1f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlops')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
