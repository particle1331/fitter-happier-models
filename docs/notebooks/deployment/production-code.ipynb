{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging Production Code [DOING]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Ongoing&color=orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Production code is designed to be deployed to end users as opposed to research code, which\n",
    "is for experimentation, building proof of concepts, and research code tends to be more short term in nature. \n",
    "On the other hand, with production code, we have some new considerations:\n",
    "\n",
    "- **Testability and maintainability** are huge.\n",
    "We want to divide up our code into modules which are more extensible and easier to test.\n",
    "We separate config from code where possible, and we ensure that functionality is tested and documented. We also look to ensure that our code adheres to standards like PEP 8 so that it's easy for others to\n",
    "read.  \n",
    "\n",
    "+++\n",
    "\n",
    "- **Scalability and performance** are also important areas to consider.\n",
    "With our production code, the code needs to be ready to be deployed to infrastructure that can be scaled. And in modern web applications, this typically means containerisation for vertical or horizontal scaling. And we'll be looking at that more in later sections.\n",
    "Where appropriate, we might also refactor inefficient parts of the code base.  \n",
    "\n",
    "+++\n",
    "\n",
    "- Finally, we have to look at **reproducibility**.\n",
    "The code resides under version control with clear processes for tracking releases and release versions, requirements, files, mark which dependencies and which versions are used by the code.\n",
    "\n",
    "```{margin}\n",
    "A **module** is basically just a Python file and a **package** is a\n",
    "collection of modules.\n",
    "```\n",
    "\n",
    "So that's a quick overview of some of the key considerations with production code. In this article, we will be packaging up our machine learning model into what's called the Python package. But a package has certain standardized files which have to be present so that it can be published and then installed in other Python applications.\n",
    "Packaging allows us to wrap our train model and make it available to other consuming applications as a dependency, but with the additional benefits of version control, clear metadata and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to create a package, we'll have to follow certain Python standards and conventions and\n",
    "we'll go into those in detail in this section.\n",
    "We'll end up with a package structure that looks like this.\n",
    "And by the time we get to the end of this section, all of this will be familiar.\n",
    "\n",
    "```\n",
    ".\n",
    "├── regression_model\n",
    "│   ├── config\n",
    "│   ├── datasets\n",
    "│   ├── processing\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── data_manager.py\n",
    "│   │   ├── features.py\n",
    "│   │   └── validation.py\n",
    "│   ├── trained_models\n",
    "│   ├── pipeline.py\n",
    "│   ├── predict.py\n",
    "│   ├── train_pipeline.py\n",
    "│   ├── config.yml\n",
    "│   └── VERSION\n",
    "├── requirements\n",
    "│   ├── requirements.txt\n",
    "│   └── test_requirements.txt\n",
    "├── tests\n",
    "│   ├── conftest.py\n",
    "│   ├── test_features.py\n",
    "│   └── test_prediction.py\n",
    "├── MANIFEST.in\n",
    "├── mypy.ini\n",
    "├── pyproject.toml\n",
    "├── setup.py\n",
    "└── tox.ini\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these files here the `MANIFEST.in`, `pyproject.toml`, `setup.py`, `mypy.ini` and `tox.ini` file are all things that we're using, either for packaging or for configuring things like linting and type checking. These are kinds of tooling configurations and will be coming back to discuss these in more detail below. We have a `requirements` directory, which is where we formalize the dependencies for our package and also for testing it. And we have a couple of sample tests in the `tests` directory.  \n",
    "\n",
    "\n",
    "The `regression_model` directory is where the majority of our functionality is. In this directory, we have three key files `train_pipeline.py`, `predict.py` and `pipeline.py`.\n",
    "These are sort of top level files for the key bits of functionality of the package. The `processing` directory contains different helper functions. We have the datasets that we need to train and test the models in the `datasets` directory. The `trained_models` directory is where we save the models that we're persisting\n",
    "here as a pickle file so that it can be loaded in and accessed in the future. Finally, `config` contains the core configurations module which reads the `config.yml` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the code structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, there are three ways that we broadly speaking use to determine how to create the structure for the production code.\n",
    "Those three are conventions, packaging mandatory files and software engineering best practices, and we'll go into each of these and provide some examples.\n",
    "\n",
    "### Conventions\n",
    "\n",
    "So conventions are things that you don't have to do.\n",
    "You're not going to get something like a Python syntax error if you don't do it.\n",
    "But projects tend to adhere to some sort of convention in a particular area.\n",
    "For example, the most obvious one is PEP 8, which is the Python style guide.\n",
    "This provides loads of guidelines on how to write Python code.\n",
    "Similar to that, we add a few linting tools into our project a bit later in the section.\n",
    "Those will also be opinionated and want you to adhere to certain conventions.\n",
    "And that can be really useful.\n",
    "\n",
    "\n",
    "### Mandatory files\n",
    "\n",
    "Next up, we have things that once we've made a certain decision, we then have to do so.\n",
    "We've made the decision to package our model into a Python package once we've made that decision. Then, there are certain files that are required in order to do that, like the `setup.py` file and the `MANIFEST.in` file.\n",
    "You have to have these files in order to publish a Python package, so certain decisions basically get made for us in terms of the structure and the files we need.\n",
    "Again, broadly speaking, packaging is a really easy way for us to embed our model as a dependency in an application that runs.\n",
    "There are other ways that you can persist and share your models.\n",
    "But a Python package is a nice, easy way for us.\n",
    "To do this makes it easy to version, makes it really easy to install into our consuming application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices \n",
    "\n",
    "```{margin}\n",
    "**End user functionality** \n",
    "```\n",
    "\n",
    "Software engineering best practices can be separated into two broad categories. First, we have the stuff that is relevant for the end user of your package.\n",
    "In our case, imagine that we published this package now as the end user, perhaps creating my application,\n",
    "and I want to make predictions using this machine learning model package that I've installed.\n",
    "In my application code, I'm going to import things like the ability to predict, right?\n",
    "And that will come from the predict module. I don't want to also import all the training functions. In fact, that might be confusing for me.\n",
    "So here we have a predict and a train separation and that feeds into just general software engineering good practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Testing and maintainability** \n",
    "```\n",
    "\n",
    "The second broad category relates to testing and maintanability.\n",
    "For testing, you want to think about, can you write a single unit test for one specific bit\n",
    "of functionality?\n",
    "The most extreme case of this production code would be that we put almost everything in a single file, right?\n",
    "Now, I think everybody intuitively knows that that would be messy and hard to maintain.\n",
    "But you then continue that thread and keep following it.\n",
    "And what you start to arrive at is this idea of separation of concerns, tests, ability.\n",
    "For example, if we look at the `data_manager.py`,\n",
    "we have a function for saving the pipeline and we have a function for removing old pipelines.\n",
    "Now these could be one single function. But we choose not to have them as one function because now in a unit test, we can test removing old\n",
    "pipelines as well as saving any pipeline. This makes testing a specific discrete unit of functionality a lot easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to separation\n",
    "of concerns, which leads to modularity.\n",
    "And as the name implies, separation of concerns is about designing software systems so that different\n",
    "areas of functionality are separated.\n",
    "For example, this is why we have different modules for the `data_manager.py` module, which deals with loading the training data, and then we have the validation module in `validation.py`,\n",
    "which deals with testing that our inputs to the model are correct.\n",
    "Now you could easily have these in the same module, but we choose to separate them because they're doing different things. And again, that links to testability. This is related to the [SOLID principles](https://en.wikipedia.org/wiki/SOLID) where the first stands for single responsibility which again means that \n",
    "a class or module should have one job. Here we have `predict.py` only dealing with prediction and `train_pipeline.py` dealing with training models.\n",
    "This is just engineering good practice that links to maintainability.\n",
    "\n",
    "Imagine you want to modify this package, you're going to create a pull request.\n",
    "And if you're the person reviewing this pull request and everything is all in one file, it's really difficult for you to track what's changed and understand exactly what you need to be reviewing. On the other hand, if the change is made in just a single small module, which only has code related to one area in it, it's way easier for you to immediately get the context, follow the flow of the\n",
    "program and review that pull request.\n",
    "So, this is the long term health of your code and making it nice and easy for maintainers.\n",
    "You can apply similar thought processes when you're deciding how to structure your own projects. \n",
    "\n",
    "Another best practice related to maintainability is optimizing for readability. \n",
    "The [principle of least astonishment](https://en.wikipedia.org/wiki/Principle_of_least_astonishment) is one that is very useful.\n",
    "You go into the code base, you look at the directories and the principle of least astonishment applies to a lot to naming. Do modules do what you expect them to do when you read the names and the same for classes and functions?\n",
    "A good way to think about this is to imagine that the person who's going to maintain your code after you is a psychopath who knows where you live.\n",
    "\n",
    "\n",
    "```{note}\n",
    "Follow a standard where possible (PEP 8), pick a linting tool and then stick with it.\n",
    "In our case, publishing models as packages gives us a nice structure to work with. This is a recommended starting point until you get to some more complex applications where publishing as a package may not be possible. At that point, it might be time to find a new standard.\n",
    "Established conventions usually adhere to good practices, but not always.\n",
    "And learning when an established convention actually isn't the right choice for you takes a lot of time and practice. But that's something to work towards.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have this `requirements` directory with two requirements files inside it.\n",
    "Let's start with the `requirements.txt` file.\n",
    "We have some comments at the top explaining some of the choices we've made here.\n",
    "And then from line four onwards, starting at non-pay, we define the dependencies of our projects as\n",
    "well as the version numbers of each of these dependencies which we were willing to accept, so which\n",
    "is compatible with our particular package.\n",
    "\n",
    "```{margin}\n",
    "[`requirements/requirements.txt`](https://github.com/particle1331/model-deployment/blob/main/05-production/requirements/requirements.txt)\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "# We use compatible release functionality (see PEP 440 here: https://www.python.org/dev/peps/pep-0440/#compatible-release)\n",
    "# to specify acceptable version ranges of our project dependencies. This gives us the flexibility to keep up with small\n",
    "# updates/fixes, whilst ensuring we don't install a major update which could introduce backwards incompatible changes.\n",
    "numpy>=1.22.0,<1.23.0\n",
    "pandas>=1.4.0,<1.5.0\n",
    "pydantic>=1.8.1,<1.9.0\n",
    "scikit-learn>=1.0.0,<1.1.0\n",
    "strictyaml>=1.3.2,<1.4.0\n",
    "ruamel.yaml==0.16.12\n",
    "feature-engine>=1.0.2,<1.1.0\n",
    "joblib>=1.0.1,<1.1.0\n",
    "```\n",
    "\n",
    "\n",
    "Now, these versions all adhere to what's known as [semantic versioning](https://www.geeksforgeeks.org/introduction-semantic-versioning/). For example, we have version `4.3.0`\n",
    "You have a major version `4` followed by a period followed by the minor version `3`, followed by a period followed by what's known as a patch version `0` (bug fix).\n",
    "And for a well maintained package, you'd expect that a minor version increment does not break the API\n",
    "and a major version increment is likely to break the API.\n",
    "But you have to be a bit careful because some less well maintain packages may actually introduce breaking\n",
    "changes, even in a minor version bump.\n",
    "So what we've done in our requirements file is play it quite conservatively.\n",
    "We're alowing patches but not minor version increases. And it's up to you and your projects, how much risk you want to take.\n",
    "\n",
    "We've split the requirements into these two files because there will be scenarios where we don't actually need to install these test requirements because these are only required when we want to test our package or when we want to run style checks, linting and type checks.\n",
    "\n",
    "```{margin}\n",
    "[`requirements/test_requirements.txt`](https://github.com/particle1331/model-deployment/blob/main/05-production/requirements/test_requirements.txt)\n",
    "```\n",
    "\n",
    "```\n",
    "# Install requirements.txt along with others\n",
    "-r requirements.txt\n",
    "\n",
    "# Testing requirements\n",
    "pytest>=6.2.3,<6.3.0\n",
    "\n",
    "# Repo maintenance tooling\n",
    "black==20.8b1\n",
    "flake8>=3.9.0,<3.10.0\n",
    "mypy==0.812\n",
    "isort==5.8.0\n",
    "```\n",
    "\n",
    "\n",
    "These two files are really important.\n",
    "If we don't define which versions of our dependencies we expect, then it can result in our package becoming very brittle and broken.\n",
    "The most basic error in a package would be that you didn't have a requirement to file.\n",
    "That would just mean that when somebody else tried to install and use the package, it would fail because it wouldn't have its necessary dependencies.\n",
    "A more likely scenario is that we neglect to include a version, and if there is no version specified, Pip is just going to assume you want the latest version of a particular dependency. And it may well be that that version has progressed and it's released new features, a new major version perhaps, and it's got breaking changes in the API.\n",
    "And that means that your package or our package in this case is going to break.\n",
    "So it's really important for us to define these versions.\n",
    "\n",
    "\n",
    "This `requirements.txt` approach to managing our projects dependencies is probably the most basic way of doing dependency management in Python.\n",
    "Nothing wrong with it at all. Many of the biggest python open source projects out there use this exact approach. However, just to avoid confusion.\n",
    "Poetry or Pipenv, then the the file used to define dependencies changes depending on which dependency manager you're working with. But the principle.\n",
    "Of defining your dependencies and specifying the version ranges remains the same across all of the tools. So focus on the principle, not so much on the exact syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with tox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now what we're going to do is see our package in action on some of its main commands.\n",
    "To start, if we've just cloned the repository and we have a look at our `trained_models` directory, which is one of the directories inside the `regression_model` directory, you can see that we just have the `__init__.py` file. There are no other files inside train models right now. We can generate a trained model serialized as a `.pkl` file by running:\n",
    "\n",
    "```\n",
    "tox -e train\n",
    "``` \n",
    "\n",
    "\n",
    "What happened here is that we've used `tox` to trigger our train pipeline script.\n",
    "So what's `tox`? How does it work? `tox` is a generic virtual environment management and test command line tool.\n",
    "For our purposes here, this means that with `tox`, we don't have to worry about different operating systems. We can run `tox` on Windows, macOS, Linux and get the same behavior across the platforms. We don't have to worry about things like setting up Python paths, configuring environment variables. We do all of that stuff inside our `tox.ini` file.\n",
    "So it's a really powerful way for us to be able to run tests of different versions of Python.\n",
    "Set up environment variables get over operating system differences.\n",
    "This is a great tool, and it's worth adding to your toolbox to get started with tox.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ pip install tox\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's set up, then we're going to start using tox and all our tox configuration is in the `tox.ini` file. There's a few comments at the top here explaining more about tox, \n",
    "what it is and why we're using it. There's quite a lot happening in our `tox.ini` file.\n",
    "\n",
    "```{margin}\n",
    "[`05-production/tox.ini`](https://github.com/particle1331/model-deployment/blob/main/05-production/tox.ini)\n",
    "```\n",
    "\n",
    "```ini\n",
    "# Tox is a generic virtualenv management and test command line tool. Its goal is to\n",
    "# standardize testing in Python. We will be using it extensively in this course.\n",
    "\n",
    "# Using Tox we can (on multiple operating systems):\n",
    "# + Eliminate PYTHONPATH challenges when running scripts/tests\n",
    "# + Eliminate virtualenv setup confusion\n",
    "# + Streamline steps such as model training, model publishing\n",
    "\n",
    "\n",
    "[tox]\n",
    "envlist = test_package, typechecks, stylechecks, lint\n",
    "skipsdist = True\n",
    "\n",
    "\n",
    "[testenv]\n",
    "install_command = pip install {opts} {packages}\n",
    "\n",
    "\n",
    "[testenv:test_package]\n",
    "deps =\n",
    "\t-rrequirements/test_requirements.txt\n",
    "\n",
    "setenv =\n",
    "\tPYTHONPATH=.\n",
    "\tPYTHONHASHSEED=0\n",
    "\n",
    "commands =\n",
    "\tpython regression_model/train_pipeline.py\n",
    "\tpytest -s -v tests/\n",
    "\n",
    "\n",
    "[testenv:train]\n",
    "envdir \t = {toxworkdir}/test_package\n",
    "deps \t = {[testenv:test_package]deps}\n",
    "setenv \t = {[testenv:test_package]setenv}\n",
    "commands = \n",
    "\tpython regression_model/train_pipeline.py\n",
    "\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me break it down a bit for you. Every time you see something in square brackets like this, this is a different tox environment and an environment is something which is going to set up a virtual environment in your `.tox` hidden directory. We can run commands within a specific environment, and we can also inherit commands and dependencies from other environments. So this is a sort of foundational unit when we're working with tox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have the default `tox` environment and a default `testenv` environment.\n",
    "And what this means is that if we just run the `tox` command on its own, it's going to run all the commands in these different environments (`test_package`, `typechecks`, `stylechecks`, and `lint`). You will see that these names corresponds to environments defined further in the file. Continuing, we set `skipsdist=True` since we don't want to build the package when using tox. The `testenv` is almost like a base class, if you think of inheritance (using the `:` syntax). And so this `install_command` is going to be consistent whenever we inherit from this base environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `test_package` environment which inherits from `testenv`, we define `deps` and that tells `tox` that for this particular environment, we're going to need to install `requirements/test_requirements.txt` with flag `-r`. This also sets environmental variables `PYTHONPATH=.` for the root directory and `PYTHONHASHSEED=0` to disable setting hash seed to a random integer for test commands. Finally, the following two commands are run:\n",
    "\n",
    "```\n",
    "$ python regression_model/train_pipeline.py\n",
    "$ pytest -s -v tests\n",
    "```\n",
    "\n",
    "Here `-s` means to disable all capturing and `-v` to get verbose outputs. You can test this by running the following script in the terminal:\n",
    "\n",
    "```\n",
    "$ tox -e test_package\n",
    "test_package installed: appdirs==1.4.4,attrs==21.4.0,black==20.8b1,click==8.0.4,feature-engine==1.0.2,flake8==3.9.2,iniconfig==1.1.1,isort==5.8.0,joblib==1.0.1,mccabe==0.6.1,mypy==0.812,mypy-extensions==0.4.3,numpy==1.22.3,packaging==21.3,pandas==1.4.1,pathspec==0.9.0,patsy==0.5.2,pluggy==1.0.0,py==1.11.0,pycodestyle==2.7.0,pydantic==1.8.2,pyflakes==2.3.1,pyparsing==3.0.7,pytest==6.2.5,python-dateutil==2.8.2,pytz==2021.3,regex==2022.3.2,ruamel.yaml==0.16.12,ruamel.yaml.clib==0.2.6,scikit-learn==1.0.2,scipy==1.8.0,six==1.16.0,statsmodels==0.13.2,strictyaml==1.3.2,threadpoolctl==3.1.0,toml==0.10.2,typed-ast==1.4.3,typing_extensions==4.1.1\n",
    "test_package run-test-pre: PYTHONHASHSEED='0'\n",
    "test_package run-test: commands[0] | python regression_model/train_pipeline.py\n",
    "test_package run-test: commands[1] | pytest -s -v tests/\n",
    "============================= test session starts ==============================\n",
    "platform darwin -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /Users/particle1331/code/model-deployment/05-production/.tox/test_package/bin/python\n",
    "cachedir: .tox/test_package/.pytest_cache\n",
    "rootdir: /Users/particle1331/code/model-deployment/05-production, configfile: pyproject.toml\n",
    "collected 2 items\n",
    "\n",
    "tests/test_features.py::test_temporal_variable_transformer PASSED\n",
    "tests/test_prediction.py::test_make_prediction PASSED\n",
    "\n",
    "============================== 2 passed in 0.19s ===============================\n",
    "___________________________________ summary ____________________________________\n",
    "  test_package: commands succeeded\n",
    "  congratulations :)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have the `train` environment. Notice that we set `envdir={toxworkdir}/test_package` which tells tox to recreate the `test_package` environment in the hidden `.tox` directory. This is to save time as setting up a new virtual environment takes a while. Furthermore, setting `deps={[testenv:test_package]deps}` installs `test_requirements.txt` instead of `requirements.txt`. Again this saves time, though the train script should not require tooling libraries such as `mypy`, `isort`, etc. After setting up the environment, the training pipeline is triggered without running the tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ tox -e train\n",
    "train installed: appdirs==1.4.4,attrs==21.4.0,black==20.8b1,click==8.0.4,feature-engine==1.0.2,flake8==3.9.2,iniconfig==1.1.1,isort==5.8.0,joblib==1.0.1,mccabe==0.6.1,mypy==0.812,mypy-extensions==0.4.3,numpy==1.22.3,packaging==21.3,pandas==1.4.1,pathspec==0.9.0,patsy==0.5.2,pluggy==1.0.0,py==1.11.0,pycodestyle==2.7.0,pydantic==1.8.2,pyflakes==2.3.1,pyparsing==3.0.7,pytest==6.2.5,python-dateutil==2.8.2,pytz==2021.3,regex==2022.3.2,ruamel.yaml==0.16.12,ruamel.yaml.clib==0.2.6,scikit-learn==1.0.2,scipy==1.8.0,six==1.16.0,statsmodels==0.13.2,strictyaml==1.3.2,threadpoolctl==3.1.0,toml==0.10.2,typed-ast==1.4.3,typing_extensions==4.1.1\n",
    "train run-test-pre: PYTHONHASHSEED='0'\n",
    "train run-test: commands[0] | python regression_model/train_pipeline.py\n",
    "___________________________________ summary ____________________________________\n",
    "  train: commands succeeded\n",
    "  congratulations :)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the source, we also have tox commands to run our type checks, style checks, and linting. These are using bits of tooling that will be getting into later in the section. But it's just a really elegant way for us to make sure that our Python code adheres to things like PEP 8\n",
    "and has a consistent style that makes it really easy for other programmers to work with.\n",
    "These are defined following the same pattern as the `train` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to talk about config. You may have noticed that we have a `config.yml` file here inside the `regression_model` directory. A good rule of thumb is that you want to limit the amount of power that your config files have. If you write them in Python, it'll be tempting to add small bits of Python code and that can cause bugs. Moreover, config files in standard formats like YAML or JSON can also be edited by developers who don't know Python. For our purposes, we have taken all those global constants and hyperparameters, and put them in YAML format in the `config.yml` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`regression_model/config.yml`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/config.yml)\n",
    "```\n",
    "\n",
    "````{note} \n",
    "If you're not familiar with YAML syntax, we explain its most relevant features here. Key-value pairs corresponds to an assignment operation: `package_name: regression_model` will be loaded as `package_name = \"regression_model\"` in Python. Nested keys with indentation will be read as keys of a dictionary:\n",
    "\n",
    "```yaml\n",
    "variables_to_rename:\n",
    "  1stFlrSF: FirstFlrSF\n",
    "  2ndFlrSF: SecondFlrSF\n",
    "  3SsnPorch: ThreeSsnPortch\n",
    "```\n",
    "\n",
    "```python\n",
    "variables_to_rename = {'1stFlrSF': 'FirstFlrSF', '2ndFlrSF': 'SecondFlrSF', '3SsnPorch': 'ThreeSsnPortch'}\n",
    "```\n",
    "\n",
    "Finally, we have the indented hyphen syntax which is going to be a list.\n",
    "\n",
    "```yaml\n",
    "numericals_log_vars:\n",
    "  - LotFrontage\n",
    "  - FirstFlrSF\n",
    "  - GrLivArea\n",
    "```\n",
    "\n",
    "```python\n",
    "numericals_log_vars = ['LotFrontage', 'FirstFlrSF', 'GrLivArea']\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we head over to the `config` directory, we have our `core.py` file, there are a few things that are happening here. First, we are using `pathlib` to define the location of files and directories that we're interested in using. Here `regression_model.__file__` refers to the `__init__.py` file in `regression_model`, so that `PACKAGE_ROOT` refers to the path of the `regression_model` directory. We also define the paths of the config YAML file, the datasets, and trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`regression_model/config/core.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/config/core.py)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Project Directories\n",
    "PACKAGE_ROOT = Path(regression_model.__file__).resolve().parent\n",
    "ROOT = PACKAGE_ROOT.parent\n",
    "CONFIG_FILE_PATH = PACKAGE_ROOT / \"config.yml\"\n",
    "DATASET_DIR = PACKAGE_ROOT / \"datasets\"\n",
    "TRAINED_MODEL_DIR = PACKAGE_ROOT / \"trained_models\"\n",
    "\n",
    "\n",
    "class AppConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Application-level config.\n",
    "    \"\"\"\n",
    "\n",
    "    package_name: str\n",
    "    training_data_file: str\n",
    "    test_data_file: str\n",
    "    pipeline_save_file: str\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    All configuration relevant to model training and feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    target: str\n",
    "    variables_to_rename: Dict\n",
    "    features: List[str]\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "    alpha: float\n",
    "    categorical_vars_with_na_frequent: List[str]\n",
    "    categorical_vars_with_na_missing: List[str]\n",
    "    numerical_vars_with_na: List[str]\n",
    "    temporal_vars: List[str]\n",
    "    ref_var: str\n",
    "    numericals_log_vars: Sequence[str]\n",
    "    binarize_vars: Sequence[str]\n",
    "    qual_vars: List[str]\n",
    "    exposure_vars: List[str]\n",
    "    finish_vars: List[str]\n",
    "    garage_vars: List[str]\n",
    "    categorical_vars: Sequence[str]\n",
    "    qual_mappings: Dict[str, int]\n",
    "    exposure_mappings: Dict[str, int]\n",
    "    garage_mappings: Dict[str, int]\n",
    "    finish_mappings: Dict[str, int]\n",
    "\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use `BaseModel` from `pydantic` to define our config classes. \n",
    "Pydantic is an excellent library for data validation and settings management using Python type annotations. This is really powerful because it means we don't have to learn a new sort of micro language for data parsing and schema validation.\n",
    "We can just use Pydantic and our existing knowledge of Python type hints.\n",
    "And so, this gives us a really clear and powerful way to understand and \n",
    "potentially test our config, and to prevent introducing bugs into our model.\n",
    "\n",
    "Here we have two differents sort of subconfigs. We have everything to do with our \n",
    "model, and then everything to do with our package. Things like the package name and \n",
    "the location of the pipeline say file, these are not really the\n",
    "sort of data science concerns &mdash; these are more just the development \n",
    "concerns. These configs go into the `AppConfig` data model. The data science configs\n",
    "go into `ModelConfig`. Then, we wrap it in an overall config like so:\n",
    "\n",
    "```{margin}\n",
    "[`regression_model/config/core.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/config/core.py)\n",
    "```\n",
    "\n",
    "```python\n",
    "class Config(BaseModel):\n",
    "    \"\"\"Master config object.\"\"\"\n",
    "\n",
    "    app_config: AppConfig\n",
    "    model_config: ModelConfig\n",
    "```\n",
    "\n",
    "\n",
    "And then at the bottom, we have three helper functions, so our `config` object, \n",
    "which is what we're going to be importing in other modules, is defined through this \n",
    "`create_and_validate_config` function.\n",
    "This uses our `parse_config_from_yaml` function, which using `CONFIG_FILE_PATH` specified above\n",
    "will check that the file exists, and then attempt to load it using the `strictyaml` load function\n",
    "if it does.\n",
    "Strict YAML is a type-safe YAML parser that passes and validates a restricted \n",
    "subset of the YAML specification.\n",
    "It's a great library for working with Yamal that can help you avoid some of \n",
    "the hidden gotchas of YAML.\n",
    "\n",
    "```{margin}\n",
    "[`regression_model/config/core.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/config/core.py)\n",
    "```\n",
    "\n",
    "```python\n",
    "def validate_config_file_path(cfg_path: Path) -> Path:\n",
    "    \"\"\"Locate the configuration file.\"\"\"\n",
    "\n",
    "    if not cfg_path.is_file():\n",
    "        raise OSError(f\"Config not found at {cfg_path!r}\")\n",
    "\n",
    "    return cfg_path\n",
    "\n",
    "\n",
    "def parse_config_from_yaml(cfg_path: Path) -> YAML:\n",
    "    \"\"\"Parse YAML containing the package configuration.\"\"\"\n",
    "\n",
    "    cfg_path = validate_config_file_path(cfg_path)\n",
    "    with open(cfg_path, \"r\") as conf_file:\n",
    "        parsed_config = load(conf_file.read())\n",
    "\n",
    "    return parsed_config\n",
    "\n",
    "\n",
    "def create_and_validate_config(parsed_config: YAML) -> Config:\n",
    "    \"\"\"Run validation on config values.\"\"\"\n",
    "\n",
    "    return Config(\n",
    "        app_config=AppConfig(**parsed_config.data),\n",
    "        model_config=ModelConfig(**parsed_config.data),\n",
    "    )\n",
    "\n",
    "\n",
    "_parsed_config = parse_config_from_yaml(CONFIG_FILE_PATH)\n",
    "config = create_and_validate_config(_parsed_config)\n",
    "```\n",
    "\n",
    "And once we load it in our YAML file, we then unpack the key value\n",
    "pairs here and pass them to `AppConfig` and `ModelConfig` as keyword arguments \n",
    "to instantiate these classes.\n",
    "And that results in us having this `config` object, which we're going to be \n",
    "seeing all over our package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at our config, let's dig into the main `regression/train_pipeline.py` scripts, so this is what\n",
    "we've been running in our tox commands.\n",
    "If we open up this file, you can see we have one function, which is `run_training`.\n",
    "And if we step through what's happening here, we are loading in the training data and we've created\n",
    "some utility functions like this `load_dataset` function, which comes from our `data_manager` module that\n",
    "will be looking at soon.\n",
    "\n",
    "```{margin}\n",
    "[`regression_model/train_pipeline.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/train_pipeline.py)\n",
    "```\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from config.core import config\n",
    "from pipeline import price_pipe\n",
    "from processing.data_manager import load_dataset, save_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def run_training() -> None:\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "\n",
    "    # Read training data\n",
    "    data = load_dataset(file_name=config.app_config.training_data_file)\n",
    "\n",
    "    # Divide train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[config.model_config.features],\n",
    "        data[config.model_config.target],\n",
    "        test_size=config.model_config.test_size,\n",
    "        random_state=config.model_config.random_state,\n",
    "    )\n",
    "    y_train = np.log(y_train)\n",
    "\n",
    "    # Fit model\n",
    "    price_pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Persist trained model\n",
    "    save_pipeline(pipeline_to_persist=price_pipe)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()\n",
    "```\n",
    "\n",
    "\n",
    "We then use the standard scikit `train_test_split`.\n",
    "This should be familiar from what you usually do in Jupyter notebooks.\n",
    "The difference is we're using `config` here to define things like our features and the target\n",
    "variable, as well as the test size and the random states.\n",
    "So whereas previously these were constants.\n",
    "Now we're making use of our config and the actual values can be read from the `config.yml` file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`regression_model/processing/data_manager.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/processing/data_manager.py)\n",
    "```\n",
    "\n",
    "```python\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    dataframe = pd.read_csv(DATASET_DIR / file_name)\n",
    "    dataframe[\"MSSubClass\"] = dataframe[\"MSSubClass\"].astype(\"O\")\n",
    "\n",
    "    # Rename variables beginning with numbers to avoid syntax errors\n",
    "    transformed = dataframe.rename(columns=config.model_config.variables_to_rename)\n",
    "    return transformed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we perform some basic preprocessing such as converting `MSSubClass` to type `object` since its a categorical variable (see `config.yml`), although the values are numeric. Note that Pandas automatically infer types so we just add to this functionality a little in this function. We also rename variables beginning with numbers to avoid syntax errors. The `*` syntax forces zero positional arguments so that all arguments are named when passed. Next, we have our `price_pipe` which is a  `scikit-learn` pipeline object and we'll look at the `pipeline` module in a moment, in the next section.\n",
    "But you can see here how we use it to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the train script is the `save_pipeline` function which we use to persist the trained pipeline in its first argument. This function is also in our data manager module. If we look at the file name of the saved pipeline, it's going to be called `\"{config.app_config.pipeline_save_file}\" + f\"{__version__}.pkl\"` where `__version__` is the content of the `VERSION` file (`0.0.1`) and `pipeline_save_file` is substituted with `\"regression_model_output_v\"` as specified in `config.yml`. \n",
    "\n",
    "The other nontrivial part of the save function is the `remove_old_pipelines` which deletes all files inside `trained_models/` so long as the file is not the init file. And why are we doing this? By ensuring that at any one time we only have one model output file in our package later on when we\n",
    "are installing the packages, and its dependencies, we can be certain about what the version is going to be.\n",
    "It basically simplifies the package where we don't have to deal with multiple versions of a persistent model and that reduces the chances of us\n",
    "making a mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{margin}\n",
    "[`regression_model/processing/data_manager.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/processing/data_manager.py)\n",
    "```\n",
    "\n",
    "```python\n",
    "def save_pipeline(*, pipeline_to_persist: Pipeline) -> None:\n",
    "    \"\"\"Persist the pipeline.\n",
    "    Saves the versioned model, and overwrites any previous\n",
    "    saved models. This ensures that when the package is\n",
    "    published, there is only one trained model that can be\n",
    "    called, and we know exactly how it was built.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare versioned save file name\n",
    "    save_file_name = f\"{config.app_config.pipeline_save_file}{_version}.pkl\"\n",
    "    save_path = TRAINED_MODEL_DIR / save_file_name\n",
    "\n",
    "    remove_old_pipelines(files_to_keep=[save_file_name])\n",
    "    joblib.dump(pipeline_to_persist, save_path)\n",
    "\n",
    "\n",
    "def remove_old_pipelines(*, files_to_keep: t.List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Remove old model pipelines.\n",
    "    This is to ensure there is a simple one-to-one\n",
    "    mapping between the package version and the model\n",
    "    version to be imported and used by other applications.\n",
    "    \"\"\"\n",
    "    do_not_delete = files_to_keep + [\"__init__.py\"]\n",
    "    for model_file in TRAINED_MODEL_DIR.iterdir():\n",
    "        if model_file.name not in do_not_delete:\n",
    "            model_file.unlink() # delete\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the last step in our save pipeline function is to use the job serialisation library to persist\n",
    "the pipeline to the save path that we've defined. And that's how our `regression_model_output_version_v0.0.1.pkl` ends up here in `trained_models/`. Finally, we define a `load_pipeline` function which will be used to load the trained pipeline during inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`regression_model/processing/data_manager.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/processing/data_manager.py)\n",
    "```\n",
    "\n",
    "```python\n",
    "def load_pipeline(*, file_name: str) -> Pipeline:\n",
    "    \"\"\"Load a persisted pipeline.\"\"\"\n",
    "\n",
    "    file_path = TRAINED_MODEL_DIR / file_name\n",
    "    trained_model = joblib.load(filename=file_path)\n",
    "    return trained_model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this section we will look at our feature engineering pipeline defined as `price_pipe` in the `pipeline` module. Looking at the code, we're applying transformations sequentially to preprocess and feature engineer our data. Thanks to the `feature_engine` API, each step is almost human readable, we only have to set the variables where the transformations are applied to. Again, these can be found in `config.yml`.\n",
    "\n",
    "```{margin}\n",
    "[`regression_model/pipeline.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/pipeline.py)\n",
    "```\n",
    "\n",
    "```python\n",
    "from feature_engine.encoding import OrdinalEncoder, RareLabelEncoder\n",
    "from feature_engine.imputation import (\n",
    "    AddMissingIndicator,\n",
    "    CategoricalImputer,\n",
    "    MeanMedianImputer,\n",
    ")\n",
    "from feature_engine.selection import DropFeatures\n",
    "from feature_engine.transformation import LogTransformer\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Binarizer, MinMaxScaler\n",
    "\n",
    "from regression_model.config.core import config\n",
    "from regression_model.processing import features as pp\n",
    "\n",
    "price_pipe = Pipeline(\n",
    "    [\n",
    "        # ===== IMPUTATION =====\n",
    "        # Impute categorical variables with string missing\n",
    "        (\n",
    "            \"missing_imputation\",\n",
    "            CategoricalImputer(\n",
    "                imputation_method=\"missing\",\n",
    "                variables=config.model_config.categorical_vars_with_na_missing,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"frequent_imputation\",\n",
    "            CategoricalImputer(\n",
    "                imputation_method=\"frequent\",\n",
    "                variables=config.model_config.categorical_vars_with_na_frequent,\n",
    "            ),\n",
    "        ),\n",
    "        # Add missing indicator\n",
    "        (\n",
    "            \"missing_indicator\",\n",
    "            AddMissingIndicator(variables=config.model_config.numerical_vars_with_na),\n",
    "        ),\n",
    "        # Impute numerical variables with the mean\n",
    "        (\n",
    "            \"mean_imputation\",\n",
    "            MeanMedianImputer(\n",
    "                imputation_method=\"mean\",\n",
    "                variables=config.model_config.numerical_vars_with_na,\n",
    "            ),\n",
    "        ),\n",
    "        \n",
    "        # == TEMPORAL VARIABLES ====\n",
    "        (\n",
    "            \"elapsed_time\",\n",
    "            pp.TemporalVariableTransformer(\n",
    "                variables=config.model_config.temporal_vars,\n",
    "                reference_variable=config.model_config.ref_var,\n",
    "            ),\n",
    "        ),\n",
    "        (\"drop_features\", DropFeatures(features_to_drop=[config.model_config.ref_var])),\n",
    "        \n",
    "        # ==== VARIABLE TRANSFORMATION =====\n",
    "        (\"log\", LogTransformer(variables=config.model_config.numericals_log_vars)),\n",
    "        (\n",
    "            \"binarizer\",\n",
    "            SklearnTransformerWrapper(\n",
    "                transformer=Binarizer(threshold=0),\n",
    "                variables=config.model_config.binarize_vars,\n",
    "            ),\n",
    "        ),\n",
    "        \n",
    "        # === MAPPERS ===\n",
    "        (\n",
    "            \"mapper_qual\",\n",
    "            pp.Mapper(\n",
    "                variables=config.model_config.qual_vars,\n",
    "                mappings=config.model_config.qual_mappings,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"mapper_exposure\",\n",
    "            pp.Mapper(\n",
    "                variables=config.model_config.exposure_vars,\n",
    "                mappings=config.model_config.exposure_mappings,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"mapper_finish\",\n",
    "            pp.Mapper(\n",
    "                variables=config.model_config.finish_vars,\n",
    "                mappings=config.model_config.finish_mappings,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"mapper_garage\",\n",
    "            pp.Mapper(\n",
    "                variables=config.model_config.garage_vars,\n",
    "                mappings=config.model_config.garage_mappings,\n",
    "            ),\n",
    "        ),\n",
    "        \n",
    "        # == CATEGORICAL ENCODING\n",
    "        (\n",
    "            \"rare_label_encoder\",\n",
    "            RareLabelEncoder(\n",
    "                tol=0.01, n_categories=1, variables=config.model_config.categorical_vars\n",
    "            ),\n",
    "        ),\n",
    "        # Encode categorical variables using the target mean\n",
    "        (\n",
    "            \"categorical_encoder\",\n",
    "            OrdinalEncoder(\n",
    "                encoding_method=\"ordered\",\n",
    "                variables=config.model_config.categorical_vars,\n",
    "            ),\n",
    "        ),\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\n",
    "            \"Lasso\",\n",
    "            Lasso(\n",
    "                alpha=config.model_config.alpha,\n",
    "                random_state=config.model_config.random_state,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although we're using a lot of transformers from the `feature_engine` library, we also have some custom ones that we've created\n",
    "in the `processing.features` module of our package. Let's have a look at this module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`regression_model/processing/features.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/processing/features.py)\n",
    "```\n",
    "```python\n",
    "class TemporalVariableTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Temporal elapsed time transformer.\"\"\"\n",
    "\n",
    "    def __init__(self, variables: List[str], reference_variable: str):\n",
    "\n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError(\"variables should be a list\")\n",
    "\n",
    "        self.variables = variables\n",
    "        self.reference_variable = reference_variable\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
    "        # we need this step to fit the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        # so that we do not over-write the original dataframe\n",
    "        X = X.copy()\n",
    "\n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[self.reference_variable] - X[feature]\n",
    "\n",
    "        return X\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first is the `TemporalVariableTransformer` and the second is the `Mapper`. These inherit from `BaseEstimator` and `TransformerMixin` in `sklearn.base`. \n",
    "And by doing that, and also ensuring that we specify a `fit` and a `transform` method, we're able to use\n",
    "this to transform variables and it's compatible with our `scikit-learn` pipeline. \n",
    "\n",
    "So, `temporal_vars` in the pipeline is `YearRemodAdd`, and then `ref_var`, again here in `config.yml`, we can see as `YrSold`. And from the code, we can see that when we apply the transform method, we're going to do `YrSold` minus  `YearRemodAdd`, the year when the remod was added. This replaces the `YrSold` feature by some integer which is the result of making that subtraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`regression_model/processing/features.py`](https://github.com/particle1331/model-deployment/blob/main/05-production/regression_model/processing/features.py)\n",
    "```\n",
    "```python\n",
    "class Mapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Categorical variable mapper.\"\"\"\n",
    "\n",
    "    def __init__(self, variables: List[str], mappings: dict):\n",
    "\n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError(\"variables should be a list\")\n",
    "\n",
    "        self.variables = variables\n",
    "        self.mappings = mappings\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
    "        # we need the fit statement to accomodate the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[feature].map(self.mappings)\n",
    "\n",
    "        return X\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in our `Mapper` class, we also apply a transformation where we take these features which we map to other values using a dictionary to specify the mapping. For example, we have `qual_vars` in our config, which are feature columns describing the quality of different parts of the house, and `qual_mappings` which map values each of these features to integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could easily create additional features here by defining\n",
    "another class which adhere to this structure (defining custom `sklearn` transformers), then adding\n",
    "it to our pipeline at whatever point in the pipeline it made sense, depending on the transform that we're\n",
    "looking to apply, and specifying which variables the transformers apply to by implementing a `variables` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a55a0d1272a360f93e747858d443ec26da69f69eac36db3e567a961ca624a861"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
