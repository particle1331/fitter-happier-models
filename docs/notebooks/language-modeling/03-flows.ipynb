{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations & Gradients, BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you'd want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video. \n",
    "\n",
    "\n",
    "Intuitive understanding of activation during training, and esp. grads flowing backwards and how they behave and what they look like. RNNs very expressive / universal approximators, we will see that they are not easy to optimize with first order methods. \n",
    "\n",
    "key to understanding: activation, gradients, and how they behave during training. Variants since RNNs (2011) have tried to improve that situation. That will be our path in this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "DATASET_DIR = Path(\"./data\").absolute()\n",
    "RANDOM_SEED = 42\n",
    "TRAIN_RATIO = 1.0\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = open(DATASET_DIR / 'names.txt', 'r').read().splitlines()\n",
    "names = names[:int(TRAIN_RATIO * len(names))]\n",
    "\n",
    "print(len(names))\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset:\n",
    "    def __init__(self, names, block_size):\n",
    "        self.names = names\n",
    "        self.chars = ['.'] + sorted(list(set(''.join(names))))\n",
    "        self.block_size = block_size\n",
    "        self.itos = dict(enumerate(self.chars))\n",
    "        self.stoi = {c: i for i, c in self.itos.items()}\n",
    "\n",
    "    def decode(self, x):\n",
    "        return ''.join(self.itos[i] for i in x)\n",
    "\n",
    "    def encode(self, word):\n",
    "        return [self.stoi[c] for c in word]\n",
    "\n",
    "    def build_dataset(self):\n",
    "        \"\"\"Creating subsequences -> next character target.\"\"\"\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for name in self.names:\n",
    "            context = ['.'] * self.block_size\n",
    "            for c in name + '.':\n",
    "                xs.append(self.encode(''.join(context)))\n",
    "                ys.append(self.stoi[c])\n",
    "                context = context[1:] + [c]\n",
    "        \n",
    "        return torch.tensor(xs), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xs</th>\n",
       "      <th>ys</th>\n",
       "      <th>seq</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 5]</td>\n",
       "      <td>13</td>\n",
       "      <td>..e</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 5, 13]</td>\n",
       "      <td>13</td>\n",
       "      <td>.em</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5, 13, 13]</td>\n",
       "      <td>1</td>\n",
       "      <td>emm</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[13, 13, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>mma</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0, 0, 15]</td>\n",
       "      <td>12</td>\n",
       "      <td>..o</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0, 15, 12]</td>\n",
       "      <td>9</td>\n",
       "      <td>.ol</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[15, 12, 9]</td>\n",
       "      <td>22</td>\n",
       "      <td>oli</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[12, 9, 22]</td>\n",
       "      <td>9</td>\n",
       "      <td>liv</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[9, 22, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>ivi</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[22, 9, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>via</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             xs  ys  seq target\n",
       "0     [0, 0, 0]   5  ...      e\n",
       "1     [0, 0, 5]  13  ..e      m\n",
       "2    [0, 5, 13]  13  .em      m\n",
       "3   [5, 13, 13]   1  emm      a\n",
       "4   [13, 13, 1]   0  mma      .\n",
       "5     [0, 0, 0]  15  ...      o\n",
       "6    [0, 0, 15]  12  ..o      l\n",
       "7   [0, 15, 12]   9  .ol      i\n",
       "8   [15, 12, 9]  22  oli      v\n",
       "9   [12, 9, 22]   9  liv      i\n",
       "10   [9, 22, 9]   1  ivi      a\n",
       "11   [22, 9, 1]   0  via      ."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = CharDataset(names, block_size=3)\n",
    "itos = dataset.itos\n",
    "stoi = dataset.stoi\n",
    "\n",
    "xs, ys = dataset.build_dataset()\n",
    "\n",
    "df = pd.DataFrame({'xs': list(xs.numpy()), 'ys': list(ys.numpy())})\n",
    "df['seq'] = df['xs'].apply(lambda x: ''.join(itos[c] for c in x))\n",
    "df['target'] = df['ys'].map(itos)\n",
    "df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def create_folds(xs, ys, seed=42):\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    folds = torch.zeros(xs.shape[0])\n",
    "    for fold, (trn_, val_) in enumerate(kf.split(xs, ys)):\n",
    "        folds[val_] = fold\n",
    "\n",
    "    xs_trn, ys_trn = xs[folds >  1], ys[folds >  1]\n",
    "    xs_dev, ys_dev = xs[folds == 1], ys[folds == 1]\n",
    "    xs_tst, ys_tst = xs[folds == 0], ys[folds == 0]\n",
    "\n",
    "    return xs_trn, ys_trn, xs_dev, ys_dev, xs_tst, ys_tst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, embedding_size, width, block_size, vocab_size=27, seed=2147483647):\n",
    "        self.emb_size = embedding_size\n",
    "        self.width = width\n",
    "        self.blk_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self._init_weights(seed)\n",
    "        \n",
    "    def _init_weights(self, seed):\n",
    "        self.g = torch.Generator().manual_seed(seed)\n",
    "        self.C  = torch.randn(self.vocab_size, self.emb_size,               generator=self.g)\n",
    "        self.W1 = torch.randn((self.blk_size * self.emb_size, self.width),  generator=self.g)\n",
    "        self.b1 = torch.randn(self.width,                                   generator=self.g)\n",
    "        self.W2 = torch.randn((self.width, self.vocab_size),                generator=self.g)\n",
    "        self.b2 = torch.randn(self.vocab_size,                              generator=self.g)\n",
    "        self.parameters = [self.C, self.W1, self.b1, self.W2, self.b2]\n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def __call__(self, xs: list[int]):\n",
    "        x = self.C[xs].view(-1, self.blk_size * self.emb_size)\n",
    "        h = torch.tanh(x @ self.W1 + self.b1)\n",
    "        logits = h @ self.W2 + self.b2\n",
    "        return logits\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "           p.grad = None\n",
    "\n",
    "    def optim_step(self, lr):\n",
    "        for p in self.parameters:\n",
    "           p.data -= lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, xs_trn, ys_trn, generator, batch_size=32, lr=0.1):\n",
    "    \"\"\"One step of backprop and weight update.\"\"\"\n",
    "    \n",
    "    B = torch.randint(\n",
    "        0, xs_trn.shape[0], \n",
    "        (batch_size,), \n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    logits = model(xs_trn[B])\n",
    "    loss = F.cross_entropy(logits, target=ys_trn[B])\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    model.optim_step(lr=lr)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        xs_trn, ys_trn,\n",
    "        xs_dev, ys_dev,\n",
    "        batch_size,\n",
    "        num_steps,\n",
    "        lr=0.1,\n",
    "        verbose=True, seed=42\n",
    "    ):\n",
    "\n",
    "    losses_trn = {}\n",
    "    losses_dev = {}\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    lr_counter = 0\n",
    "    best_loss_dev = float(\"inf\")\n",
    "    print(f\"Setting learning rate to {lr:.2e}\")\n",
    "\n",
    "    for k in range(num_steps):\n",
    "        loss = train_step(model, xs_trn, ys_trn, g, batch_size, lr)\n",
    "        losses_trn[k] = loss\n",
    "        \n",
    "        if k > 0 and ((k % (num_steps // batch_size) == 0) or (k == num_steps - 1)):\n",
    "            with torch.no_grad():\n",
    "                loss_dev = F.cross_entropy(model(xs_dev), ys_dev)\n",
    "                losses_dev[k] = loss_dev.item()\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"[{k+1:>0{int(len(str(num_steps)))}d}/{num_steps}]    loss={loss:.4f}    dev_loss={loss_dev:.4f}\")\n",
    "    \n",
    "            if loss_dev.item() < best_loss_dev:\n",
    "                best_loss_dev = loss_dev.item()\n",
    "                lr_counter = 0\n",
    "            else:\n",
    "                lr_counter += 1\n",
    "                if lr_counter == 3:\n",
    "                    lr *= 0.1\n",
    "                    lr_counter = 0\n",
    "                    if verbose:\n",
    "                        print(f\"Decreasing learning rate to {lr:.2e}\")\n",
    "                    \n",
    "    return {\"loss_trn\": losses_trn, \"loss_dev\": losses_dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting learning rate to 2.01e-01\n",
      "[009376/300000]    loss=2.9054    dev_loss=2.8677\n",
      "[018751/300000]    loss=3.0840    dev_loss=2.5692\n",
      "[028126/300000]    loss=2.4552    dev_loss=2.5369\n",
      "[037501/300000]    loss=2.5357    dev_loss=2.6065\n",
      "[046876/300000]    loss=2.6222    dev_loss=2.4017\n",
      "[056251/300000]    loss=2.5329    dev_loss=2.4637\n",
      "[065626/300000]    loss=2.4862    dev_loss=2.4841\n",
      "[075001/300000]    loss=3.2175    dev_loss=2.5284\n",
      "Decreasing learning rate to 2.01e-02\n",
      "[084376/300000]    loss=2.1287    dev_loss=2.1713\n",
      "[093751/300000]    loss=2.2897    dev_loss=2.1729\n",
      "[103126/300000]    loss=1.9273    dev_loss=2.1669\n",
      "[112501/300000]    loss=1.9314    dev_loss=2.1692\n",
      "[121876/300000]    loss=2.2929    dev_loss=2.1664\n",
      "[131251/300000]    loss=1.6294    dev_loss=2.1642\n",
      "[140626/300000]    loss=1.7725    dev_loss=2.1652\n",
      "[150001/300000]    loss=2.0152    dev_loss=2.1678\n",
      "[159376/300000]    loss=1.6645    dev_loss=2.1698\n",
      "Decreasing learning rate to 2.01e-03\n",
      "[168751/300000]    loss=1.9124    dev_loss=2.1472\n",
      "[178126/300000]    loss=2.4708    dev_loss=2.1468\n",
      "[187501/300000]    loss=2.0519    dev_loss=2.1462\n",
      "[196876/300000]    loss=1.7858    dev_loss=2.1462\n",
      "[206251/300000]    loss=2.3263    dev_loss=2.1458\n",
      "[215626/300000]    loss=1.7310    dev_loss=2.1463\n",
      "[225001/300000]    loss=1.8441    dev_loss=2.1466\n",
      "[234376/300000]    loss=2.3243    dev_loss=2.1452\n",
      "[243751/300000]    loss=2.1917    dev_loss=2.1458\n",
      "[253126/300000]    loss=1.7910    dev_loss=2.1463\n",
      "[262501/300000]    loss=1.8563    dev_loss=2.1458\n",
      "Decreasing learning rate to 2.01e-04\n",
      "[271876/300000]    loss=2.1069    dev_loss=2.1440\n",
      "[281251/300000]    loss=2.0027    dev_loss=2.1437\n",
      "[290626/300000]    loss=2.2936    dev_loss=2.1438\n",
      "[300000/300000]    loss=2.1017    dev_loss=2.1438\n"
     ]
    }
   ],
   "source": [
    "xs_trn, ys_trn, \\\n",
    "xs_dev, ys_dev, \\\n",
    "xs_tst, ys_tst = create_folds(xs, ys)\n",
    "\n",
    "lr0 = 0.2014206395146228\n",
    "model = MLP(embedding_size=8, width=180, block_size=3)\n",
    "hist = train_model(model, xs_trn, ys_trn, xs_dev, ys_dev, batch_size=32, num_steps=300000, lr=lr0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.1047465801239014\n",
      "valid loss: 2.139568328857422\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(model, split: str):\n",
    "    x, y = {\n",
    "        'train': (xs_trn, ys_trn),\n",
    "        'valid': (xs_dev, ys_dev),\n",
    "        'test':  (xs_tst, ys_tst)\n",
    "    }[split]\n",
    "    loss = F.cross_entropy(model(x), y)\n",
    "    return loss.item()\n",
    "\n",
    "print(\"train loss:\", split_loss(model, 'train'))\n",
    "print(\"valid loss:\", split_loss(model, 'valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chris                  1.399\n",
      "kaniel                 1.641\n",
      "zilynn                 1.731\n",
      "daila                  1.745\n",
      "jahani                 1.876\n",
      "hene                   1.950\n",
      "kuneth                 2.158\n",
      "ariehlaysmaizleigan    2.310\n",
      "adzia                  2.380\n",
      "gole                   2.515\n",
      "sassidl                2.531\n",
      "rla                    2.766\n"
     ]
    }
   ],
   "source": [
    "def generate_names(model, block_size, sample_size, seed=2147483647):\n",
    "    \"\"\"Generate names from a Markov process with cond probability table P.\"\"\"\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    names = []\n",
    "    for _ in range(sample_size):\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            p = model(torch.tensor(context)).exp()\n",
    "            j = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            if j == 0:\n",
    "                break\n",
    "            context = context[1:] + [j]\n",
    "            out.append(itos[j])\n",
    "        names.append(''.join(out))\n",
    "    return names\n",
    "\n",
    "\n",
    "def name_loss(name, model, block_size):\n",
    "    nll = 0.0\n",
    "    context = [0] * block_size\n",
    "    for c in name + '.':\n",
    "        p = F.softmax(model(torch.tensor(context))).reshape(-1)[stoi[c]]\n",
    "        nll += -math.log(p)\n",
    "        context = context[1:] + [stoi[c]]\n",
    "    return nll / (len(name) + 1)\n",
    "\n",
    "\n",
    "sample = generate_names(model, block_size=3, sample_size=12, seed=0)\n",
    "name_losses = {n: name_loss(n, model, block_size=3) for n in sample}\n",
    "for n in sorted(sample, key=lambda n: name_losses[n]):\n",
    "    print(f\"{n:<22} {name_losses[n]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bb4561ca8d8b7b3a7bc7514080b6e7dab3824c9a0b3ef748f0e5ff42277ee64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
