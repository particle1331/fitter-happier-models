{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=brightgreen)\n",
    "[![Source](https://img.shields.io/static/v1.svg?label=GitHub&message=Source&color=181717&logo=GitHub)](https://github.com/particle1331/inefficient-networks/blob/master/docs/notebooks/tensorflow/05-tensorflow-cnn.ipynb)\n",
    "[![Stars](https://img.shields.io/github/stars/particle1331/inefficient-networks?style=social)](https://github.com/particle1331/inefficient-networks)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on [this tutorial](https://www.youtube.com/watch?v=TCH_1BHY58I) by [Andrej Karpathy](https://karpathy.ai/) on language modeling. In this tutorial, we implement a multilayer perceptron (MLP) character-level language model. This also introduces many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).\n",
    "\n",
    "\n",
    "**Readings**\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "- E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n",
    "- E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n",
    "- E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since context is length 1, the generated names are pretty bad. We saw that using  If we use `k` characters as context, $27^k$ rows. Too few data to train this. \n",
    "\n",
    "We move to a new model MLP to predict next character. Bengio 2013 paper. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "DATASET_DIR = Path(\"./data\").absolute()\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = open(DATASET_DIR / 'names.txt', 'r').read().splitlines()\n",
    "print(len(names))\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import functools\n",
    "\n",
    "chars = ['.'] + sorted(list(set(''.join(names))))\n",
    "itos = dict(enumerate(chars))\n",
    "stoi = {c: i for i, c in itos.items()}\n",
    "\n",
    "# @functools.lru_cache()\n",
    "# def itox(context_size):\n",
    "#     return dict(enumerate(map(lambda z: ''.join(z), product(stoi.keys(), repeat=context_size))))\n",
    "\n",
    "# @functools.lru_cache()\n",
    "# def xtoi(context_size):\n",
    "#     return {x: i for i, x in itox(context_size).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(context_size=1):\n",
    "    \"\"\"Creating subsequences -> next character target.\"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i, name in enumerate(names):\n",
    "        context = ['.'] * context_size\n",
    "        for c in name + '.':\n",
    "            xs.append([stoi[c] for c in context])\n",
    "            ys.append(stoi[c])\n",
    "            context = context[1:] + [c]\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys =  build_dataset(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xs</th>\n",
       "      <th>ys</th>\n",
       "      <th>seq</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 5]</td>\n",
       "      <td>13</td>\n",
       "      <td>..e</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 5, 13]</td>\n",
       "      <td>13</td>\n",
       "      <td>.em</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5, 13, 13]</td>\n",
       "      <td>1</td>\n",
       "      <td>emm</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[13, 13, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>mma</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0, 0, 15]</td>\n",
       "      <td>12</td>\n",
       "      <td>..o</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0, 15, 12]</td>\n",
       "      <td>9</td>\n",
       "      <td>.ol</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[15, 12, 9]</td>\n",
       "      <td>22</td>\n",
       "      <td>oli</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[12, 9, 22]</td>\n",
       "      <td>9</td>\n",
       "      <td>liv</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[9, 22, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>ivi</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[22, 9, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>via</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             xs  ys  seq target\n",
       "0     [0, 0, 0]   5  ...      e\n",
       "1     [0, 0, 5]  13  ..e      m\n",
       "2    [0, 5, 13]  13  .em      m\n",
       "3   [5, 13, 13]   1  emm      a\n",
       "4   [13, 13, 1]   0  mma      .\n",
       "5     [0, 0, 0]  15  ...      o\n",
       "6    [0, 0, 15]  12  ..o      l\n",
       "7   [0, 15, 12]   9  .ol      i\n",
       "8   [15, 12, 9]  22  oli      v\n",
       "9   [12, 9, 22]   9  liv      i\n",
       "10   [9, 22, 9]   1  ivi      a\n",
       "11   [22, 9, 1]   0  via      ."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "xs, ys = build_dataset(context_size=3)\n",
    "\n",
    "df = pd.DataFrame({'xs': xs, 'ys': ys})\n",
    "df['seq'] = df['xs'].apply(lambda x: ''.join(itos[c] for c in x))\n",
    "df['target'] = df['ys'].map(itos)\n",
    "df.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed 27 letters to 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "C = torch.randn(27, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bb4561ca8d8b7b3a7bc7514080b6e7dab3824c9a0b3ef748f0e5ff42277ee64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
