{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an introduction to neural network learning in terms of backpropagation in **directed acyclic computational graphs** (DAGs). Our main result is that a training step &mdash; consisting of both a forward and a backward pass into the network &mdash; takes $\\mathcal O(M)$ steps where $M$ is the network size. In the last section of the notebook, we take a closer look at the implementation of `.backward` in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent on the loss surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Loss surfaces from samples**\n",
    "```\n",
    "\n",
    "Recall that the network measures its error rate using a **loss function** $\\mathcal L.$ In theory, we assume the existence of an underlying distribution for the input and output data, hence an expected value for the loss. In practice, we approximate the loss using the performance of the network on a dataset $\\mathcal X$ where each of the points $\\mathbf x_i$ is sampled independently using\n",
    "  \n",
    "$$\\mathcal L(\\mathcal X, \\mathbf w) = \\frac{1}{N}\\sum_{i=1}^N \\mathcal L(\\mathbf x_i, \\mathbf w).$$ \n",
    "\n",
    "We can imagine this as forming a surface in $\\mathbb R^d \\times \\mathbb R$ where $d$ is the number of parameters of the network, with the current parameter setting being a point $(\\mathbf w, \\mathcal L(\\mathcal X, \\mathbf w))$ on this surface. Note that the empirical loss surface will generally vary with $\\mathcal X$, but we expect these surfaces to be similar for large $N = |\\mathcal X|$ assuming the points sampled from the same distribution. The networks learns by finding the parameters that minimizes the loss, typically through an iterative process. In practice, to update the weights, we use variants of **gradient descent** characterized by the update rule\n",
    "\n",
    "$$\\mathbf w \\leftarrow \\mathbf w - \\epsilon \\nabla_{\\mathbf w} \\mathcal L$$ \n",
    "\n",
    "where $-\\nabla_{\\mathbf w} \\mathcal L$ is the direction of steepest descent, and $\\epsilon > 0$ is some positive number called the **learning rate**. Note that we can compute the gradient as the average of gradients $\\nabla_\\mathbf w \\mathcal L (\\mathbf x_i, \\mathbf w)$ at the point $\\mathbf w$ on the loss surface generated by the data point $\\mathbf x_i.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**The need for efficient backprop.**\n",
    "```\n",
    "\n",
    "Since $\\nabla_\\mathbf w \\mathcal L$ consists of partial derivatives for each weight in the network, this can easily number in millions (or even billions for SoTA models). How do we compute these derivatives efficiently? As discussed above, we have to compute the gradient at the current state of the network, so we would have to perform a forward pass to compute all parameter values given $\\mathbf w$ up to the final node. This is followed by a backward pass where we compute compute every partial derivative by a clever use of the chain rule, recursively backward for each layer of the network. Both forward and backward passes will be implemented efficiently such that that no value is computed twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation on DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Forward pass** \n",
    "```\n",
    "\n",
    "Note that a neural network as a DAG of compute and parameter nodes that implements a function $f$ can be extended to implement the calculation of the loss value for each training example and parameter setting. The values for each node are calculated from left to right,  storing every value in the nodes so we don't have to recompute any known value. Assuming each activation and each arithmetic operation between a weight and an input takes constant time, then one forward pass takes $\\mathcal O(V + E)$ calculations, i.e. the network size. The node values after completing a forward pass reflect the current network state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "**Backward pass** \n",
    "```\n",
    "\n",
    "During backward pass, we divide the calculation of gradients into two groups: (1) **local gradients** obtained when perturbing adjacent compute nodes $u$ and $w$, and (2) **backpropagated gradients** of the form ${\\frac{\\partial{\\mathcal L}}{\\partial u}}$ for a node ${u}.$ Our goal is to calculate the gradient of the top-most node with respect to the leaves of the graph (i.e. nodes with zero fan-in).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation proceeds by **induction**. (1) For the base step, $\\frac{\\partial{\\mathcal L}}{\\partial \\mathcal L} = 1$ for the node which computes the loss value. This value is stored. (2) For the inductive step, suppose ${\\frac{\\partial{\\mathcal L}}{\\partial u}}$ are stored for each compute node $u$ in the upper layer, then after computing local gradients ${\\frac{\\partial{u}}{\\partial w}}$, the backpropagated gradients ${\\frac{\\partial{\\mathcal L}}{\\partial w}}$ for compute nodes $w$ can be calculated via the chain rule:\n",
    "\n",
    "$${ \\frac{\\partial\\mathcal L}{\\partial w} } = \\sum_{ {u} }\\left( {{\\frac{\\partial\\mathcal L}{\\partial u}}} \\right)\\left( {{\\frac{\\partial{u}}{\\partial w}}} \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus, continuing the \"flow\" of gradients to the current layer. The process ends on so-called **leaf nodes** which are nodes with zero fan-in. Note that (1) partial derivatives are evaluated on the current network state &mdash; these values are stored during forward prop which precedes backprop; (2) all backpropagated gradients are stored in each compute node for use by the next layer. On the other hand, there is no need to store local gradients; these are computed as needed.\n",
    "    \n",
    "\n",
    "```{figure} ../img/backprop-compgraph.png\n",
    "---\n",
    "width: 35em\n",
    "name: backprop-compgraph\n",
    "---\n",
    "Backprop on a generic comp. graph with fan out > 1 on node <code>y</code>. Each backpropagated gradient computation is stored in the corresponding node. For node <code>y</code> to calculate the backpropagated gradient we have to sum over the two incoming gradients which can be implemented using matrix multiplication of the gradient vectors.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation algorithm.** Now that we know how to compute each backpropagated gradient implemented as `u.backward()` for node `u` which *sends* its gradient $\\partial \\mathcal L / \\partial u$ to all its parent nodes, i.e. nodes on the lower layer. We now write the complete algorithm:\n",
    "\n",
    "```python \n",
    "def Forward():\n",
    "    for c in compute: \n",
    "        c.forward()\n",
    "\n",
    "def Backward(loss):\n",
    "    for c in compute: c.grad = 0\n",
    "    for c in params:  c.grad = 0\n",
    "    for c in inputs:  c.grad = 0\n",
    "    loss.grad = 1\n",
    "\n",
    "    for c in compute[::-1]: \n",
    "        c.backward()\n",
    "\n",
    "def SGD(eta):\n",
    "    for w in params:\n",
    "        w.value -= eta * w.grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Backpropagation on fully connected networks\n",
    "\n",
    "To demonstrate the above observations concretely, we consider a dense fully connected neural network as an example of a DAG. Let ${z_k}^{[t]} = \\sum_l {w_{kl}}^{[t]}{a_l}^{[t-1]}$ and ${a_j}^{[t]} = \\phi^{[t]}({z_k}^{[t]})$ be the values of compute nodes at the $t$th layer of the network. Then, the backpropagated gradients are given by\n",
    "    \n",
    "$$\\begin{aligned}\n",
    "        \\dfrac{\\partial \\mathcal L}{\\partial {a_j}^{[t]}} \n",
    "        &= \\sum_{k}\\dfrac{\\partial \\mathcal L}{\\partial {z_k}^{[t+1]}} \\dfrac{\\partial {z_k}^{[t+1]}}{\\partial {a_j}^{[t]}} = \\sum_{k}\\dfrac{\\partial \\mathcal L}{\\partial {z_k}^{[t+1]}} {w_{kj}}^{[t+1]}\n",
    "    \\end{aligned}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\dfrac{\\partial \\mathcal L}{\\partial {z_j}^{[t]}} \n",
    "    &= \\sum_{k}\\dfrac{\\partial \\mathcal L}{\\partial {a_k}^{[t]}} \\dfrac{\\partial {a_k}^{[t]}}{\\partial {z_j}^{[t]}}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Similarly, the backpropagated gradients for the parameter nodes is given by\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\dfrac{\\partial \\mathcal L}{\\partial {w_{kl}}^{[t]}} \n",
    "    &= \\dfrac{\\partial \\mathcal L}{\\partial {z_k}^{[t]}} \\dfrac{\\partial {z_k}^{[t]}}{\\partial {w_{kl}}^{[t]}} = \\dfrac{\\partial \\mathcal L}{\\partial {z_k}^{[t]}} {a^{[t-1]}_l}. \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "The backpropagated gradients are stored on the compute and parameter nodes until the weights are updated. Then, they are set to zero since we get a different state after an update of the network parameters. Observe that the first factor in each term are backpropagated gradients stored in the compute nodes of the $t+1$-layer. The latter factor are local gradients obtained using autodifferentiation. Both set of gradients are evaluated on the current network state obtained during forward pass. Finally, the gradients are combined using matrix multiplication. Thus, the equations are consistent with the claim that to compute  ${\\partial\\mathcal L} / {\\partial w}$ for any parameter node $w$, we only need to access gradients stored in the compute nodes on the upper layer. \n",
    "    \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the ff:\n",
    "* **Modularity.** The dependence only on nodes belonging to the upper layer suggests a modularity in the computation, e.g. we can connect DAG layers with possibly distinct network architectures by only connecting the nodes between layers. \n",
    "\n",
    "<br>\n",
    "\n",
    "* **Bottleneck and complexity.** From the above discussion, it follows that the bottlenecks for backprop in general are (1) large matrix multiplication in the chain rule, and (2) autodifferentiation for finding local gradients. If each node computation takes unit time, then backpropagation requires $\\mathcal O(V + E)$ computations, $E$ for chain rule and autodifferentiation, and $V$ for the gradient step. Fast GPUs can reduce this complexity by some constant factor. \n",
    "\n",
    "<br>\n",
    "\n",
    "* **Leaf tensors in PyTorch.** Observe that parameter nodes are compute nodes that have zero fan-in. This explains why we only have to store gradients for compute nodes, since the gradient stops flowing on each parameter node. In PyTorch, the backpropagated gradients for the current state can be accessed in a leaf tensor $u$ using `u.grad` assuming `requires_grad=True`. Here a leaf tensor can be thought of as a high-dimensional array of leaf nodes, i.e. nodes with zero fan-in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../img/backprop-compgraph2.png\n",
    "---\n",
    "width: 35em\n",
    "name: backprop-compgraph2\n",
    "---\n",
    "Backprop with weights for a single layer neural network with sigmoid activation and cross-entropy loss. Observe the gradient flowing from node <code>L</code> to the node <code>w0</code>.\n",
    "```\n",
    "\n",
    "```{figure} ../img/backprop-compgraph3.png\n",
    "---\n",
    "width: 25em\n",
    "name: backprop-compgraph3\n",
    "---\n",
    "Backprop with weights for a single layer neural network with sigmoid activation and cross-entropy loss. Local gradients that require current values of the nodes while backpropagated gradients are accessed from the layer above. Node <code>u</code> which has fan-in > 1 performs chain rule on the backpropagated gradients.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch autograd\n",
    "\n",
    "The `autograd` package allows automatic differentiation by building computational graphs on the fly every time we pass data through our model. Autograd tracks which data combined through which operations to produce the output. This allows us to take derivatives over ordinary imperative code. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Backward for scalars.** Let $y = \\mathbf x^\\top \\mathbf x = \\sum_i {x_i}^2.$ In this example, we initialize a tensor `x` which initially has no gradient. Calling backward on `y` results in gradients being stored on the leaf tensor `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float, requires_grad=True)\n",
    "y = x.T @ x \n",
    "\n",
    "y.backward() \n",
    "(x.grad == 2*x).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward for vectors.** Let $\\mathbf y = g(\\mathbf x)$ and let $\\mathbf v$ be a vector having the same length as $\\mathbf y.$ Then `y.backward(v)` implements   \n",
    "\n",
    "$$\\sum_i v_i \\left(\\frac{\\partial y_i}{\\partial x_j}\\right)$$ \n",
    "  \n",
    "resulting in a vector of same length as `x` that is stored in `x.grad`. Note that the terms on the right are the local gradients in backprop. Hence, if `v` contains backpropagated gradients of nodes that depend on `y`, then this operation gives us the backpropagated gradients with respect to `x`, i.e. setting $v_i = \\frac{\\partial \\mathcal{L} }{\\partial y_i}$ gives us the vector $\\frac{\\partial \\mathcal{L} }{\\partial x_j}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(size=(4,), dtype=torch.float, requires_grad=True)\n",
    "v = torch.rand(size=(2,), dtype=torch.float)\n",
    "y = x[:2]\n",
    "\n",
    "# Computing the Jacobian by hand\n",
    "J = torch.tensor(\n",
    "    [[1, 0, 0, 0],\n",
    "    [0, 1, 0, 0]], dtype=torch.float\n",
    ")\n",
    "\n",
    "# Confirming the above formula\n",
    "y.backward(v)\n",
    "(x.grad == v @ J).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locally disabling gradient tracking.** To stop PyTorch from building computational graphs, we can put the code inside a `with torch.no_grad()` block. In this mode, the result of every computation will have `requires_grad=False`, even when the inputs have `requires_grad=True`. \n",
    "<br><br>\n",
    "Another method is to use the `.detach()` method which returns a new tensor detached from the current graph but shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. Disabling gradient computation is useful when computing values, e.g. accuracy, whose gradients will not be backpropagated into the network."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9b28b0fcd5be7e4e7eff8f7cf674e5d8a777a4aa6f51979c0f0ca8ec7b00ef4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
