{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ensembling: Blending and Stacking","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we implement stacking of machine learning. Stacking several uncorrelated models is known to generalize better than individual models. Stacking mainly requires good cross-validation strategy between levels of prediction. In particular, we will demostrate that maintaining the same cross-validation folds between levels minimizes overfitting.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize, fmin\nfrom xgboost import XGBClassifier\n\nfrom sklearn import model_selection, linear_model, metrics, decomposition, ensemble\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n\nfrom functools import partial\nfrom typing import List\nimport warnings\nimport random\n\n\nwarnings.simplefilter(action='ignore')\nNUM_FOLDS = 5","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-24T16:46:42.841014Z","iopub.execute_input":"2021-08-24T16:46:42.841429Z","iopub.status.idle":"2021-08-24T16:46:42.849125Z","shell.execute_reply.started":"2021-08-24T16:46:42.841396Z","shell.execute_reply":"2021-08-24T16:46:42.847768Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"markdown","source":"We do not really care too much about the dataset. The dataset used here is particularly nice. No issues. Idea is that we have text data in the form of a movie review, along with its sentiment classification. We will build a **sentiment classifier** using an ensemble of three models.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv', sep='\\t')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:46:42.853669Z","iopub.execute_input":"2021-08-24T16:46:42.854012Z","iopub.status.idle":"2021-08-24T16:46:43.211139Z","shell.execute_reply.started":"2021-08-24T16:46:42.853982Z","shell.execute_reply":"2021-08-24T16:46:43.210236Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"       id  sentiment                                             review\n0  5814_8          1  With all this stuff going down at the moment w...\n1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n3  3630_4          0  It must be assumed that those who praised this...\n4  9495_8          1  Superbly trashy and wondrously unpretentious 8...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5814_8</td>\n      <td>1</td>\n      <td>With all this stuff going down at the moment w...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2381_9</td>\n      <td>1</td>\n      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7759_3</td>\n      <td>0</td>\n      <td>The film starts with a manager (Nicholas Bell)...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3630_4</td>\n      <td>0</td>\n      <td>It must be assumed that those who praised this...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9495_8</td>\n      <td>1</td>\n      <td>Superbly trashy and wondrously unpretentious 8...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Train and test split","metadata":{}},{"cell_type":"code","source":"df_train, df_test = model_selection.train_test_split(df, test_size=0.20)\nprint(df_train.shape, df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:46:43.212686Z","iopub.execute_input":"2021-08-24T16:46:43.212994Z","iopub.status.idle":"2021-08-24T16:46:43.225145Z","shell.execute_reply.started":"2021-08-24T16:46:43.212966Z","shell.execute_reply":"2021-08-24T16:46:43.223960Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(20000, 3) (5000, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Cross-validation folds","metadata":{}},{"cell_type":"markdown","source":"Here we create cross-validation folds. Very important for evaluating models, and creating Level 1 features that are not overfitted.","metadata":{}},{"cell_type":"code","source":"df_train.loc[:, 'kfold'] = -1 \ndf_train = df_train.sample(frac=1.0).reset_index(drop=True)\ny = df_train['sentiment'].values\n\nskf = model_selection.StratifiedKFold(n_splits=NUM_FOLDS+1)\nfor f, (t_, v_) in enumerate(skf.split(X=df_train, y=y)):\n    df_train.loc[v_, \"kfold\"] = f","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:46:43.226933Z","iopub.execute_input":"2021-08-24T16:46:43.227452Z","iopub.status.idle":"2021-08-24T16:46:43.251156Z","shell.execute_reply.started":"2021-08-24T16:46:43.227414Z","shell.execute_reply":"2021-08-24T16:46:43.249976Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_train.kfold.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:46:43.253275Z","iopub.execute_input":"2021-08-24T16:46:43.253776Z","iopub.status.idle":"2021-08-24T16:46:43.263709Z","shell.execute_reply.started":"2021-08-24T16:46:43.253726Z","shell.execute_reply":"2021-08-24T16:46:43.262575Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0    3334\n1    3334\n2    3333\n3    3333\n4    3333\n5    3333\nName: kfold, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Stacking","metadata":{}},{"cell_type":"markdown","source":"We define a class that automates training and prediction of stacked models. Several models can be trained on the training set whose predict probabilities can be used as feature for a further metamodel called a **stacker**. Observe that this process can be iterated to several more levels. To avoid creating meta features that are overfitted to the train set, the meta features are generated by out-of-fold (OOF) training and prediction of the models on the features of the previous level. This requires defining cross-validation folds. The same cross-validation folds will be used to generate metafeatures at deeper levels. This will be justified later. \n\nAfter generating metafeatures, the models will be retrained on the whole training set (not just on train folds). This increases accuracy of prediction on the test set. Finally, prediction on the test set will simulate conditions when the model was trained &mdash; essentially the test set acts like an extra validation fold.\n\n\n:::{note}\nAlternatively, we could make predictions on the test dataset using each base model immediately after it gets fitted on each fold. In our case, this would generate test-set predictions for five of each base models. Then, we would average the predictions per model to generate our level 1 meta features.\n\nOne benefit to this is that it’s less time consuming than the first approach (since we don’t have to retrain each model on the full training dataset). It also helps that our train meta features and test meta features should follow a similar distribution. However, the test meta features are likely more accurate in the first approach since each base model was trained on the full training dataset (as opposed to 80% of the training dataset, five times in the 2nd approach).\n:::\n\n","metadata":{}},{"cell_type":"markdown","source":"### Implementation","metadata":{}},{"cell_type":"code","source":"class StackingClassifier:\n    \"\"\"Implements model stacking for classification.\"\"\"\n    \n    def __init__(self, model_dict_list):\n        \"\"\"Initialize by passing `model_dict` which is a list of dictionaries \n        of name-model pairs for each level.\"\"\"\n        \n        self.model_dict_list = model_dict_list\n        self.cv_scores_ = {}\n        self.metafeatures_ = None\n        \n    def fit(self, df):\n        \"\"\"Fit classifier. This assumes `df` is a DataFrame with \"id\", \"kfold\", \n        \"sentiment\" (target) columns, followed by features columns.\"\"\"\n        \n        df = df.copy()\n        \n        # Iterating over all stacking levels\n        metafeatures = []\n        for m in range(len(self.model_dict_list)):\n            \n            # Get models in current layer\n            model_dict = self.model_dict_list[m]\n            level = m + 1\n            \n            # Identify feature columns, i.e. preds of prev. layer\n            if m == 0:\n                feature_cols = ['review']\n            else:\n                prev_level_names = self.model_dict_list[m-1].keys()\n                feature_cols = [f'{name}_{level-1}' for name in prev_level_names]\n            \n            # Iterate over models in the current layer\n            for model_name in model_dict.keys():\n                print(f'\\nLevel {level} preds: {model_name}')\n                self.cv_scores_[f'{model_name}_{level}'] = []\n                model = model_dict[model_name]\n                \n                # Generate feature for next layer models from OOF preds\n                oof_preds = []\n                for j in range(df.kfold.nunique()):\n                    oof_pred, oof_auc = self._oof_pred(df, feature_cols, model, \n                                                        model_name, fold=j, level=level)\n                    oof_preds.append(oof_pred)\n                    self.cv_scores_[f'{model_name}_{level}'].append(oof_auc)\n                \n                pred = pd.concat(oof_preds)\n                df = df.merge(pred[['id', f'{model_name}_{level}']], on='id', how='left')   \n                metafeatures.append(f'{model_name}_{level}')\n        \n                # Train models on entire feature columns for inference\n                model.fit(df[feature_cols], df.sentiment.values)\n        \n        self.metafeatures_ = df[metafeatures]\n        return self\n        \n    def predict_proba(self, test_df):\n        \"\"\"Return classification probabilities.\"\"\"\n        \n        test_df = test_df.copy()\n        \n        # Iterate over layers to make predictions\n        for m in range(len(self.model_dict_list)):\n            \n            # Get models for current layer\n            model_dict = self.model_dict_list[m]\n            level = m + 1\n            \n            # Get feature columns to use for prediction\n            if m == 0:\n                feature_cols = ['review']\n            else:\n                prev_names = self.model_dict_list[m-1].keys()\n                feature_cols = [f\"{model_name}_{level-1}\" for model_name in prev_names]\n\n            # Append predictions to test DataFrame\n            for model_name in model_dict.keys():\n                model = model_dict[model_name]\n                pred = model.predict_proba(test_df[feature_cols])[:, 1] \n                test_df.loc[:, f\"{model_name}_{level}\"] = pred\n                    \n        # Return last predictions\n        return np.c_[1 - pred, pred]\n        \n    def _oof_pred(self, df, feature_cols, model, model_name, fold, level):\n        \"Train on K-1 folds, predict on fold K. Return OOF predictions with IDs.\"\n\n        # Get folds; include ID and target cols, and feature cols\n        df_trn = df[df.kfold != fold][['id', 'sentiment']+feature_cols]\n        df_oof = df[df.kfold == fold][['id', 'sentiment']+feature_cols]\n        \n        # Fit model. \n        model.fit(df_trn[feature_cols], df_trn.sentiment.values)\n        oof_pred = model.predict_proba(df_oof[feature_cols])[:, 1] \n        auc = metrics.roc_auc_score(df_oof.sentiment.values, oof_pred)\n        print(f\"fold={fold}, auc={auc}\")\n\n        # Return OOF predictions with ids\n        df_oof.loc[:, f\"{model_name}_{level}\"] = oof_pred\n        return df_oof[[\"id\", f\"{model_name}_{level}\"]], auc","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:46:43.265354Z","iopub.execute_input":"2021-08-24T16:46:43.265678Z","iopub.status.idle":"2021-08-24T16:46:43.285930Z","shell.execute_reply.started":"2021-08-24T16:46:43.265650Z","shell.execute_reply":"2021-08-24T16:46:43.284842Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Blending","metadata":{}},{"cell_type":"markdown","source":"Let's start with a simple stacked model where we simply perform a weighted average of the prediction probabilities. This method is called **blending**. We will use three base models to generate probabilities. Hopefully these are uncorrelated:\n1. Logistic Regression + TF-IDF\n2. Logistic Regression + Count Vectorizer\n3. Random Forest + TF-IDF + SVD","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:34:52.240952Z","iopub.execute_input":"2021-08-23T17:34:52.241474Z","iopub.status.idle":"2021-08-23T17:34:52.256728Z","shell.execute_reply.started":"2021-08-23T17:34:52.241439Z","shell.execute_reply":"2021-08-23T17:34:52.255385Z"}}},{"cell_type":"code","source":"class ReviewColumnExtractor(BaseEstimator, ClassifierMixin):\n    \"\"\"Extract text column, e.g. letting X = df_train[['review']]\n    as train dataset for TfidfVectorizer and CountVectorizer does\n    not work as expected.\"\"\"\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X.review","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:46:43.287202Z","iopub.execute_input":"2021-08-24T16:46:43.287501Z","iopub.status.idle":"2021-08-24T16:46:43.303921Z","shell.execute_reply.started":"2021-08-24T16:46:43.287470Z","shell.execute_reply":"2021-08-24T16:46:43.302812Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Initialize base models:","metadata":{}},{"cell_type":"code","source":"# logistic regression + tfidf\nlr = make_pipeline(\n    ReviewColumnExtractor(),\n    TfidfVectorizer(max_features=1000),\n    linear_model.LogisticRegression()\n)\n\n# logistic regression + count vectorizer\nlr_cnt = make_pipeline(\n    ReviewColumnExtractor(),\n    CountVectorizer(),\n    linear_model.LogisticRegression(solver='liblinear')\n)\n\n# random forest + decomposed (svd) tfidf features\nrf_svd = make_pipeline(\n    ReviewColumnExtractor(),\n    TfidfVectorizer(max_features=None),\n    decomposition.TruncatedSVD(n_components=120),\n    ensemble.RandomForestClassifier(n_estimators=100, n_jobs=-1)\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:46:43.305470Z","iopub.execute_input":"2021-08-24T16:46:43.306071Z","iopub.status.idle":"2021-08-24T16:46:43.317709Z","shell.execute_reply.started":"2021-08-24T16:46:43.306024Z","shell.execute_reply":"2021-08-24T16:46:43.315976Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Run training:","metadata":{}},{"cell_type":"code","source":"basemodels = {'lr': lr, 'lr_cnt': lr_cnt, 'rf_svd': rf_svd}\nstack = StackingClassifier([basemodels])\nstack.fit(df_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:46:43.320338Z","iopub.execute_input":"2021-08-24T16:46:43.320668Z","iopub.status.idle":"2021-08-24T16:51:02.394511Z","shell.execute_reply.started":"2021-08-24T16:46:43.320638Z","shell.execute_reply":"2021-08-24T16:51:02.393089Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\nLevel 1 preds: lr\nfold=0, auc=0.9384901442591425\nfold=1, auc=0.9335381634668255\nfold=2, auc=0.9280035949592794\nfold=3, auc=0.9304714786729175\nfold=4, auc=0.9414464824536768\nfold=5, auc=0.937182146174952\n\nLevel 1 preds: lr_cnt\nfold=0, auc=0.950827093427299\nfold=1, auc=0.9397791490696997\nfold=2, auc=0.9401545861295928\nfold=3, auc=0.9398243567308318\nfold=4, auc=0.946280093042683\nfold=5, auc=0.9494958987764743\n\nLevel 1 preds: rf_svd\nfold=0, auc=0.8836239531783937\nfold=1, auc=0.8793645155904087\nfold=2, auc=0.8715397976827204\nfold=3, auc=0.8733627512404492\nfold=4, auc=0.8865565205493264\nfold=5, auc=0.8709063379926689\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<__main__.StackingClassifier at 0x7f81e4aace10>"},"metadata":{}}]},{"cell_type":"markdown","source":"Check if basemodels are uncorrelated:","metadata":{}},{"cell_type":"code","source":"stack.metafeatures_.corr()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:51:02.396539Z","iopub.execute_input":"2021-08-24T16:51:02.396849Z","iopub.status.idle":"2021-08-24T16:51:02.409242Z","shell.execute_reply.started":"2021-08-24T16:51:02.396819Z","shell.execute_reply":"2021-08-24T16:51:02.408492Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"              lr_1  lr_cnt_1  rf_svd_1\nlr_1      1.000000  0.886559  0.828966\nlr_cnt_1  0.886559  1.000000  0.722760\nrf_svd_1  0.828966  0.722760  1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lr_1</th>\n      <th>lr_cnt_1</th>\n      <th>rf_svd_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>lr_1</th>\n      <td>1.000000</td>\n      <td>0.886559</td>\n      <td>0.828966</td>\n    </tr>\n    <tr>\n      <th>lr_cnt_1</th>\n      <td>0.886559</td>\n      <td>1.000000</td>\n      <td>0.722760</td>\n    </tr>\n    <tr>\n      <th>rf_svd_1</th>\n      <td>0.828966</td>\n      <td>0.722760</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"stack.metafeatures_.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:51:02.410428Z","iopub.execute_input":"2021-08-24T16:51:02.410914Z","iopub.status.idle":"2021-08-24T16:51:02.426357Z","shell.execute_reply.started":"2021-08-24T16:51:02.410868Z","shell.execute_reply":"2021-08-24T16:51:02.425331Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"       lr_1  lr_cnt_1  rf_svd_1\n0  0.007027  0.000033      0.24\n1  0.884595  0.999684      0.71\n2  0.063354  0.001449      0.44\n3  0.420351  0.421524      0.63\n4  0.231211  0.161320      0.56","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lr_1</th>\n      <th>lr_cnt_1</th>\n      <th>rf_svd_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.007027</td>\n      <td>0.000033</td>\n      <td>0.24</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.884595</td>\n      <td>0.999684</td>\n      <td>0.71</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.063354</td>\n      <td>0.001449</td>\n      <td>0.44</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.420351</td>\n      <td>0.421524</td>\n      <td>0.63</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.231211</td>\n      <td>0.161320</td>\n      <td>0.56</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can also check scores of the base models on each validation fold. This informs us of the stability of the folds and the cross-validation performance of the base models. ","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(stack.cv_scores_).describe().loc[['mean', 'std']]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:51:02.427659Z","iopub.execute_input":"2021-08-24T16:51:02.427954Z","iopub.status.idle":"2021-08-24T16:51:02.459465Z","shell.execute_reply.started":"2021-08-24T16:51:02.427926Z","shell.execute_reply":"2021-08-24T16:51:02.458748Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"          lr_1  lr_cnt_1  rf_svd_1\nmean  0.934855  0.944394  0.877559\nstd   0.005098  0.005121  0.006620","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lr_1</th>\n      <th>lr_cnt_1</th>\n      <th>rf_svd_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>0.934855</td>\n      <td>0.944394</td>\n      <td>0.877559</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.005098</td>\n      <td>0.005121</td>\n      <td>0.006620</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Let us try to blend the probabilities using some hand-designed coefficients.","metadata":{}},{"cell_type":"code","source":"target = df_train.sentiment.values\n\n# roc is scale invariant, so we dont bother dividing by total weights\navg_preds = (stack.metafeatures_ * [1, 1, 1]).sum(axis=1)\nwtd_preds = (stack.metafeatures_ * [1, 3, 1]).sum(axis=1)\nrank_avg_preds = (stack.metafeatures_.rank() * [1, 1, 1]).sum(axis=1)\nrank_wtd_preds = (stack.metafeatures_.rank() * [1, 3, 1]).sum(axis=1)\n\n# Calculate AUC over combined OOF preds\nprint(f\"Train OOF-AUC (averaged):     \", metrics.roc_auc_score(target, avg_preds))\nprint(f\"Train OOF-AUC (wtd. avg):     \", metrics.roc_auc_score(target, wtd_preds))\nprint(f\"Train OOF-AUC (rank avg):     \", metrics.roc_auc_score(target, rank_avg_preds)) \nprint(f\"Train OOF-AUC (wtd. rank avg):\", metrics.roc_auc_score(target, rank_wtd_preds))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:51:02.460501Z","iopub.execute_input":"2021-08-24T16:51:02.460886Z","iopub.status.idle":"2021-08-24T16:51:02.525240Z","shell.execute_reply.started":"2021-08-24T16:51:02.460859Z","shell.execute_reply":"2021-08-24T16:51:02.524142Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Train OOF-AUC (averaged):      0.9481721046043312\nTrain OOF-AUC (wtd. avg):      0.949190945103563\nTrain OOF-AUC (rank avg):      0.9432180771768579\nTrain OOF-AUC (wtd. rank avg): 0.9492301051227515\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Since these coefficients are hand-designed, we may want to devise a strategy for automatically finding the optimal coefficients for blending. This is accomplished by the folowing class.","metadata":{}},{"cell_type":"code","source":"class Blender(BaseEstimator, ClassifierMixin):\n    \"\"\"Implement blending that maximizes AUC score.\"\"\"\n    \n    def __init__(self, rank=False):\n        self.coef_ = None\n        self.rank = rank\n\n    def fit(self, X, y):\n        \"\"\"Find optimal blending coefficients.\"\"\"\n        \n        if self.rank:\n            X = X.rank()\n\n        self.coef_ = self._optimize_auc(X, y)\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Return blended probabilities for class 0 and class 1.\"\"\"\n        \n        if self.rank:\n            X = X.rank()\n            \n        pred = np.sum(X * self.coef_, axis=1)\n        return np.c_[1 - pred, pred]\n\n    def _auc(self, coef, X, y):\n        \"\"\"Calculate AUC of blended predict probas.\"\"\"\n\n        auc = metrics.roc_auc_score(y, np.sum(X * coef, axis=1))\n        return -1.0 * auc # min -auc = max auc\n    \n    def _optimize_auc(self, X, y):\n        \"\"\"Maximize AUC as a bound-constrained optimization problem using Nelder-Mead \n        method with Dirichlet init. \n        \n        Reference: \n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n        \"\"\"\n        partial_loss = partial(self._auc, X=X, y=y) \n        init_coef = np.random.dirichlet(np.ones(X.shape[1]))\n        return minimize(partial_loss, init_coef, \n                        method='Nelder-Mead', \n                        bounds=[(0, 1), (0, 1), (0, 1)])['x']","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:51:02.526340Z","iopub.execute_input":"2021-08-24T16:51:02.526760Z","iopub.status.idle":"2021-08-24T16:51:02.537607Z","shell.execute_reply.started":"2021-08-24T16:51:02.526731Z","shell.execute_reply":"2021-08-24T16:51:02.536628Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"This implementation uses `partial` from `functools` and `minimize` from `scipy.optimize` to minimize the coefficients constained in $(0, 1).$ The initial values of the coefficient are drawn from a Dirichlet distribution $\\operatorname{Dir}(\\boldsymbol{\\alpha})$ with $\\boldsymbol{\\alpha} = [1, 1, 1].$","metadata":{}},{"cell_type":"code","source":"target = df_train.sentiment.values\n\n# Blended predictions\nblender = Blender()\nblender.fit(stack.metafeatures_, target)\ncombined_oof_preds = (stack.metafeatures_ * blender.coef_).sum(axis=1)\n\n# Blended ranked predictions\nblender_rk = Blender(rank=True)\nblender_rk.fit(stack.metafeatures_, target)\ncombined_oof_rk_preds = (stack.metafeatures_.rank() * blender_rk.coef_).sum(axis=1)\n\nprint(f\"Train OOF-AUC (Blended):    \", metrics.roc_auc_score(target, combined_oof_preds))\nprint(f\"Train OOF-AUC (Blended rk.):\", metrics.roc_auc_score(target, combined_oof_rk_preds))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:51:02.538948Z","iopub.execute_input":"2021-08-24T16:51:02.539399Z","iopub.status.idle":"2021-08-24T16:51:04.355350Z","shell.execute_reply.started":"2021-08-24T16:51:02.539368Z","shell.execute_reply":"2021-08-24T16:51:04.354152Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Train OOF-AUC (Blended):     0.949660465333628\nTrain OOF-AUC (Blended rk.): 0.9501244955610028\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note that this is not the same as train AUC. However, this is a better approximation of the test AUC. Calculating the AUC on the entire out-of-fold predictions involves tracking the rows of the confusion matrix, which is the sum of the confusion matrix of each fold, over all thresholds. On the other hand, the latter approach tracks each confusion matrix separately, then averages the individual AUCs. The two should be similar to cross-validation scores if error is well-distributed between folds &mdash; and we are blending probabilities. [^ref]\n\n[^ref]: For some reason OOF-AUC is bad when blending ranking models, e.g. linear regression, and usual classifiers, even after transforming predict probabilities to rank.","metadata":{}},{"cell_type":"code","source":"# Inference\ntest_target = df_test.sentiment.values\ntest_features = []\nfor model_name in basemodels.keys():\n    test_features.append(basemodels[model_name].predict_proba(df_test)[:, 1])\n\ntest_pred = (pd.DataFrame(np.c_[test_features].T) * blender.coef_).sum(axis=1)\ntest_rk_pred = (pd.DataFrame(np.c_[test_features].T) * blender_rk.coef_).sum(axis=1)\nprint('Test AUC (Blended):    ', metrics.roc_auc_score(test_target, test_pred))\nprint('Test AUC (Blended rk.):', metrics.roc_auc_score(test_target, test_rk_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:51:04.356930Z","iopub.execute_input":"2021-08-24T16:51:04.357388Z","iopub.status.idle":"2021-08-24T16:51:08.800437Z","shell.execute_reply.started":"2021-08-24T16:51:04.357341Z","shell.execute_reply":"2021-08-24T16:51:08.799424Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Test AUC (Blended):     0.9466536617647081\nTest AUC (Blended rk.): 0.9466128614448339\n","output_type":"stream"}]},{"cell_type":"markdown","source":":::{tip}\nUsing blended **rank probabilities** is a good trick when optimizing AUC score. Here individual probabilities are replaced by their rank index. Recall that AUC only cares about the predict probability of a randomly chosen negative examples to be assigned lower predict proba than a randomly chosen positive example. Note that this only works for ensembles; for single models using rank probabilities does not affect AUC score.\n:::","metadata":{}},{"cell_type":"markdown","source":"### XGB Metamodel","metadata":{}},{"cell_type":"markdown","source":"Blending can be easily generalized to more complex machine learning model that learns and predicts with the metafeatures using more complex algorithms. For example, we can use `XGBoostClassifier`.","metadata":{}},{"cell_type":"code","source":"basemodels = {'lr': lr, 'lr_cnt': lr_cnt, 'rf_svd': rf_svd}\nmetamodel = {'xgb': XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False)}\nstack = StackingClassifier([basemodels, metamodel])\nstack.fit(df_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:51:08.803677Z","iopub.execute_input":"2021-08-24T16:51:08.803988Z","iopub.status.idle":"2021-08-24T16:55:34.464074Z","shell.execute_reply.started":"2021-08-24T16:51:08.803960Z","shell.execute_reply":"2021-08-24T16:55:34.463324Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\nLevel 1 preds: lr\nfold=0, auc=0.9384901442591425\nfold=1, auc=0.9335381634668255\nfold=2, auc=0.9280035949592794\nfold=3, auc=0.9304714786729175\nfold=4, auc=0.9414464824536768\nfold=5, auc=0.937182146174952\n\nLevel 1 preds: lr_cnt\nfold=0, auc=0.950827093427299\nfold=1, auc=0.9397791490696997\nfold=2, auc=0.9401545861295928\nfold=3, auc=0.9398243567308318\nfold=4, auc=0.946280093042683\nfold=5, auc=0.9494958987764743\n\nLevel 1 preds: rf_svd\nfold=0, auc=0.8802085582434412\nfold=1, auc=0.8797558591782036\nfold=2, auc=0.8708704237543847\nfold=3, auc=0.8721752327867435\nfold=4, auc=0.8833828432749296\nfold=5, auc=0.8728789941020156\n\nLevel 2 preds: xgb\nfold=0, auc=0.9485932862353577\nfold=1, auc=0.9412457429014772\nfold=2, auc=0.9417221597697266\nfold=3, auc=0.9407211168002535\nfold=4, auc=0.9483645516019616\nfold=5, auc=0.9488598310540757\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<__main__.StackingClassifier at 0x7f81e466cbd0>"},"metadata":{}}]},{"cell_type":"code","source":"y_train = df_train.sentiment.values\ny_test = df_test.sentiment.values\n\nprint(f\"Train AUC (XGB stack):\", metrics.roc_auc_score(y_train, stack.predict_proba(df_train)[:, 1]))\nprint(f\"Test AUC  (XGB stack):\", metrics.roc_auc_score(y_test, stack.predict_proba(df_test)[:, 1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:55:34.465284Z","iopub.execute_input":"2021-08-24T16:55:34.465765Z","iopub.status.idle":"2021-08-24T16:55:55.513546Z","shell.execute_reply.started":"2021-08-24T16:55:34.465734Z","shell.execute_reply":"2021-08-24T16:55:55.512611Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Train AUC (XGB stack): 0.9990695195440646\nTest AUC  (XGB stack): 0.9437543590341748\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame(stack.cv_scores_).describe().loc[['mean', 'std']]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:55:55.514760Z","iopub.execute_input":"2021-08-24T16:55:55.515055Z","iopub.status.idle":"2021-08-24T16:55:55.539559Z","shell.execute_reply.started":"2021-08-24T16:55:55.515026Z","shell.execute_reply":"2021-08-24T16:55:55.538499Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"          lr_1  lr_cnt_1  rf_svd_1     xgb_2\nmean  0.934855  0.944394  0.876545  0.944918\nstd   0.005098  0.005121  0.005200  0.004056","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lr_1</th>\n      <th>lr_cnt_1</th>\n      <th>rf_svd_1</th>\n      <th>xgb_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>0.934855</td>\n      <td>0.944394</td>\n      <td>0.876545</td>\n      <td>0.944918</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.005098</td>\n      <td>0.005121</td>\n      <td>0.005200</td>\n      <td>0.004056</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Observe that cross-validated AUC scores is indicative of test performance. Meanwhile, train AUC is useless. A better estimate is the mean cross-validation AUC score. If we assume that each fold has the same error distribution, then this should approximate the test AUC which can be thought of as predicting on another fold (i.e. fold `NUM_FOLDS + 1`). Indeed, the above results supports this.","metadata":{}},{"cell_type":"markdown","source":"### Conclusion\n\nThe above examples of building ensembles with **blending** or **stacking** (e.g. with XGBoost) show that stacked models significantly outperform single models.","metadata":{}},{"cell_type":"markdown","source":"## Experiment: CV Folds","metadata":{}},{"cell_type":"markdown","source":"Consider stacking three levels of models where level three is just a single classifier. Our current implementation does this by keeping the same cross-validation folds when training the level 2 models [^ref2]. It is unclear whether using same folds between levels 1 and 2 affect generalization error. To check this empirically, we compute cv scores with folds unshuffled (the usual) and shuffled (new) for training models in level 2. We also calculate test set AUC. If CV scores decrease and test AUC decrease, while overall train AUC increases, then this indicates using a different validation fold between levels results in overfitting.\n\nHere we have level two features from the three metamodels. We blend the predict probabilities of each metamodel optimizing AUC as before.\n\n[^ref2]: GM Abishek Thakur recommends keeping the same folds in [AAAMLP](https://github.com/abhishekkrthakur/approachingalmost/blob/master/AAAMLP.pdf).","metadata":{}},{"cell_type":"code","source":"class LinearRegressionClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"Linear regression for model-based AUC optimization.\n    Note that we transform probabilities to rank probabilities!\"\"\"\n    \n    def __init__(self): \n        self.lr = linear_model.LinearRegression()\n        \n    def fit(self, X, y):\n        self.lr.fit(pd.DataFrame(X).rank(), y)\n        return self\n        \n    def predict_proba(self, X):\n        return np.c_[[0]*len(X), self.lr.predict(pd.DataFrame(X).rank())]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:09:05.169447Z","iopub.execute_input":"2021-08-24T17:09:05.169841Z","iopub.status.idle":"2021-08-24T17:09:05.176650Z","shell.execute_reply.started":"2021-08-24T17:09:05.169809Z","shell.execute_reply":"2021-08-24T17:09:05.175384Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Define models for stacking.","metadata":{}},{"cell_type":"code","source":"# basemodels\nlevel1 = {\n    'lr': make_pipeline(\n        ReviewColumnExtractor(),\n        TfidfVectorizer(max_features=1000),\n        linear_model.LogisticRegression()\n    ), \n    \n    'lr_cnt': make_pipeline(\n        ReviewColumnExtractor(),\n        CountVectorizer(),\n        linear_model.LogisticRegression(solver='liblinear')\n    ), \n    \n    'rf_svd': make_pipeline(\n        ReviewColumnExtractor(),\n        TfidfVectorizer(max_features=None),\n        decomposition.TruncatedSVD(n_components=120),\n        ensemble.RandomForestClassifier(n_estimators=100, n_jobs=-1)\n    )\n}\n\n# metamodels\nlevel2 = {\n    'lr': linear_model.LogisticRegression(),\n    'linreg': make_pipeline(\n        StandardScaler(), \n        LinearRegressionClassifier()\n    ),\n    'xgb': XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False)\n}\n\n# blender head\nlevel3 = {'blender': Blender(rank=True)}","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:09:05.349825Z","iopub.execute_input":"2021-08-24T17:09:05.350193Z","iopub.status.idle":"2021-08-24T17:09:05.358685Z","shell.execute_reply.started":"2021-08-24T17:09:05.350146Z","shell.execute_reply":"2021-08-24T17:09:05.357309Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Same Folds","metadata":{}},{"cell_type":"code","source":"# Run training of stack models\nstack = StackingClassifier([level1, level2, level3])\nstack.fit(df_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:11:03.006594Z","iopub.execute_input":"2021-08-24T17:11:03.006966Z","iopub.status.idle":"2021-08-24T17:15:38.387828Z","shell.execute_reply.started":"2021-08-24T17:11:03.006937Z","shell.execute_reply":"2021-08-24T17:15:38.387084Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"\nLevel 1 preds: lr\nfold=0, auc=0.9384901442591425\nfold=1, auc=0.9335381634668255\nfold=2, auc=0.9280035949592794\nfold=3, auc=0.9304714786729175\nfold=4, auc=0.9414464824536768\nfold=5, auc=0.937182146174952\n\nLevel 1 preds: lr_cnt\nfold=0, auc=0.950827093427299\nfold=1, auc=0.9397791490696997\nfold=2, auc=0.9401545861295928\nfold=3, auc=0.9398243567308318\nfold=4, auc=0.946280093042683\nfold=5, auc=0.9494958987764743\n\nLevel 1 preds: rf_svd\nfold=0, auc=0.8799762710839731\nfold=1, auc=0.876132287447353\nfold=2, auc=0.8681146123716432\nfold=3, auc=0.8732959578283319\nfold=4, auc=0.8908581243113617\nfold=5, auc=0.8755053614765845\n\nLevel 2 preds: lr\nfold=0, auc=0.9532762745385923\nfold=1, auc=0.9452961040531321\nfold=2, auc=0.9431921538861496\nfold=3, auc=0.9449924744888774\nfold=4, auc=0.9543280690762705\nfold=5, auc=0.9518860587205911\n\nLevel 2 preds: linreg\nfold=0, auc=0.9553720768883092\nfold=1, auc=0.9462853486718429\nfold=2, auc=0.9446216398977105\nfold=3, auc=0.9457900346389555\nfold=4, auc=0.9545135063120674\nfold=5, auc=0.9536370903277378\n\nLevel 2 preds: xgb\nfold=0, auc=0.9498351498872931\nfold=1, auc=0.9423812690543844\nfold=2, auc=0.9412324617909553\nfold=3, auc=0.9411240377067716\nfold=4, auc=0.9493684331813829\nfold=5, auc=0.9493009196246607\n\nLevel 3 preds: blender\nfold=0, auc=0.9553652396210282\nfold=1, auc=0.9467632376691684\nfold=2, auc=0.9448596475182754\nfold=3, auc=0.9461609091105494\nfold=4, auc=0.9550762993209037\nfold=5, auc=0.953731069198695\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<__main__.StackingClassifier at 0x7f81cc37dfd0>"},"metadata":{}}]},{"cell_type":"code","source":"same_train_auc = metrics.roc_auc_score(y_train, stack.predict_proba(df_train)[:, 1])\nsame_test_auc = metrics.roc_auc_score(y_test, stack.predict_proba(df_test)[:, 1])\n\nprint(f\"Train AUC (same):\", same_train_auc)\nprint(f\"Test AUC  (same):\", same_test_auc)\n\npd.DataFrame(stack.cv_scores_).describe().loc[['mean', 'std']]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:15:38.389120Z","iopub.execute_input":"2021-08-24T17:15:38.389614Z","iopub.status.idle":"2021-08-24T17:15:59.958908Z","shell.execute_reply.started":"2021-08-24T17:15:38.389583Z","shell.execute_reply":"2021-08-24T17:15:59.957718Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Train AUC (same): 0.9960898580840304\nTest AUC  (same): 0.9472895067497329\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"          lr_1  lr_cnt_1  rf_svd_1      lr_2  linreg_2     xgb_2  blender_3\nmean  0.934855  0.944394  0.877314  0.948829  0.950037  0.945540   0.950326\nstd   0.005098  0.005121  0.007694  0.004865  0.004958  0.004365   0.004888","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lr_1</th>\n      <th>lr_cnt_1</th>\n      <th>rf_svd_1</th>\n      <th>lr_2</th>\n      <th>linreg_2</th>\n      <th>xgb_2</th>\n      <th>blender_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>0.934855</td>\n      <td>0.944394</td>\n      <td>0.877314</td>\n      <td>0.948829</td>\n      <td>0.950037</td>\n      <td>0.945540</td>\n      <td>0.950326</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.005098</td>\n      <td>0.005121</td>\n      <td>0.007694</td>\n      <td>0.004865</td>\n      <td>0.004958</td>\n      <td>0.004365</td>\n      <td>0.004888</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"experiment_results = {\n    'same': {'train': same_train_auc, 'test': same_test_auc}\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:15:59.961494Z","iopub.execute_input":"2021-08-24T17:15:59.961974Z","iopub.status.idle":"2021-08-24T17:15:59.970891Z","shell.execute_reply.started":"2021-08-24T17:15:59.961926Z","shell.execute_reply":"2021-08-24T17:15:59.969603Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Different Folds","metadata":{}},{"cell_type":"markdown","source":"Now we train the same model except the folds are shuffled beyond training the level 1 models simulating the use of different cross-validation folds when training higher level models.","metadata":{}},{"cell_type":"code","source":"class StackingClassifierShuffledCV:\n    \"\"\"Implements model stacking for classification.\"\"\"\n    \n    def __init__(self, model_dict_list):\n        \"\"\"Initialize by passing `model_dict` which is a list of dictionaries \n        of name-model pairs for each level.\"\"\"\n        \n        self.model_dict_list = model_dict_list\n        self.cv_scores_ = {}\n        self.metafeatures_ = None\n        \n    def fit(self, df):\n        \"\"\"Fit classifier. This assumes `df` is a DataFrame with \"id\", \"kfold\", \n        \"sentiment\" (target) columns, followed by features columns.\"\"\"\n        \n        df = df.copy()\n        \n        # Iterating over all stacking levels\n        metafeatures = []\n        for m in range(len(self.model_dict_list)):\n            \n            # Get models in current layer\n            model_dict = self.model_dict_list[m]\n            level = m + 1\n            \n            # Identify feature columns, i.e. preds of prev. layer\n            if m == 0:\n                feature_cols = ['review']\n            else:\n                prev_level_names = self.model_dict_list[m-1].keys()\n                feature_cols = [f'{name}_{level-1}' for name in prev_level_names]\n                \n                # Shuffle folds for level 2 models and up <---------- <!> SHUFFLE FOLDS HERE <!>\n                df['kfold'] = random.sample(df.kfold.tolist(), len(df))\n            \n            # Iterate over models in the current layer\n            for model_name in model_dict.keys():\n                print(f'\\nLevel {level} preds: {model_name}')\n                self.cv_scores_[f'{model_name}_{level}'] = []\n                model = model_dict[model_name]\n                \n                # Generate feature for next layer models from OOF preds\n                oof_preds = []\n                for j in range(df.kfold.nunique()):\n                    oof_pred, oof_auc = self._oof_pred(df, feature_cols, model, \n                                                        model_name, fold=j, level=level)\n                    oof_preds.append(oof_pred)\n                    self.cv_scores_[f'{model_name}_{level}'].append(oof_auc)\n                \n                pred = pd.concat(oof_preds)\n                df = df.merge(pred[['id', f'{model_name}_{level}']], on='id', how='left')   \n                metafeatures.append(f'{model_name}_{level}')\n        \n                # Train models on entire feature columns for inference\n                model.fit(df[feature_cols], df.sentiment.values)\n        \n        self.metafeatures_ = df[metafeatures]\n        return self\n        \n    def predict_proba(self, test_df):\n        \"\"\"Return classification probabilities.\"\"\"\n        \n        test_df = test_df.copy()\n        \n        # Iterate over layers to make predictions\n        for m in range(len(self.model_dict_list)):\n            \n            # Get models for current layer\n            model_dict = self.model_dict_list[m]\n            level = m + 1\n            \n            # Get feature columns to use for prediction\n            if m == 0:\n                feature_cols = ['review']\n            else:\n                prev_names = self.model_dict_list[m-1].keys()\n                feature_cols = [f\"{model_name}_{level-1}\" for model_name in prev_names]\n\n            # Append predictions to test DataFrame\n            for model_name in model_dict.keys():\n                model = model_dict[model_name]\n                pred = model.predict_proba(test_df[feature_cols])[:, 1] \n                test_df.loc[:, f\"{model_name}_{level}\"] = pred\n                    \n        # Return last predictions\n        return np.c_[1 - pred, pred]\n        \n    def _oof_pred(self, df, feature_cols, model, model_name, fold, level):\n        \"Train on K-1 folds, predict on fold K. Return OOF predictions with IDs.\"\n\n        # Get folds; include ID and target cols, and feature cols\n        df_trn = df[df.kfold != fold][['id', 'sentiment']+feature_cols]\n        df_oof = df[df.kfold == fold][['id', 'sentiment']+feature_cols]\n        \n        # Fit model. \n        model.fit(df_trn[feature_cols], df_trn.sentiment.values)\n        oof_pred = model.predict_proba(df_oof[feature_cols])[:, 1] \n        auc = metrics.roc_auc_score(df_oof.sentiment.values, oof_pred)\n        print(f\"fold={fold}, auc={auc}\")\n\n        # Return OOF predictions with ids\n        df_oof.loc[:, f\"{model_name}_{level}\"] = oof_pred\n        return df_oof[[\"id\", f\"{model_name}_{level}\"]], auc","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:15:59.975601Z","iopub.execute_input":"2021-08-24T17:15:59.977651Z","iopub.status.idle":"2021-08-24T17:16:00.007422Z","shell.execute_reply.started":"2021-08-24T17:15:59.977588Z","shell.execute_reply":"2021-08-24T17:16:00.006316Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":":::{danger}\nThe implementation of `StackingClassifier` has a side-effect: models inside the dictionaries are trained. Hence, if we train another model using the same model dictionaries (as we do here for the shuffled version of the stacker), then the models inside the dictionaries will be retrained. This means calling `stack.predict_proba(df_test)` will yield **different results** before and after training `stack_shuffled`! As usual, the stateful approach is error prone. We can modify the `StackingClassifier` to instead save a list of model dictionaries that are *clones* of the stacked models. This allows all state to be localized inside the  stacked model. In fact, [sklearn does exactly this](https://github.com/scikit-learn/scikit-learn/blob/2beed5584/sklearn/model_selection/_search.py#L765).\n:::","metadata":{}},{"cell_type":"code","source":"# Run training of stack models \nstack_shuffled = StackingClassifierShuffledCV([level1, level2, level3])\nstack_shuffled.fit(df_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:16:00.008864Z","iopub.execute_input":"2021-08-24T17:16:00.009186Z","iopub.status.idle":"2021-08-24T17:20:31.837523Z","shell.execute_reply.started":"2021-08-24T17:16:00.009145Z","shell.execute_reply":"2021-08-24T17:20:31.836563Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"\nLevel 1 preds: lr\nfold=0, auc=0.9384901442591425\nfold=1, auc=0.9335381634668255\nfold=2, auc=0.9280035949592794\nfold=3, auc=0.9304714786729175\nfold=4, auc=0.9414464824536768\nfold=5, auc=0.937182146174952\n\nLevel 1 preds: lr_cnt\nfold=0, auc=0.950827093427299\nfold=1, auc=0.9397791490696997\nfold=2, auc=0.9401545861295928\nfold=3, auc=0.9398243567308318\nfold=4, auc=0.946280093042683\nfold=5, auc=0.9494958987764743\n\nLevel 1 preds: rf_svd\nfold=0, auc=0.8818237726745374\nfold=1, auc=0.8812812894942149\nfold=2, auc=0.8704255547449934\nfold=3, auc=0.8757854977279438\nfold=4, auc=0.8840824637587228\nfold=5, auc=0.8688171984934575\n\nLevel 2 preds: lr\nfold=0, auc=0.9455989128995296\nfold=1, auc=0.9517838944259236\nfold=2, auc=0.9461596144163118\nfold=3, auc=0.9439228206431949\nfold=4, auc=0.9574097745579195\nfold=5, auc=0.9484029784852765\n\nLevel 2 preds: linreg\nfold=0, auc=0.947067189488291\nfold=1, auc=0.9530462807823629\nfold=2, auc=0.9461995932959381\nfold=3, auc=0.9452739131123667\nfold=4, auc=0.9590427083295826\nfold=5, auc=0.9499046086722925\n\nLevel 2 preds: xgb\nfold=0, auc=0.9422735183361834\nfold=1, auc=0.9468752339085323\nfold=2, auc=0.9424790363001024\nfold=3, auc=0.9402514285138127\nfold=4, auc=0.9553840004897001\nfold=5, auc=0.944727585558711\n\nLevel 3 preds: blender\nfold=0, auc=0.9408083039393831\nfold=1, auc=0.9528316835106077\nfold=2, auc=0.952974516490283\nfold=3, auc=0.9531538682615063\nfold=4, auc=0.9491569310995549\nfold=5, auc=0.9518523559628773\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"<__main__.StackingClassifierShuffledCV at 0x7f81cc37d890>"},"metadata":{}}]},{"cell_type":"code","source":"shuffled_train_auc = metrics.roc_auc_score(y_train, stack_shuffled.predict_proba(df_train)[:, 1])\nshuffled_test_auc = metrics.roc_auc_score(y_test, stack_shuffled.predict_proba(df_test)[:, 1])\n\nprint(f\"Train AUC (shuffled):\", shuffled_train_auc)\nprint(f\"Test AUC  (shuffled):\", shuffled_test_auc)\n\nexperiment_results['shuffled'] = {'train': shuffled_train_auc, 'test': shuffled_test_auc}","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:20:31.838879Z","iopub.execute_input":"2021-08-24T17:20:31.839155Z","iopub.status.idle":"2021-08-24T17:20:53.246201Z","shell.execute_reply.started":"2021-08-24T17:20:31.839128Z","shell.execute_reply":"2021-08-24T17:20:53.245293Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Train AUC (shuffled): 0.9962149081453051\nTest AUC  (shuffled): 0.9468861435873658\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame(experiment_results)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:20:53.247908Z","iopub.execute_input":"2021-08-24T17:20:53.248705Z","iopub.status.idle":"2021-08-24T17:20:53.265632Z","shell.execute_reply.started":"2021-08-24T17:20:53.248651Z","shell.execute_reply":"2021-08-24T17:20:53.264492Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"          same  shuffled\ntrain  0.99609  0.996215\ntest   0.94729  0.946886","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>same</th>\n      <th>shuffled</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>0.99609</td>\n      <td>0.996215</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>0.94729</td>\n      <td>0.946886</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Observe that the train AUC increased while test score decreased when using different folds between layers. This is a classic symptom of overfitting. Moreover, if we look at CV scores we see significant decrease in performance for `linreg_2`, `xgb_2`, and `blender_3` ($\\sim 2 \\times 10^{-4}$). On the other hand, standard deviation is generally higher with shuffling, which indicates worse fold stability.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(stack_shuffled.cv_scores_).describe().loc[['mean', 'std']]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:20:53.267456Z","iopub.execute_input":"2021-08-24T17:20:53.268227Z","iopub.status.idle":"2021-08-24T17:20:53.308911Z","shell.execute_reply.started":"2021-08-24T17:20:53.268152Z","shell.execute_reply":"2021-08-24T17:20:53.308083Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"          lr_1  lr_cnt_1  rf_svd_1      lr_2  linreg_2     xgb_2  blender_3\nmean  0.934855  0.944394  0.877036  0.948880  0.950089  0.945332   0.950130\nstd   0.005098  0.005121  0.006378  0.004983  0.005223  0.005424   0.004803","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lr_1</th>\n      <th>lr_cnt_1</th>\n      <th>rf_svd_1</th>\n      <th>lr_2</th>\n      <th>linreg_2</th>\n      <th>xgb_2</th>\n      <th>blender_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>0.934855</td>\n      <td>0.944394</td>\n      <td>0.877036</td>\n      <td>0.948880</td>\n      <td>0.950089</td>\n      <td>0.945332</td>\n      <td>0.950130</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.005098</td>\n      <td>0.005121</td>\n      <td>0.006378</td>\n      <td>0.004983</td>\n      <td>0.005223</td>\n      <td>0.005424</td>\n      <td>0.004803</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pd.DataFrame(stack.cv_scores_).describe().loc[['mean', 'std']]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:20:53.310731Z","iopub.execute_input":"2021-08-24T17:20:53.311044Z","iopub.status.idle":"2021-08-24T17:20:53.344464Z","shell.execute_reply.started":"2021-08-24T17:20:53.311011Z","shell.execute_reply":"2021-08-24T17:20:53.343277Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"          lr_1  lr_cnt_1  rf_svd_1      lr_2  linreg_2     xgb_2  blender_3\nmean  0.934855  0.944394  0.877314  0.948829  0.950037  0.945540   0.950326\nstd   0.005098  0.005121  0.007694  0.004865  0.004958  0.004365   0.004888","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lr_1</th>\n      <th>lr_cnt_1</th>\n      <th>rf_svd_1</th>\n      <th>lr_2</th>\n      <th>linreg_2</th>\n      <th>xgb_2</th>\n      <th>blender_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>0.934855</td>\n      <td>0.944394</td>\n      <td>0.877314</td>\n      <td>0.948829</td>\n      <td>0.950037</td>\n      <td>0.945540</td>\n      <td>0.950326</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.005098</td>\n      <td>0.005121</td>\n      <td>0.007694</td>\n      <td>0.004865</td>\n      <td>0.004958</td>\n      <td>0.004365</td>\n      <td>0.004888</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Conclusion\n\nEmpirical results above strongly indicate that we should use the **same folds** across levels of stacking. Indeed, the same is recommended by GM Abishek Thakur in his book [AAAMLP](https://github.com/abhishekkrthakur/approachingalmost/blob/master/AAAMLP.pdf). The following theoretical example shows that overfitting, when using different folds, can happen due to the second stage model taking advantage of a certain relationship between ground truth and first stage predictions, without this structure generalizing well to the test set.\n\nConsider a dataset $\\{(x_1, t_1), (x_2, t_2) \\ldots, (x_{10}, t_{10})\\}$ with five folds such that the first fold is $F_1 = (x_1, x_2)$. Let $x_1 {\\mapsto} y_1$ and $x_2 \\mapsto y_2$ where the mapping is trained on $F_{\\neg 1} = (x_3, \\ldots, x_{10}).$ We can think of modelling on $F_{\\neg 1}$ as defining some rule or distribution that the points in $F_1$ are compared against. Suppose we reshuffle folds in the next level such that the first fold is $G_1 = (y_1, y_{10}).$ Then, the model trained on $G_{\\neg 1} = (y_2, \\ldots y_9)$  overfits slightly since $y_2$ is modelled using the ground truths $(x_3, t_3), \\ldots, (x_{9}, t_9).$ This doesn't happen if we have kept the same cross-validation folds. Overfitting can be observed at the fold level, by noticing that validation scores with shuffled folds are generally lower.\n\nTheoretically (as mentioned in the [Kaggle Guide to Model Stacking](https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)), there is always some leakage if you train a second level model on the same training set, which you used to derive the first stage predictions. This is because you used the ground truth to get those first stage predictions, and now you take those predictions as input, and try to predict the same ground truth. However, this leakage doesn't seem to be significant in practice.","metadata":{}},{"cell_type":"markdown","source":"## TODO\n\n* Parallelize stacker training [using joblib](https://www.youtube.com/watch?v=Ny3O4VpACkc&ab_channel=AbhishekThakur).\n* Modify `StackingClassifier` to keep a dictionary of model clones. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}