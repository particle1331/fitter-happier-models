{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e169dd1",
   "metadata": {
    "papermill": {
     "duration": 0.010551,
     "end_time": "2024-11-24T16:40:42.752868",
     "exception": false,
     "start_time": "2024-11-24T16:40:42.742317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def72d65",
   "metadata": {
    "papermill": {
     "duration": 0.006069,
     "end_time": "2024-11-24T16:40:42.764892",
     "exception": false,
     "start_time": "2024-11-24T16:40:42.758823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Recall that the state vector is initialized as zero. So we use a **warmup context** or a **prompt** to allow the RNN cell to update its state iteratively by processing one character at a time from the warmup text. Then, the algorithm simulates the prediction process of the `RNNLanguageModel`, but instead of using a predefined input sequence, it uses the *previous output* as the next input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345110a1",
   "metadata": {
    "papermill": {
     "duration": 0.002354,
     "end_time": "2024-11-24T16:40:42.771530",
     "exception": false,
     "start_time": "2024-11-24T16:40:42.769176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "```{figure} ../../../img/nn/04-rnn-textgen.png\n",
    "---\n",
    "width: 500px\n",
    "name: 04-rnn-textgen\n",
    "align: center\n",
    "---\n",
    "The RNN cell outputs a final state vector after warmup. The state is used to generate the next character. The sampled character then becomes the next input that updates the state. This process is repeated until the number of predicted tokens is reached.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd26308a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:40:42.776655Z",
     "iopub.status.busy": "2024-11-24T16:40:42.776303Z",
     "iopub.status.idle": "2024-11-24T16:40:44.485430Z",
     "shell.execute_reply": "2024-11-24T16:40:44.485029Z"
    },
    "papermill": {
     "duration": 1.713348,
     "end_time": "2024-11-24T16:40:44.486896",
     "exception": false,
     "start_time": "2024-11-24T16:40:42.773548",
     "status": "completed"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from chapter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c068c",
   "metadata": {
    "papermill": {
     "duration": 0.001888,
     "end_time": "2024-11-24T16:40:44.491167",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.489279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the trained RNN language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9832e1fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:40:44.496134Z",
     "iopub.status.busy": "2024-11-24T16:40:44.495559Z",
     "iopub.status.idle": "2024-11-24T16:40:44.578472Z",
     "shell.execute_reply": "2024-11-24T16:40:44.578056Z"
    },
    "papermill": {
     "duration": 0.086826,
     "end_time": "2024-11-24T16:40:44.579910",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.493084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"  # faster for RNN inference\n",
    "WEIGHTS_PATH = \"./artifacts/rnn_lm.pkl\"\n",
    "_, vocab = TimeMachine().build()\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "model = RNNLanguageModel(VOCAB_SIZE, 64, VOCAB_SIZE)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=DEVICE));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432733cf",
   "metadata": {
    "papermill": {
     "duration": 0.001794,
     "end_time": "2024-11-24T16:40:44.583885",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.582091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Text generation utils and algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d130f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:40:44.588123Z",
     "iopub.status.busy": "2024-11-24T16:40:44.587919Z",
     "iopub.status.idle": "2024-11-24T16:40:44.592581Z",
     "shell.execute_reply": "2024-11-24T16:40:44.592221Z"
    },
    "papermill": {
     "duration": 0.008204,
     "end_time": "2024-11-24T16:40:44.593771",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.585567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inp(indices: list[int]):\n",
    "    \"\"\"Preprocess indices (T,) to (1, T, V) mini-batch shape with bs=1.\"\"\"\n",
    "    n = VOCAB_SIZE\n",
    "    return F.one_hot(torch.tensor(indices), n).float().view(1, -1, n).to(DEVICE)\n",
    "\n",
    "\n",
    "def get_next_idx(model, state, temp=1.0):\n",
    "    \"\"\"Sample next token from RNN cell state with softmax temperature.\"\"\"\n",
    "    s = model.linear(state)\n",
    "    p = F.softmax(s / temp)   # higher temp => more uniform, i.e. exp ~ 1\n",
    "    return torch.multinomial(p, num_samples=1).item()\n",
    "\n",
    "\n",
    "def predict(model, vocab, warmup: str, num_preds: int, temp=1.0):\n",
    "    \"\"\"Simulate RNN character generation one at a time.\"\"\"\n",
    "\n",
    "    # Iterate over warmup text. RNN cell outputs final state\n",
    "    warmup_indices = vocab[list(warmup.lower())]\n",
    "    state = model.rnn(inp(warmup_indices))[1]       # out, state = model.rnn(...)\n",
    "\n",
    "    # Next token sampling and state update\n",
    "    indices = []\n",
    "    for _ in range(num_preds):\n",
    "        i = get_next_idx(model, state, temp)\n",
    "        indices.append(i)\n",
    "        state = model.rnn(inp([i]), state)[1]\n",
    "    \n",
    "    return \"\".join(vocab.to_tokens(warmup_indices + indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba1dfa",
   "metadata": {
    "papermill": {
     "duration": 0.001708,
     "end_time": "2024-11-24T16:40:44.597500",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.595792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Sanity test.** Completing 'thank you':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a7b406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:40:44.601645Z",
     "iopub.status.busy": "2024-11-24T16:40:44.601447Z",
     "iopub.status.idle": "2024-11-24T16:40:44.614219Z",
     "shell.execute_reply": "2024-11-24T16:40:44.613826Z"
    },
    "papermill": {
     "duration": 0.016263,
     "end_time": "2024-11-24T16:40:44.615419",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.599156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = []\n",
    "for i in range(10):\n",
    "    s.append(predict(model, vocab, \"thank y\", num_preds=2))\n",
    "\n",
    "(np.array(s) == \"thank you\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e112370",
   "metadata": {
    "papermill": {
     "duration": 0.001954,
     "end_time": "2024-11-24T16:40:44.619453",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.617499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Example.** The network can generate output given warmup prompt of arbitrary length. Here we also look at the effect of temperature on the generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3d06dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:40:44.624223Z",
     "iopub.status.busy": "2024-11-24T16:40:44.624014Z",
     "iopub.status.idle": "2024-11-24T16:40:44.671377Z",
     "shell.execute_reply": "2024-11-24T16:40:44.670946Z"
    },
    "papermill": {
     "duration": 0.051339,
     "end_time": "2024-11-24T16:40:44.672760",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.621421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warmup = \"mr williams i underst\"\n",
    "text = []\n",
    "temp = []\n",
    "for i in range(1, 6):\n",
    "    t = 0.20 * i\n",
    "    s = predict(model, vocab, warmup, num_preds=100, temp=t)\n",
    "    text.append(s)\n",
    "    temp.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e2cc94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:40:44.678676Z",
     "iopub.status.busy": "2024-11-24T16:40:44.678482Z",
     "iopub.status.idle": "2024-11-24T16:40:44.779172Z",
     "shell.execute_reply": "2024-11-24T16:40:44.778772Z"
    },
    "papermill": {
     "duration": 0.105202,
     "end_time": "2024-11-24T16:40:44.780387",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.675185",
     "status": "completed"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_56de5_row0_col0, #T_56de5_row0_col1, #T_56de5_row1_col0, #T_56de5_row1_col1, #T_56de5_row2_col0, #T_56de5_row2_col1, #T_56de5_row3_col0, #T_56de5_row3_col1, #T_56de5_row4_col0, #T_56de5_row4_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_56de5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_56de5_level0_col0\" class=\"col_heading level0 col0\" >temp</th>\n",
       "      <th id=\"T_56de5_level0_col1\" class=\"col_heading level0 col1\" >text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_56de5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_56de5_row0_col0\" class=\"data row0 col0\" >0.2</td>\n",
       "      <td id=\"T_56de5_row0_col1\" class=\"data row0 col1\" >mr williams i understand the time traveller in the time traveller in the time traveller and the start the time traveller </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_56de5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_56de5_row1_col0\" class=\"data row1 col0\" >0.4</td>\n",
       "      <td id=\"T_56de5_row1_col1\" class=\"data row1 col1\" >mr williams i understand of the sun night the machine the time traveller the parent shall and in the time traveller and i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_56de5_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_56de5_row2_col0\" class=\"data row2 col0\" >0.6</td>\n",
       "      <td id=\"T_56de5_row2_col1\" class=\"data row2 col1\" >mr williams i understand white palain said the matches i had how the flanked fire the great possibly the time travellen i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_56de5_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_56de5_row3_col0\" class=\"data row3 col0\" >0.8</td>\n",
       "      <td id=\"T_56de5_row3_col1\" class=\"data row3 col1\" >mr williams i understand the brignt came it was to my for the travelling in the been remailation well our stist though re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_56de5_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_56de5_row4_col0\" class=\"data row4 col0\" >1.0</td>\n",
       "      <td id=\"T_56de5_row4_col1\" class=\"data row4 col1\" >mr williams i understond the leveriout and feeling the palance of she was but dinned me and buphed of very towre filpts m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x107237490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df = pd.DataFrame({\"temp\": [f\"{t:.1f}\" for t in temp], \"text\": text})\n",
    "df = df.style.set_properties(**{\"text-align\": \"left\"})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3ad44",
   "metadata": {
    "papermill": {
     "duration": 0.001889,
     "end_time": "2024-11-24T16:40:44.784987",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.783098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'Time traveller' mentioned! Σ(°ロ°) Here we can see that the higher the temperature, the text looks more random. On the other hand, with lower temp, the softmax becomes more like argmax. The sampling algorithm gets the largest probability token which makes it prone to cycles. \n",
    "\n",
    "**Remark.** It would be nice if text generation does some backtracking, e.g. looking at the probability of the text when we add a new character, as well as characters that will follow the added character. We will see in future chapters how this can be done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc682eec",
   "metadata": {
    "papermill": {
     "duration": 0.002028,
     "end_time": "2024-11-24T16:40:44.788798",
     "exception": false,
     "start_time": "2024-11-24T16:40:44.786770",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.420794,
   "end_time": "2024-11-24T16:40:45.210949",
   "environment_variables": {},
   "exception": null,
   "input_path": "04dc-decoding.ipynb",
   "output_path": "04dc-decoding.ipynb",
   "parameters": {},
   "start_time": "2024-11-24T16:40:41.790155",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
