{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e169dd1",
   "metadata": {
    "papermill": {
     "duration": 0.006776,
     "end_time": "2024-11-21T16:17:53.855954",
     "exception": false,
     "start_time": "2024-11-21T16:17:53.849178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def72d65",
   "metadata": {
    "papermill": {
     "duration": 0.004585,
     "end_time": "2024-11-21T16:17:53.865476",
     "exception": false,
     "start_time": "2024-11-21T16:17:53.860891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Recall that the state vector is initialized as zero. So we use a **warmup context** or a **prompt** to allow the RNN cell to update its state iteratively by processing one character at a time from the warmup text. Then, the algorithm simulates the prediction process of the `RNNLanguageModel`, but instead of using a predefined input sequence, it uses the *previous output* as the next input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345110a1",
   "metadata": {
    "papermill": {
     "duration": 0.001921,
     "end_time": "2024-11-21T16:17:53.870796",
     "exception": false,
     "start_time": "2024-11-21T16:17:53.868875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "```{figure} ../../../img/nn/04-rnn-textgen.png\n",
    "---\n",
    "width: 500px\n",
    "name: 04-rnn-textgen\n",
    "align: center\n",
    "---\n",
    "The RNN cell outputs a final state vector after warmup. The state is used to generate the next character. The sampled character then becomes the next input that updates the state. This process is repeated until the number of predicted tokens is reached.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd26308a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:17:53.873498Z",
     "iopub.status.busy": "2024-11-21T16:17:53.873347Z",
     "iopub.status.idle": "2024-11-21T16:17:55.481039Z",
     "shell.execute_reply": "2024-11-21T16:17:55.480726Z"
    },
    "papermill": {
     "duration": 1.610468,
     "end_time": "2024-11-21T16:17:55.482260",
     "exception": false,
     "start_time": "2024-11-21T16:17:53.871792",
     "status": "completed"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from chapter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c068c",
   "metadata": {
    "papermill": {
     "duration": 0.001104,
     "end_time": "2024-11-21T16:17:55.484997",
     "exception": false,
     "start_time": "2024-11-21T16:17:55.483893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the trained RNN language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9832e1fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:17:55.487859Z",
     "iopub.status.busy": "2024-11-21T16:17:55.487676Z",
     "iopub.status.idle": "2024-11-21T16:17:55.557153Z",
     "shell.execute_reply": "2024-11-21T16:17:55.556794Z"
    },
    "papermill": {
     "duration": 0.07265,
     "end_time": "2024-11-21T16:17:55.558709",
     "exception": false,
     "start_time": "2024-11-21T16:17:55.486059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"  # faster for RNN inference\n",
    "WEIGHTS_PATH = \"./artifacts/rnn_lm.pkl\"\n",
    "_, vocab = TimeMachine().build()\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "model = RNNLanguageModel(VOCAB_SIZE, 64, VOCAB_SIZE)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=DEVICE));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432733cf",
   "metadata": {
    "papermill": {
     "duration": 0.001134,
     "end_time": "2024-11-21T16:17:55.561395",
     "exception": false,
     "start_time": "2024-11-21T16:17:55.560261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Text generation utils and algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d130f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:17:55.564174Z",
     "iopub.status.busy": "2024-11-21T16:17:55.564044Z",
     "iopub.status.idle": "2024-11-21T16:17:55.567521Z",
     "shell.execute_reply": "2024-11-21T16:17:55.567220Z"
    },
    "papermill": {
     "duration": 0.005976,
     "end_time": "2024-11-21T16:17:55.568451",
     "exception": false,
     "start_time": "2024-11-21T16:17:55.562475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inp(indices: list[int]):\n",
    "    \"\"\"Preprocess indices (T,) to (1, T, V) mini-batch shape with bs=1.\"\"\"\n",
    "    n = VOCAB_SIZE\n",
    "    return F.one_hot(torch.tensor(indices), n).float().view(1, -1, n).to(DEVICE)\n",
    "\n",
    "\n",
    "def get_next_idx(model, state, temp=1.0):\n",
    "    \"\"\"Sample next token from RNN cell state with softmax temperature.\"\"\"\n",
    "    s = model.linear(state)\n",
    "    p = F.softmax(s / temp)   # higher temp => more uniform, i.e. exp ~ 1\n",
    "    return torch.multinomial(p, num_samples=1).item()\n",
    "\n",
    "\n",
    "def predict(model, vocab, warmup: str, num_preds: int, temp=1.0):\n",
    "    \"\"\"Simulate RNN character generation one at a time.\"\"\"\n",
    "\n",
    "    # Iterate over warmup text. RNN cell outputs final state\n",
    "    warmup_indices = vocab[list(warmup.lower())]\n",
    "    state = model.rnn(inp(warmup_indices))[1]       # out, state = model.rnn(...)\n",
    "\n",
    "    # Next token sampling and state update\n",
    "    indices = []\n",
    "    for _ in range(num_preds):\n",
    "        i = get_next_idx(model, state, temp)\n",
    "        indices.append(i)\n",
    "        state = model.rnn(inp([i]), state)[1]\n",
    "    \n",
    "    return \"\".join(vocab.to_tokens(warmup_indices + indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba1dfa",
   "metadata": {},
   "source": [
    "**Sanity test.** Completing 'thank you':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a7b406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = []\n",
    "for i in range(10):\n",
    "    s.append(predict(model, vocab, \"thank y\", num_preds=2))\n",
    "\n",
    "(np.array(s) == \"thank you\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e112370",
   "metadata": {
    "papermill": {
     "duration": 0.00115,
     "end_time": "2024-11-21T16:17:55.570968",
     "exception": false,
     "start_time": "2024-11-21T16:17:55.569818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Example.** The network can generate output given warmup prompt of arbitrary length. Here we also look at the effect of temperature on the generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3d06dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:17:55.573619Z",
     "iopub.status.busy": "2024-11-21T16:17:55.573500Z",
     "iopub.status.idle": "2024-11-21T16:17:55.833396Z",
     "shell.execute_reply": "2024-11-21T16:17:55.805303Z"
    },
    "papermill": {
     "duration": 0.265571,
     "end_time": "2024-11-21T16:17:55.837627",
     "exception": false,
     "start_time": "2024-11-21T16:17:55.572056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warmup = \"mr williams i underst\"\n",
    "text = []\n",
    "temp = []\n",
    "for i in range(1, 6):\n",
    "    t = 0.20 * i\n",
    "    s = predict(model, vocab, warmup, num_preds=100, temp=t)\n",
    "    text.append(s)\n",
    "    temp.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e2cc94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:17:55.880187Z",
     "iopub.status.busy": "2024-11-21T16:17:55.879685Z",
     "iopub.status.idle": "2024-11-21T16:17:56.026678Z",
     "shell.execute_reply": "2024-11-21T16:17:56.024940Z"
    },
    "papermill": {
     "duration": 0.159253,
     "end_time": "2024-11-21T16:17:56.030171",
     "exception": false,
     "start_time": "2024-11-21T16:17:55.870918",
     "status": "completed"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6c1f3_row0_col0, #T_6c1f3_row0_col1, #T_6c1f3_row1_col0, #T_6c1f3_row1_col1, #T_6c1f3_row2_col0, #T_6c1f3_row2_col1, #T_6c1f3_row3_col0, #T_6c1f3_row3_col1, #T_6c1f3_row4_col0, #T_6c1f3_row4_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6c1f3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6c1f3_level0_col0\" class=\"col_heading level0 col0\" >temp</th>\n",
       "      <th id=\"T_6c1f3_level0_col1\" class=\"col_heading level0 col1\" >text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6c1f3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6c1f3_row0_col0\" class=\"data row0 col0\" >0.2</td>\n",
       "      <td id=\"T_6c1f3_row0_col1\" class=\"data row0 col1\" >mr williams i understand were to the sun had to the laboratory i saw the said the said the sincession and the said the su</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c1f3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6c1f3_row1_col0\" class=\"data row1 col0\" >0.4</td>\n",
       "      <td id=\"T_6c1f3_row1_col1\" class=\"data row1 col1\" >mr williams i understand of the larged of the time traveller storith the said the probles for the time to a been upon the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c1f3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6c1f3_row2_col0\" class=\"data row2 col0\" >0.6</td>\n",
       "      <td id=\"T_6c1f3_row2_col1\" class=\"data row2 col1\" >mr williams i understand been caloul of the own an and in said the beast of the but to me was to fire to felt for the sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c1f3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6c1f3_row3_col0\" class=\"data row3 col0\" >0.8</td>\n",
       "      <td id=\"T_6c1f3_row3_col1\" class=\"data row3 col1\" >mr williams i understance and her of moding and for the came for a storimp note down a slishing fight the exactle that my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c1f3_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6c1f3_row4_col0\" class=\"data row4 col0\" >1.0</td>\n",
       "      <td id=\"T_6c1f3_row4_col1\" class=\"data row4 col1\" >mr williams i understy tome is theut go my along sected utfvery weants the palaned the he was by countione back bupped of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x103e1f940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df = pd.DataFrame({\"temp\": [f\"{t:.1f}\" for t in temp], \"text\": text})\n",
    "df = df.style.set_properties(**{\"text-align\": \"left\"})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3ad44",
   "metadata": {
    "papermill": {
     "duration": 0.001993,
     "end_time": "2024-11-21T16:17:56.033967",
     "exception": false,
     "start_time": "2024-11-21T16:17:56.031974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'Time traveller' mentioned! Σ(°ロ°) Here we can see that the higher the temperature, the text looks more random. On the other hand, with lower temp, the softmax becomes more like argmax. The sampling algorithm gets the largest probability token which makes it prone to cycles. \n",
    "\n",
    "**Remark.** It would be nice if text generation does some backtracking, i.e. looking at the probability of the text when we add a new character, as well as characters that will follow the added character. We will see in future chapters how this can be done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc682eec",
   "metadata": {
    "papermill": {
     "duration": 0.001733,
     "end_time": "2024-11-21T16:17:56.037764",
     "exception": false,
     "start_time": "2024-11-21T16:17:56.036031",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.817136,
   "end_time": "2024-11-21T16:17:56.561047",
   "environment_variables": {},
   "exception": null,
   "input_path": "04dc-decoding.ipynb",
   "output_path": "04dc-decoding.ipynb",
   "parameters": {},
   "start_time": "2024-11-21T16:17:52.743911",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
