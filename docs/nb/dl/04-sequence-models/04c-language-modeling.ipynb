{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746aa897",
   "metadata": {
    "papermill": {
     "duration": 0.004365,
     "end_time": "2024-11-19T07:02:27.146834",
     "exception": false,
     "start_time": "2024-11-19T07:02:27.142469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691565a8",
   "metadata": {
    "papermill": {
     "duration": 0.002629,
     "end_time": "2024-11-19T07:02:27.152072",
     "exception": false,
     "start_time": "2024-11-19T07:02:27.149443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Estimating the joint probability $p(\\boldsymbol{\\mathsf x}_1, \\ldots,\\boldsymbol{\\mathsf x}_T)$ of sequences composed of discrete tokens prove useful for all sorts of reasons. This task is called *sequence* or *language modeling.*\n",
    "For example, we want to evaluate the likelihood of sentences. We might wish to compare the naturalness to resolve ambiguity in candidate outputs generated by a machine translation or ASR system. In this case, we can reject outputs with low probability of occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff280b",
   "metadata": {
    "papermill": {
     "duration": 0.002324,
     "end_time": "2024-11-19T07:02:27.156965",
     "exception": false,
     "start_time": "2024-11-19T07:02:27.154641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Language modeling gives us not only the capacity to evaluate likelihood, but the ability to generate sequences, and even to optimize for the most likely sequences. Recall that we can write a joint distribution as a chain of conditional distributions:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\mathsf x}_1, \\ldots,\\boldsymbol{\\mathsf x}_T) = p(\\boldsymbol{\\mathsf x}_1) \\prod_{t = 2}^{T} p(\\boldsymbol{\\mathsf x}_{t} \\mid \\boldsymbol{\\mathsf x}_{1}, \\ldots, \\boldsymbol{\\mathsf x}_{t-1}).\n",
    "$$\n",
    "\n",
    "Hence, the output of a model for discrete data must be a distribution $p(\\boldsymbol{\\mathsf x}_{t} \\mid \\boldsymbol{\\mathsf x}_{1}, \\ldots, \\boldsymbol{\\mathsf x}_{t-1})$ for each token instead of expected values for regression models. In practice, this means that we need to have a finite collection of valid tokens called a **vocabulary**. Then, we can generate text, simply by drawing one token at a time $\\boldsymbol{\\mathsf{x}}_t \\sim p(\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_{t-1})$. For example,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\;p(\\text{deep}, \\text{learning}, \\text{is}, \\text{fun}) \\\\\n",
    "=& \\;p(\\text{deep}) \\cdot p(\\text{learning} \\mid \\text{deep}) \\cdot p(\\text{is} \\mid \\text{deep}, \\text{learning}) \\cdot p(\\text{fun} \\mid \\text{deep}, \\text{learning}, \\text{is}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The probabilities can be estimated using [relative frequencies](https://en.wikipedia.org/wiki/Empirical_probability) perhaps with [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing):\n",
    "\n",
    "$$\n",
    "p(\\text{deep} \\mid \\text{learning}) \\approx \\frac{\\#(\\text{deep},\\, \\text{learning}) + \\kappa}{\\#(\\text{learning}) + \\kappa|\\mathcal{V}|}\n",
    "$$ \n",
    "\n",
    "where $\\kappa > 0$ can be thought of as *pseudo-count*. Observe that the smoothing parameter $\\kappa$ acts as a regularizer when $\\kappa \\gg 1,$ where the distribution becomes uniform. Moreover, we usually truncate the context to a fixed number of terms as a Markov hypothesis, and because *n*-grams become sparse in naturally occuring text as *n* increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39338bf6",
   "metadata": {
    "papermill": {
     "duration": 0.002517,
     "end_time": "2024-11-19T07:02:27.162180",
     "exception": false,
     "start_time": "2024-11-19T07:02:27.159663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd552065",
   "metadata": {
    "papermill": {
     "duration": 0.002122,
     "end_time": "2024-11-19T07:02:27.166701",
     "exception": false,
     "start_time": "2024-11-19T07:02:27.164579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we need a generic metric to measure the quality of the language model.\n",
    "One way is to check how *surprising* the text is.\n",
    "A good language model is able to predict, with high accuracy, the tokens that come next.\n",
    "Consider the following continuations of the phrase \"It is sunny\", as proposed by three different language models:\n",
    "\n",
    "```text\n",
    "1. It is sunny outside\n",
    "2. It is sunny banana tree\n",
    "3. It is sunny soiupt;mkj ldfosim\n",
    "```\n",
    "\n",
    "The first example is clearly the best, although not necessarily factual or accurate, model predicts kind of word correctly. The next is nonsensical, but at least model has learned some degree of correlation between words ('banana' and 'tree'). Finally, the last example \n",
    "indicates poor training.\n",
    "\n",
    "To evaluate a language model, we can use the cross-entropy on the next token which is equivalent to maximizing the likelihood of a text. We normalize this over the number of tokens predicted. For example, we evaluate the model on contexts of variable length $\\delta = 1, \\ldots, T$ starting from $\\boldsymbol{\\mathsf{x}}_{t}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{n}\\sum_{t}\\sum_{\\delta = 1}^{T} \\log p(\\boldsymbol{\\mathsf{x}}_{t + \\delta} \\mid \\boldsymbol{\\mathsf{x}}_{t}, \\ldots, \\boldsymbol{\\mathsf{x}}_{t + \\delta - 1})\n",
    "$$\n",
    "\n",
    "where $n$ is the number predictions. For a classifier that predicts all tokens uniformly random, then $\\mathcal{L} = \\log |\\mathcal{V}|$ where $\\mathcal{V}$ is the set of tokens. This is a useful baseline. A similarly simple model predicts prior probabilities based on counts of each token in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5086e6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T07:02:27.170387Z",
     "iopub.status.busy": "2024-11-19T07:02:27.170187Z",
     "iopub.status.idle": "2024-11-19T07:02:27.998897Z",
     "shell.execute_reply": "2024-11-19T07:02:27.998554Z"
    },
    "papermill": {
     "duration": 0.831805,
     "end_time": "2024-11-19T07:02:28.000091",
     "exception": false,
     "start_time": "2024-11-19T07:02:27.168286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3714)\n",
      "3.332204510175204\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Reduction over B Ã— T elements\n",
    "B, C, T = 32, 28, 128\n",
    "print(F.cross_entropy(torch.rand(B, C, T), target=torch.randint(C, size=(B, T))))\n",
    "print(math.log(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e998124",
   "metadata": {
    "papermill": {
     "duration": 0.000856,
     "end_time": "2024-11-19T07:02:28.002125",
     "exception": false,
     "start_time": "2024-11-19T07:02:28.001269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Historically, researchers in NLP have also used *perplexity* (PP) which is simply the exponential of the cross-entropy:\n",
    "\n",
    "$$\n",
    "\\text{PP} = \\exp\\left(-\\frac{1}{n}\\sum_{t}\\sum_{\\delta = 1}^{T} \\log p(\\boldsymbol{\\mathsf{x}}_{t + \\delta} \\mid \\boldsymbol{\\mathsf{x}}_{t}, \\ldots, \\boldsymbol{\\mathsf{x}}_{t + \\delta - 1})\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c85280",
   "metadata": {
    "papermill": {
     "duration": 0.00087,
     "end_time": "2024-11-19T07:02:28.003878",
     "exception": false,
     "start_time": "2024-11-19T07:02:28.003008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that perplexity is equivalent to an inverse likelihood, and to the geometric mean of $\\frac{1}{p(\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_{<t})}$: \n",
    "\n",
    "$$\n",
    "\\text{PP} = \\frac{1}{\\sqrt[n]{\\prod_{t}\\prod_{\\delta = 1}^{T} p(\\boldsymbol{\\mathsf{x}}_{t + \\delta} \\mid \\boldsymbol{\\mathsf{x}}_{[t:\\,t + \\delta-1]})}} = \\sqrt[n]{\\prod_{t}\\prod_{\\delta = 1}^{T} \\frac{1} {p(\\boldsymbol{\\mathsf{x}}_{t + \\delta} \\mid \\boldsymbol{\\mathsf{x}}_{[t:\\,t + \\delta-1]})}}.\n",
    "$$\n",
    "\n",
    "Hence, for a perfect model, $\\text{PP} = 1.$ On the other hand, if the model predicts $p \\approx 0$ for the correct token at one step, then[^1] we get $\\text{PP} = \\infty.$ As a baseline, for a uniformly random model, we have $\\text{PP} = |\\mathcal{V}|.$ This provides a nontrivial upper bound that any useful model must beat. So, we have $\\text{PP}$ values $\\infty > |\\mathcal{V}| \\geq 1$ for the three regimes[^2]. This can be interpreted as the average number of tries to get the correct prediction at each step, e.g. single try for a perfect model.\n",
    "\n",
    "**Remark.** For the sake of concreteness, we evaluated cross-entropy over predictions with context of varying length $\\delta = 1, \\ldots, T$ from $t.$ But we can also use fixed-length contexts, depending on the given task. In general, we simply evaluate cross-entropy over all instances of next-token prediction regardless of the particulars of the prediction process.\n",
    "\n",
    "[^1]: More precisely, for any $\\epsilon > 0$, if $p_{t + \\delta} \\leq \\epsilon$ for some $(t, \\delta)$, then $\\text{PP} \\geq \\epsilon^{-1/n}.$ \n",
    "\n",
    "[^2]: The regimes correspond to $\\infty > \\log |\\mathcal{V}| \\geq 0$ with cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6657c59",
   "metadata": {
    "papermill": {
     "duration": 0.000831,
     "end_time": "2024-11-19T07:02:28.005543",
     "exception": false,
     "start_time": "2024-11-19T07:02:28.004712",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.036428,
   "end_time": "2024-11-19T07:02:28.323642",
   "environment_variables": {},
   "exception": null,
   "input_path": "04c-language-modeling.ipynb",
   "output_path": "04c-language-modeling.ipynb",
   "parameters": {},
   "start_time": "2024-11-19T07:02:26.287214",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}