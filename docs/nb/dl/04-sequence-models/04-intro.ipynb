{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e508fe60",
   "metadata": {
    "papermill": {
     "duration": 0.02802,
     "end_time": "2024-11-19T07:02:10.128736",
     "exception": false,
     "start_time": "2024-11-19T07:02:10.100716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Modeling and RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e9fee",
   "metadata": {
    "papermill": {
     "duration": 0.003122,
     "end_time": "2024-11-19T07:02:10.138038",
     "exception": false,
     "start_time": "2024-11-19T07:02:10.134916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this chapter, we will deal with variable-length sequence data. \n",
    "This is fundamentally different from fixed shape data that we have previously encountered. But variable-length data is abundant in the real-world. Tasks such as translating passages of text from one language to another, \n",
    "engaging in dialogue, or controlling a robot, demand that models both ingest and output sequentially structured data. Here we focus on text data which is our primary interest. In particular, we sample character sequences as training data from a dataset of Spanish names. To increase complexity, we proceed with *The Time Machine* (1895) by [H. G. Wells](https://en.wikipedia.org/wiki/H._G._Wells).\n",
    "\n",
    "For sequence modeling, we explore two approaches. The first approach uses fixed-length windows, or **contexts**, to predict the probability distribution of the next token. This allows us to use familiar models such as CNNs and MLPs. The second approach introduces **Recurrent Neural Networks** (RNNs) which can process sequences of arbitrary length. RNNs are neural networks that capture the dynamics of sequences via *recurrent connections* ({numref}`04-rnn`), which can be thought of as cycles that iteratively update a hidden state vector in the network. \n",
    "The resulting hidden representation depend on the specific input order. Hence, RNNs inherit causality from the structure of the text.\n",
    "\n",
    "To understand the challenges of training RNNs, we derive the **BPTT equations** (**B**ack**p**ropagation **T**hrough **T**ime). We will see that RNNs accumulate gradients with depth corresponding to time steps, instead of number of layers for MLPs[^1]. In particular, we will see that RNNs struggle to model long-term dependencies (i.e., tokens that are spaced far apart but share a significant relationship) which manifest as vanishing gradient. This had motivated the development of more advanced RNN architectures (e.g., LSTM {cite}`lstm` and GRU {cite}`gru`) that aim to minimize or address vanishing gradients.\n",
    "\n",
    "[^1]: Both can be formulated in terms of the path length in the computation graph between two nodes that share a dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32be725",
   "metadata": {
    "papermill": {
     "duration": 0.002742,
     "end_time": "2024-11-19T07:02:10.147894",
     "exception": false,
     "start_time": "2024-11-19T07:02:10.145152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "```{figure} ../../../img/nn/04-rnn.svg\n",
    "---\n",
    "width: 600px\n",
    "name: 04-rnn\n",
    "align: center\n",
    "---\n",
    "RNN unit (a) cyclic, and (b) unrolled RNN (essentially a deep MLP with shared weights).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c41b54",
   "metadata": {
    "papermill": {
     "duration": 0.00449,
     "end_time": "2024-11-19T07:02:10.155610",
     "exception": false,
     "start_time": "2024-11-19T07:02:10.151120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sequential data\n",
    "\n",
    "We consider inputs of the form\n",
    "$\\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_T$ where $\\boldsymbol{\\mathsf{x}}_t \\in \\mathbb{R}^d$ for $t = 1, \\ldots, T.$ \n",
    "For example collection of words in a document, or sequence of events that occur for an RL agent.\n",
    "In each of these, the entities are represented using a state vector in $\\mathbb{R}^d.$ Note that $T$ is usually a maximum length, and the model may process variable-length inputs of length $\\tau$ where  $1 \\leq \\tau \\leq T.$ In terms of targets, we can have:\n",
    "\n",
    "\n",
    "|Task|Mapping|Example|\n",
    "|------|------|-----|\n",
    "| Fixed target |$(\\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_T) \\mapsto \\boldsymbol{\\mathsf{y}}$ | Sentiment Analysis [[1]](https://www.nvidia.com/en-us/glossary/sentiment-analysis/) |\n",
    "| Fixed input | $\\boldsymbol{\\mathsf{x}} \\mapsto (\\boldsymbol{\\mathsf{y}}_1, \\ldots, \\boldsymbol{\\mathsf{y}}_T)$ | Image Captioning [[3]](https://cs.stanford.edu/people/karpathy/deepimagesent/) |\n",
    "| Sequence-to-Sequence | $(\\boldsymbol{\\mathsf{y}}_1, \\ldots, \\boldsymbol{\\mathsf{y}}_T) \\mapsto (\\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_T)$  | Video Captioning, Machine Translation |\n",
    "\n",
    "Sequence-to-sequence tasks take two forms: \n",
    "\n",
    "|Type|Constraint|Example|\n",
    "|------|------|-----|\n",
    "| Aligned | Corresponding target aligns with input at each time step | Speech recognition ([STT](https://www.nvidia.com/en-us/glossary/speech-to-text/)) |\n",
    "| Unaligned | No step-for-step correspondence required | Machine Translation [[2](https://research.google/research-areas/machine-translation/)] | \n",
    "\n",
    "\n",
    "To construct training data from historical data, we typically create examples by sampling windows randomly. We will often assume that the underlying data generation process does not change, i.e. is *stationary*. In practice, this means that the weights are independent of time steps.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a6335",
   "metadata": {
    "papermill": {
     "duration": 0.00572,
     "end_time": "2024-11-19T07:02:10.165374",
     "exception": false,
     "start_time": "2024-11-19T07:02:10.159654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Autoregressive modeling\n",
    "\n",
    "The goal of autoregressive modeling is to characterize \n",
    "$p(\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_{t-1}).$\n",
    "This is autoregressive in the sense that previous elements of the same sequence are used to estimate the next element. Note that the \n",
    "entire distribution is generally hard to compute, and we may be content with $\\mathbb{E}\\left[\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_{t-1}\\right]$, i.e. estimating the average value of the next element.\n",
    "One issue is that the length of sequences increase with the amount of data that we encounter.\n",
    "Much of sequence modeling literature revolve around techniques for dealing with \n",
    "increasing context size to predict the next token or certain statistics of the distribution $p(\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_{t-1}).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53965d86",
   "metadata": {
    "papermill": {
     "duration": 0.004716,
     "end_time": "2024-11-19T07:02:10.173312",
     "exception": false,
     "start_time": "2024-11-19T07:02:10.168596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A natural strategy is to just ignore more data, i.e. only use past $\\tau$ observations, so that we estimate $p(\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_{t-\\tau}, \\ldots, \\boldsymbol{\\mathsf{x}}_{t-1}).$ This is a [Markovian assumption](https://en.wikipedia.org/wiki/Markov_model) where we assume that the past $\\tau$ elements are sufficient to approximate the next element. This makes sense especially for phenomenon where long-range dependency is rare, or that the importance of long-range dependency decays quickly with time.\n",
    "In this case, all inputs are of length $\\tau$\n",
    "which allows us to train any linear model or deep network that requires fixed-length vectors as inputs.\n",
    "\n",
    "Next, we develop models that maintain some summary $\\boldsymbol{\\mathsf{h}}_t$ of the past observations used to predict the next output, and also updates with each observation, i.e. $\\boldsymbol{\\mathsf{y}}_t = g(\\boldsymbol{\\mathsf{h}}_{t})$ and $\\boldsymbol{\\mathsf{h}}_t = f(\\boldsymbol{\\mathsf{x}}_{t-1}, \\boldsymbol{\\mathsf{h}}_{t-1}).$\n",
    "Since the state $\\boldsymbol{\\mathsf{h}}_t$ is never observed, these models are called **latent autoregressive models**. RNNs are example of such models[^2].\n",
    "\n",
    "[^2]: A [future chapter](dl/07-attention) covers stateless models that simply look at interaction between pairs of tokens in a context. Since iterative processing of inputs are not necessary, such models are highly-parallelizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac55cea7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T07:02:10.177450Z",
     "iopub.status.busy": "2024-11-19T07:02:10.177119Z",
     "iopub.status.idle": "2024-11-19T07:02:10.321595Z",
     "shell.execute_reply": "2024-11-19T07:02:10.321135Z"
    },
    "papermill": {
     "duration": 0.148052,
     "end_time": "2024-11-19T07:02:10.322837",
     "exception": false,
     "start_time": "2024-11-19T07:02:10.174785",
     "status": "completed"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!rm chapter.py; touch chapter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8681261",
   "metadata": {
    "papermill": {
     "duration": 0.000795,
     "end_time": "2024-11-19T07:02:10.324716",
     "exception": false,
     "start_time": "2024-11-19T07:02:10.323921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## References and readings\n",
    "\n",
    "- [Recurrent Neural Networks. Dive into Deep Learning](https://www.d2l.ai/chapter_recurrent-neural-networks/index.html)\n",
    "- [Sentiment Analysis. NVIDIA](https://www.nvidia.com/en-us/glossary/sentiment-analysis/)\n",
    "- [Machine Translation. Google Research](https://research.google/research-areas/machine-translation/)\n",
    "- [Automatic Speech Recognition (ASR), or Speech-to-Text](https://www.nvidia.com/en-us/glossary/speech-to-text/)\n",
    "- [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/deepimagesent/)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.256908,
   "end_time": "2024-11-19T07:02:10.441844",
   "environment_variables": {},
   "exception": null,
   "input_path": "04-intro.ipynb",
   "output_path": "04-intro.ipynb",
   "parameters": {},
   "start_time": "2024-11-19T07:02:09.184936",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
