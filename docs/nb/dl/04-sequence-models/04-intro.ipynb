{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will deal with variable-length sequence data. \n",
    "This is fundamentally different from previous types of data that we have encountered (i.e. fixed shape). But variable-length data is abundant in the real-world. Tasks such as translating passages of text from one natural language to another, \n",
    "engaging in dialogue, or controlling a robot, demand that models both ingest and output sequentially structured data.\n",
    "Here we focus on text data which is our primary interest.\n",
    "\n",
    "We will create a dataset consisting of character sequences from *The Time Machine* (1895) by [H. G. Wells](https://en.wikipedia.org/wiki/H._G._Wells) (1866-1946). And to model this dataset, we introduce **Recurrent Neural Networks** (RNNs).\n",
    "RNNs are neural network models that capture the dynamics of sequences via recurrent connections, \n",
    "which can be thought of as cycles in the network of nodes that iteratively update a hidden state vector ({numref}`04-rnn`). \n",
    "The updates depend on the specific order in which inputs are fed into the network. Hence, RNNs have a built-in causal structure. \n",
    "\n",
    "Finally, we derive the **BPTT equations** (**B**ack**p**ropagation **T**hrough **T**ime) which characterize gradient flow through RNNs&mdash; essentially accumulating gradients with depth corresponding to time steps instead of number of layers for MLPs. We will see that calculating gradients is challenging when training RNNs, motivating the modern RNN architectures that will be discussed in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "```{figure} ../../../img/nn/04-rnn.svg\n",
    "---\n",
    "width: 600px\n",
    "name: 04-rnn\n",
    "align: center\n",
    "---\n",
    "RNN unit (a) cyclic, and (b) unrolled RNN (essentially a deep MLP with shared weights).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential data\n",
    "\n",
    "We consider inputs of the form\n",
    "$\\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_T$ where $\\boldsymbol{\\mathsf{x}}_t \\in \\mathbb{R}^d$ for $t = 1, \\ldots, T.$ \n",
    "For example collection of words in a document, or sequence of events that occur for an RL agent.\n",
    "In each of these, the entities are represented using a state vector in $\\mathbb{R}^d.$ Note that $T$ is usually a maximum length, and the model may process variable-length inputs of length $\\tau$ where  $1 \\leq \\tau \\leq T.$ In terms of targets, we can have:\n",
    "\n",
    "\n",
    "|Task|Mapping|Example|\n",
    "|------|------|-----|\n",
    "| Fixed target |$(\\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_T) \\mapsto \\boldsymbol{\\mathsf{y}}$ | Sentiment Analysis [[1]](https://www.nvidia.com/en-us/glossary/sentiment-analysis/) |\n",
    "| Fixed input | $\\boldsymbol{\\mathsf{x}} \\mapsto (\\boldsymbol{\\mathsf{y}}_1, \\ldots, \\boldsymbol{\\mathsf{y}}_T)$ | Image Captioning [[3]](https://cs.stanford.edu/people/karpathy/deepimagesent/) |\n",
    "| Sequence-to-Sequence | $(\\boldsymbol{\\mathsf{y}}_1, \\ldots, \\boldsymbol{\\mathsf{y}}_T) \\mapsto (\\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_T)$  | Video Captioning, Machine Translation |\n",
    "\n",
    "Sequence-to-sequence tasks take two forms: \n",
    "\n",
    "|Type|Constraint|Example|\n",
    "|------|------|-----|\n",
    "| Aligned | Corresponding target aligns with input at each time step | Speech recognition ([STT](https://www.nvidia.com/en-us/glossary/speech-to-text/)) |\n",
    "| Unaligned | No step-for-step correspondence required | Machine Translation [[2](https://research.google/research-areas/machine-translation/)] | \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Autoregressive modeling\n",
    "\n",
    "- $p(\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_{t-1})$\n",
    "- entire distribution hard to compute, and a user can be content with $\\mathbb{E}\\left[\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_{t-1}\\right]$ e.g. with a linear regression model\n",
    "- such models that regress the value of a signal on the previous values of that same signal are naturally called autoregressive models.\n",
    "- one issue is that the inputs vary with $t$. \n",
    "- In other words, the number of inputs increases with the amount of data that we encounter.\n",
    "- Much of what follows in this chapter revolve around techniques for dealing with this when estimating $p(\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_1, \\ldots, \\boldsymbol{\\mathsf{x}}_{t-1})$ or some statistic(s) of this distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first strategy is to only use past $\\tau$ observations, so that we estimate $p(\\boldsymbol{\\mathsf{x}}_t \\mid \\boldsymbol{\\mathsf{x}}_{t-\\tau}, \\ldots, \\boldsymbol{\\mathsf{x}}_{t-1})$\n",
    "- then all inputs are of length $\\tau$\n",
    "- this allows us to train any linear model or deep network that requires fixed-length vectors as inputs.\n",
    "-  Second, we might develop models that maintain some summary $\\boldsymbol{\\mathsf{h}}_t$ of the past observations used to predict the next output and also updates with each observation, i.e. $\\boldsymbol{\\mathsf{y}}_t = f(\\boldsymbol{\\mathsf{h}}_{t})$ and $\\boldsymbol{\\mathsf{h}}_t = g(\\boldsymbol{\\mathsf{x}}_{t-1}, \\boldsymbol{\\mathsf{h}}_{t-1}).$\n",
    "- Since $\\boldsymbol{\\mathsf{h}}_t$ is never observed, these models are also called **latent autoregressive models**.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- To construct training data from historical data, one typically creates examples by sampling windows randomly\n",
    "- we often assume that the underlying data generation process does not change, i.e. is stationary. In practice, this\n",
    "means that the weights are independent of the current time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!rm chapter.py; touch chapter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and readings\n",
    "\n",
    "- [Recurrent Neural Networks. Dive into Deep Learning](https://www.d2l.ai/chapter_recurrent-neural-networks/index.html)\n",
    "- [Sentiment Analysis. NVIDIA](https://www.nvidia.com/en-us/glossary/sentiment-analysis/)\n",
    "- [Machine Translation. Google Research](https://research.google/research-areas/machine-translation/)\n",
    "- [Automatic Speech Recognition (ASR), or Speech-to-Text](https://www.nvidia.com/en-us/glossary/speech-to-text/)\n",
    "- [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/deepimagesent/)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
