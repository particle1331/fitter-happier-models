{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the state is updated at each time step with the current input. Thus, we have to track the dependencies across time steps where the RNN parameters are shared. This is called *Backprogation Through Time* (BPTT) or BP for sequence models.\n",
    "Hopefully, this discussion will bring some precision to the notion of vanishing and exploding gradients. \n",
    "\n",
    "This procedure requires us to expand (or unroll) the computational graph of an RNN one time step at a time. The unrolled RNN is essentially a feedforward neural network with the special property that the same parameters are repeated throughout the unrolled network, appearing at each time step.\n",
    "Then, we can apply the usual BP through the unrolled net. In particular, we want to see causality in the equations, i.e. state at time $t$ only influences future time steps.\n",
    "\n",
    "For long sequences, e.g. text sequences containing over a thousand tokens, BP across many layers poses problems both from a computational (too much memory to compress in a single state vector) and optimization standpoint (numerical instability). Here input from the first step passes through $T$ matrix products before arriving at the output. Similarly, we expect $T$ matrix products are required to compute the gradient at the first time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/nn/04-rnn-backprop.svg\n",
    "---\n",
    "name: 04-rnn-backprop\n",
    "width: 500px\n",
    "align: center\n",
    "---\n",
    "RNN cell backpropation. Note that the matrices $\\boldsymbol{\\mathsf{W}}, \\boldsymbol{\\mathsf{U}},$ and $\\boldsymbol{\\mathsf{V}}$ are shared across time steps.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\mathsf{H}}_t &= f(\\boldsymbol{\\mathsf{X}}_t \\boldsymbol{\\mathsf{U}} + \\boldsymbol{\\mathsf{H}}_{t-1} \\boldsymbol{\\mathsf{W}} + \\boldsymbol{\\mathsf{b}}) \\\\\n",
    "\\boldsymbol{\\mathsf{Y}}_t &= \\boldsymbol{\\mathsf{H}}_t \\boldsymbol{\\mathsf{V}} + \\boldsymbol{\\mathsf{c}} \\\\\n",
    "\\boldsymbol{\\mathsf{H}}_{t+1} &= f(\\boldsymbol{\\mathsf{X}}_{t+1} \\boldsymbol{\\mathsf{U}} + \\boldsymbol{\\mathsf{H}}_{t} \\boldsymbol{\\mathsf{W}} + \\boldsymbol{\\mathsf{b}}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assume incoming gradients $\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Y}}_t}$ and $\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{H}}_{t+1}}$ from the next layer.\n",
    "We start by calculating the gradient with respect to $\\boldsymbol{\\mathsf{V}}.$ Here we abstract the product between two tensors on appropriate indices by using the $\\text{prod}$ notation. The exact formula can be recovered with [tensor index notation](https://en.wikipedia.org/wiki/Einstein_notation). Let $f$ be an activation function. Upper case indicates that a tensor's first dimension is the batch dimension when applicable. Then, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{V}}}}_{(h, q)} &= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Y}}_t}, \\frac{\\partial \\boldsymbol{\\mathsf{Y}}_t}{\\partial \\boldsymbol{\\mathsf{V}}}\\right) = \\sum_{t=1}^T \\underbrace{\\boldsymbol{\\mathsf{H}}_t^\\top \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Y}}_t}}_{(h, B) \\,\\times\\, (B, q)} \\\\\n",
    "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{c}}}}_{(1, q)}\n",
    "&= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Y}}_t}, \\frac{\\partial \\boldsymbol{\\mathsf{Y}}_t}{\\partial \\boldsymbol{\\mathsf{c}}}\\right) = \\sum_{t=1}^T \\underbrace{\\boldsymbol{\\mathsf{1}}^\\top \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Y}}_t}}_{(1, B) \\,\\times\\, (B, q)} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Next, we calculate the gradients flowing to $\\boldsymbol{\\mathsf{H}}_t$ which will be our gateway to compute gradients of $\\boldsymbol{\\mathsf{W}}$, $\\boldsymbol{\\mathsf{U}}$, and $\\boldsymbol{\\mathsf{b}}$, and finally $\\boldsymbol{\\mathsf{X}}_t.$ Note that $\\boldsymbol{\\mathsf{H}}_t$ affects not only $\\boldsymbol{\\mathsf{Y}}_t$, but also future $\\boldsymbol{\\mathsf{Y}}_{t^\\prime}$ via $\\boldsymbol{\\mathsf{H}}_{t^\\prime}$ for $t^\\prime > t.$ But in terms of direct dependence, the nodes that immediately depend on $\\boldsymbol{\\mathsf{H}}_t$ are $\\boldsymbol{\\mathsf{Y}}_t$ and $\\boldsymbol{\\mathsf{H}}_{t+1}$ ({numref}`04-rnn-backprop`). Let $\\boldsymbol{\\mathsf{Z}}_{t+1} = \\boldsymbol{\\mathsf{X}}_{t+1} \\boldsymbol{\\mathsf{U}} + \\boldsymbol{\\mathsf{H}}_{t} \\boldsymbol{\\mathsf{W}} + \\boldsymbol{\\mathsf{b}}.$ Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\underbrace{\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_t}}_{(B, h)}\n",
    "&= \n",
    "\\text{prod}\\left(\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{Y}}_t}, \n",
    "    \\frac{\\partial\\boldsymbol{\\mathsf{Y}}_t}{\\partial\\boldsymbol{\\mathsf{H}}_t}\n",
    "\\right) + \n",
    "\\text{prod}\\left(\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_{t + 1}}, \n",
    "    \\frac{\\partial\\boldsymbol{\\mathsf{H}}_{t + 1}}{\\partial\\boldsymbol{\\mathsf{Z}}_{t+1}},\n",
    "    \\frac{\\partial\\boldsymbol{\\mathsf{Z}}_{t+1}}{\\partial\\boldsymbol{\\mathsf{H}}_{t }}\n",
    "\\right) \\\\\n",
    "&= \n",
    "\\underbrace{\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{Y}}_t}\\, \\boldsymbol{\\mathsf{V}}^\\top}_{(B, q)\\,\\times\\,(q, h)} +\n",
    "\\underbrace{\n",
    "    \\left(\n",
    "        \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_{t + 1}}\n",
    "        \\odot \n",
    "        f^\\prime(\\boldsymbol{\\mathsf{Z}}_{t+1})\n",
    "    \\right) \\boldsymbol{\\mathsf{W}}^\\top\n",
    "}_{((B, h)\\, \\cdot \\, (B, h)) \\, \\times \\, (h, h)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sense of this, recall $\\boldsymbol{\\mathsf{V}}$ and $\\boldsymbol{\\mathsf{W}}$ acts on $\\boldsymbol{\\mathsf{H}}_t$ from the left. Hence, when we take its transpose, multiplying a tensor to the right of $\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_t}$, results in a summation along the dimension containing information about the state $\\boldsymbol{\\mathsf{H}}_t.$ \n",
    "Similarly, the orientation of the products within the expression are also correct.\n",
    "\n",
    "Note that the above expression is recursive, we should be able to get a closed form expression from terms in time step $t, t+1, \\ldots, T.$ For tractability, let's assume we have no nonlinearity, or $f = \\text{Id},$ so that $f^\\prime(\\boldsymbol{\\mathsf{Z}}_{t + 1}) = \\mathbf{1}_{(B, h)}$. Then, we can write:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_t = \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_t}, \\quad\n",
    "b_t = \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{Y}}_t}\\, \\boldsymbol{\\mathsf{V}}^\\top, \\quad\n",
    "c_t = \\boldsymbol{\\mathsf{W}}^\\top\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with $a_{T+1} = 0$ and $a_T = b_T.$ Thus,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_t &= b_t + a_{t+1} c_t \\\\\n",
    "&= b_t + (b_{t + 1} + a_{t + 2} c_{t + 1}) c_t \\\\ \n",
    "&= b_t + (b_{t + 1} + (b_{t + 2} + a_{t + 3} c_{t + 2}) c_{t + 1}) c_t \\\\\n",
    "&= b_t + b_{t + 1} c_t + b_{t + 2}c_{t + 1}c_t + a_{t + 3} c_{t + 2}c_{t + 1}c_t \\\\\n",
    "&\\vdots \\\\\n",
    "&= b_t + \\sum_{t^\\prime = t + 1}^T b_{t^\\prime} \\prod_{j=t}^{t^\\prime - 1} c_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_t} = \n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{Y}}_t}\\, \\boldsymbol{\\mathsf{V}}^\\top\n",
    "+ \n",
    "\\sum_{t^\\prime = t + 1}^T\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{Y}}_{t^\\prime}}\\, \\boldsymbol{\\mathsf{V}}^\\top\n",
    "\\prod_{j=t}^{t^\\prime - 1} \n",
    " \\boldsymbol{\\mathsf{W}}^\\top\n",
    "=\n",
    " \\sum_{t^\\prime = t}^T\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{Y}}_{t^\\prime}}\\, \\boldsymbol{\\mathsf{V}}^\\top\n",
    "\\left(\\boldsymbol{\\mathsf{W}}^\\top\\right)^{t^\\prime - t}.\n",
    "}\n",
    "$$ (state_vec_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This formula is similar to that for gradient flow across the layers of a deep MLP network, but here the depth is along sequence length. \n",
    "The terms in the sum correspond to paths of increasing path lengths $t^\\prime - t$ from the  current time step $t.$ Finally, observe that the change in loss due to the current time step is only due to its effect on future time steps, not on the past, so we have a notion of causality in RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate the rest of the parameter gradients. Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{U}}}}_{(d, h)} &= \\sum_{t=1}^T \\text{prod}\\left(\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{H}}_t}, \n",
    "    \\frac{\\partial \\boldsymbol{\\mathsf{H}}_t}{\\partial \\boldsymbol{\\mathsf{Z}}_t},\n",
    "    \\frac{\\partial \\boldsymbol{\\mathsf{Z}}_t}{\\partial \\boldsymbol{\\mathsf{U}}}\n",
    "\\right) \n",
    "= \n",
    "\\sum_{t=1}^T \\underbrace{\n",
    "    \\boldsymbol{\\mathsf{X}}_{t}^\\top \n",
    "    \\left(\n",
    "        \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_{t}}\n",
    "        \\odot \n",
    "        f^\\prime(\\boldsymbol{\\mathsf{Z}}_{t})\n",
    "    \\right)\n",
    "}_{(d, B) \\,\\times\\, ((B, h) \\, \\cdot\\, (B, h))}\n",
    "\\\\\\\\\n",
    "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{W}}}}_{(h, h)} &= \\sum_{t=1}^T \\text{prod}\\left(\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{H}}_t}, \n",
    "    \\frac{\\partial \\boldsymbol{\\mathsf{H}}_t}{\\partial \\boldsymbol{\\mathsf{Z}}_t},\n",
    "    \\frac{\\partial \\boldsymbol{\\mathsf{Z}}_t}{\\partial \\boldsymbol{\\mathsf{W}}}\n",
    "\\right) \n",
    "= \n",
    "\\sum_{t=1}^T \\underbrace{\\boldsymbol{\\mathsf{H}}_{t-1}^\\top \\left(\n",
    "        \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_{t}}\n",
    "        \\odot \n",
    "        f^\\prime(\\boldsymbol{\\mathsf{Z}}_{t})\n",
    "    \\right)}_{(h, B) \\,\\times\\, ((B, h) \\, \\cdot\\, (B, h))} \n",
    "\\\\\\\\\n",
    "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{b}}}}_{(1, h)}\n",
    "&= \n",
    "\\sum_{t=1}^T \\text{prod}\\left(\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{H}}_t}, \n",
    "    \\frac{\\partial \\boldsymbol{\\mathsf{H}}_t}{\\partial \\boldsymbol{\\mathsf{Z}}_t},\n",
    "    \\frac{\\partial \\boldsymbol{\\mathsf{Z}}_t}{\\partial \\boldsymbol{\\mathsf{b}}}\n",
    "\\right) \n",
    "= \\sum_{t=1}^T \\underbrace{\\boldsymbol{\\mathsf{1}}^\\top\n",
    "    \\left(\n",
    "        \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_{t}}\n",
    "        \\odot \n",
    "        f^\\prime(\\boldsymbol{\\mathsf{Z}}_{t})\n",
    "    \\right)\n",
    "}_{(1, B) \\,\\times\\, ((B, h) \\, \\cdot \\, (B, h))}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient to inputs may be also relevant (e.g. deep RNNs):\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{X}}_t}}_{(B, d)}\n",
    "=\n",
    "\\text{prod}\\left(\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{H}}_t}, \n",
    "    \\frac{\\partial \\boldsymbol{\\mathsf{H}}_t}{\\partial \\boldsymbol{\\mathsf{Z}}_t},\n",
    "    \\frac{\\partial \\boldsymbol{\\mathsf{Z}}_t}{\\partial \\boldsymbol{\\mathsf{X}}_t}\n",
    "\\right) \n",
    "= \\underbrace{\n",
    "    \\left(\n",
    "        \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_{t}}\n",
    "        \\odot \n",
    "        f^\\prime(\\boldsymbol{\\mathsf{Z}}_{t})\n",
    "    \\right) \\boldsymbol{\\mathsf{U}}^\\top\n",
    "}_{((B, h) \\, \\cdot \\, (B, h)) \\, \\times \\, (h, d)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the key quantity that affects the numerical stability is $\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mathsf{H}}_t}$ {eq}`state_vec_grad`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from chapter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "B, T, V, h = 32, 5, 30, 64\n",
    "\n",
    "# forward pass\n",
    "O = torch.randint(low=0, high=V, size=(B, T))   # (B, T)\n",
    "x = torch.randint(low=0, high=V, size=(B, T))   # (B, T)\n",
    "X = F.one_hot(x, num_classes=V).float()         # (B, T, V)\n",
    "X.requires_grad = True\n",
    "\n",
    "model = RNNLanguageModel(V, h, V)              \n",
    "\n",
    "W  = model.rnn.W                                # (h, h)\n",
    "U  = model.rnn.U                                # (V, h)\n",
    "b  = model.rnn.b                                # (h,)\n",
    "Vt = model.linear.weight                        # (V, h)\n",
    "c  = model.linear.bias                          # (V,)\n",
    "Y  = model(X)                                   # (B, V, T)\n",
    "H  = model.rnn(X)[0]                            # (B, T, h)\n",
    "J  = 1 - H * H                                  # (B, T, h)\n",
    "\n",
    "# backprop\n",
    "X.retain_grad()\n",
    "Y.retain_grad()\n",
    "loss = F.cross_entropy(Y, O)\n",
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoke test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ((H @ Vt.T + c).transpose(1, 2) - Y).abs().max() == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the gradients by hand: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dY = Y.grad\n",
    "dH = [None] * T\n",
    "dH[T - 1] = dY[:, :, T - 1] @ Vt\n",
    "for t in range(T - 2, -1, -1):\n",
    "    dH[t] = dY[:, :, t] @ Vt + (dH[t + 1] * J[:, t + 1]) @ W.T\n",
    "    \n",
    "dH = torch.stack(dH, dim=1)\n",
    "dZ = dH * J\n",
    "dc = torch.einsum('bjt -> j', dY)\n",
    "dV = torch.einsum('bth, bvt -> hv', H, dY)\n",
    "dU = torch.einsum('btv, bth -> vh', X, dH * J)\n",
    "db = torch.einsum('bth -> h', dH * J)\n",
    "dX = torch.einsum('bth, vh -> btv', dH * J, U)\n",
    "dW = sum([H[:, t-1].T @ (dH * J)[:, t] for t in range(1, T)], torch.zeros((h, h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating absolute errors versus `autograd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dV  | exact: True  | approx: True  | maxdiff: 0.00e+00\n",
      "dc  | exact: True  | approx: True  | maxdiff: 0.00e+00\n",
      "dU  | exact: False | approx: True  | maxdiff: 9.31e-10\n",
      "dW  | exact: False | approx: True  | maxdiff: 9.31e-10\n",
      "db  | exact: False | approx: True  | maxdiff: 2.79e-09\n",
      "dX  | exact: True  | approx: True  | maxdiff: 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "def compare(name, dt, t):\n",
    "    exact  = torch.all(dt == t.grad).item()\n",
    "    approx = torch.allclose(dt, t.grad, rtol=1e-5)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{name:<3s} | exact: {str(exact):5s} | approx: {str(approx):5s} | maxdiff: {maxdiff:.2e}')\n",
    "    return approx\n",
    "\n",
    "assert compare('dV', dV.T, Vt)\n",
    "assert compare('dc', dc, c)\n",
    "assert compare('dU', dU, U)\n",
    "assert compare('dW', dW, W)\n",
    "assert compare('db', db, b)\n",
    "assert compare('dX', dX, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
