{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5aa8f7",
   "metadata": {
    "papermill": {
     "duration": 0.008835,
     "end_time": "2024-11-19T07:05:37.093798",
     "exception": false,
     "start_time": "2024-11-19T07:05:37.084963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc5cdc",
   "metadata": {
    "papermill": {
     "duration": 0.005914,
     "end_time": "2024-11-19T07:05:37.105607",
     "exception": false,
     "start_time": "2024-11-19T07:05:37.099693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Previously, we described various language models where the conditional probability of token $\\boldsymbol{\\mathsf{x}}_t$ depends on a fixed context $\\boldsymbol{\\mathsf{x}}_{[t - \\tau: t-1]}.$ If we want to incorporate the possible effect of tokens earlier than the given context, we need to increase the context size $\\tau$. For the *n*-gram model, this would increase the parameters exponentially in $\\tau$. Using embeddings, the MLP network the number of parameters grows as $O(\\tau)$. Finally, using convolutions this decreases to $O(\\log \\tau).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199176f8",
   "metadata": {
    "papermill": {
     "duration": 0.001984,
     "end_time": "2024-11-19T07:05:37.111803",
     "exception": false,
     "start_time": "2024-11-19T07:05:37.109819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Alternatively, instead of modeling the next token directly in terms of previous tokens, we can use a latent variable that, in principle, stores *all* previous information up to the previous time step:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\mathsf x}_{t} \\mid \\boldsymbol{\\mathsf x}_{1}, \\ldots, \\boldsymbol{\\mathsf x}_{t-1}) \\approx p(\\boldsymbol{\\mathsf x}_{t} \\mid \\boldsymbol{\\mathsf h}_{t-1})\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mathsf h}_{t-1}$ is a *hidden state* that stores information up to the time step $t - 1.$ The hidden state is updated based on the current input and the previous state: \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mathsf h}_{t} = f(\\boldsymbol{\\mathsf x}_{t}, \\boldsymbol{\\mathsf h}_{t-1}),\n",
    "$$\n",
    "\n",
    "so that $\\boldsymbol{\\mathsf h}_{t} = F(\\boldsymbol{\\mathsf x}_{1}, \\ldots, \\boldsymbol{\\mathsf x}_{t}, \\boldsymbol{\\mathsf h}_{0})$ for some $\\boldsymbol{\\mathsf h}_{0}$ where $F$ involves recursively applying $f.$ Note that for a sufficiently powerful function $f$, the latent variable model above is not an approximation, since $\\boldsymbol{\\mathsf h}_{t}$ can simply store all $\\boldsymbol{\\mathsf x}_{1}, \\ldots, \\boldsymbol{\\mathsf x}_{t}$ it has observed so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034ff5f",
   "metadata": {
    "papermill": {
     "duration": 0.001028,
     "end_time": "2024-11-19T07:05:37.113882",
     "exception": false,
     "start_time": "2024-11-19T07:05:37.112854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3baf14",
   "metadata": {
    "papermill": {
     "duration": 0.000939,
     "end_time": "2024-11-19T07:05:37.115761",
     "exception": false,
     "start_time": "2024-11-19T07:05:37.114822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "RNNs use the same parameters at each time step, i.e. it is assumed that the dynamics is **stationary**. Practically, this means that the number of parameters does not grow as the sequence length increases.\n",
    "The following implementation is called **Simple RNN**, the state update is calculated using essentially a linear layer where the embedding and hidden state are concatenated as input. Let tokens correspond to embedding vectors $\\boldsymbol{\\mathsf{x}}_t \\in \\mathbb{R}^d$, then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\mathsf{h}}_t &= \\varphi(\\boldsymbol{\\mathsf{x}}_t \\boldsymbol{\\mathsf{U}} + \\boldsymbol{\\mathsf{h}}_{t-1} \\boldsymbol{\\mathsf{W}} + \\boldsymbol{\\mathsf{b}}) \\\\\n",
    "\\boldsymbol{\\mathsf{y}}_t &= \\boldsymbol{\\mathsf{h}}_t \\boldsymbol{\\mathsf{V}} + \\boldsymbol{\\mathsf{c}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mathsf{U}} \\in \\mathbb{R}^{d \\times h}$, $\\boldsymbol{\\mathsf{W}} \\in \\mathbb{R}^{h \\times h}$, and $\\boldsymbol{\\mathsf{b}} \\in \\mathbb{R}^{h}.$ Here $h$ is the dimensionality of the hidden state. For the outputs, we also have $\\boldsymbol{\\mathsf{V}} \\in \\mathbb{R}^{h \\times q}$ and $\\boldsymbol{\\mathsf{c}} \\in \\mathbb{R}^{q}$ where $q$ is the dimensionality of the output. This computation can be seen in {numref}`04-simple-rnn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f8be1",
   "metadata": {
    "papermill": {
     "duration": 0.001177,
     "end_time": "2024-11-19T07:05:37.117859",
     "exception": false,
     "start_time": "2024-11-19T07:05:37.116682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that $\\boldsymbol{\\mathsf{x}}_t$ can be one-hot vectors since the matrix $\\boldsymbol{\\mathsf{U}}$ can act as the embedding matrix for the tokens. In this case, $\\boldsymbol{\\mathsf{U}}$ has shape $(|\\mathcal{V}|, h).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708f1f1",
   "metadata": {
    "papermill": {
     "duration": 0.000917,
     "end_time": "2024-11-19T07:05:37.119730",
     "exception": false,
     "start_time": "2024-11-19T07:05:37.118813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "```{figure} ../../../img/nn/04-simple-rnn.svg\n",
    "---\n",
    "width: 600px\n",
    "name: 04-simple-rnn\n",
    "align: center\n",
    "---\n",
    "Computational graph of an unrolled simple RNN. [Source](https://www.d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9c08e",
   "metadata": {
    "papermill": {
     "duration": 0.000939,
     "end_time": "2024-11-19T07:05:37.122475",
     "exception": false,
     "start_time": "2024-11-19T07:05:37.121536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, we implement the recurrent layer. To implement batch computation, an input has shape $(B, T, d).$ That is, a batch of $B$ sequences of length $T$, consisting of vectors in $\\mathbb{R}^d.$ Elements of a batch are computed in independently, ideally in parallel. At each step, the layer returns the state vector of shape $(B, h).$ This is stacked to get a tensor of shape $(B, T, h)$ consistent with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e584caa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T07:05:37.126465Z",
     "iopub.status.busy": "2024-11-19T07:05:37.126229Z",
     "iopub.status.idle": "2024-11-19T07:05:38.075594Z",
     "shell.execute_reply": "2024-11-19T07:05:38.075273Z"
    },
    "papermill": {
     "duration": 0.952431,
     "end_time": "2024-11-19T07:05:38.076740",
     "exception": false,
     "start_time": "2024-11-19T07:05:37.124309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, dim_inputs, dim_hidden):\n",
    "        super().__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_inputs = dim_inputs\n",
    "        self.W = nn.Parameter(torch.randn(dim_hidden, dim_hidden) / np.sqrt(dim_hidden))\n",
    "        self.U = nn.Parameter(torch.randn(dim_inputs, dim_hidden) / np.sqrt(dim_inputs))\n",
    "        self.b = nn.Parameter(torch.zeros(dim_hidden))\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        x = x.transpose(0, 1)  # (B, T, d) -> (T, B, d)\n",
    "        T, B, d = x.shape\n",
    "        assert d == self.dim_inputs\n",
    "        if state is None:\n",
    "            state = torch.zeros(B, self.dim_hidden, device=x.device)\n",
    "        else:\n",
    "            assert state.shape == (B, self.dim_hidden)\n",
    "\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            state = torch.tanh(x[t] @ self.U + state @ self.W + self.b)\n",
    "            outs.append(state)\n",
    "\n",
    "        outs = torch.stack(outs)\n",
    "        outs = outs.transpose(0, 1)\n",
    "        return outs, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca2d3d",
   "metadata": {
    "papermill": {
     "duration": 0.00103,
     "end_time": "2024-11-19T07:05:38.079095",
     "exception": false,
     "start_time": "2024-11-19T07:05:38.078065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Shapes test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73338dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T07:05:38.081925Z",
     "iopub.status.busy": "2024-11-19T07:05:38.081749Z",
     "iopub.status.idle": "2024-11-19T07:05:38.092276Z",
     "shell.execute_reply": "2024-11-19T07:05:38.091991Z"
    },
    "papermill": {
     "duration": 0.013125,
     "end_time": "2024-11-19T07:05:38.093203",
     "exception": false,
     "start_time": "2024-11-19T07:05:38.080078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = 5\n",
    "B, T, d = 32, 10, 512\n",
    "rnn = SimpleRNN(dim_inputs=d, dim_hidden=h)\n",
    "outs, state = rnn(torch.randn(B, T, d))\n",
    "assert outs.shape == (B, T, h)\n",
    "assert state.shape == (B, h)\n",
    "assert torch.abs(outs[:, -1, :] - state).max() < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f6ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.489755,
   "end_time": "2024-11-19T07:05:38.521541",
   "environment_variables": {},
   "exception": null,
   "input_path": "04d-rnn.ipynb",
   "output_path": "04d-rnn.ipynb",
   "parameters": {},
   "start_time": "2024-11-19T07:05:36.031786",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
