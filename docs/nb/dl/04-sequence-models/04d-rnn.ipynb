{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5aa8f7",
   "metadata": {
    "papermill": {
     "duration": 0.002817,
     "end_time": "2024-11-24T16:39:09.885365",
     "exception": false,
     "start_time": "2024-11-24T16:39:09.882548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc5cdc",
   "metadata": {
    "papermill": {
     "duration": 0.001698,
     "end_time": "2024-11-24T16:39:09.889301",
     "exception": false,
     "start_time": "2024-11-24T16:39:09.887603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Previously, we described various language models where the conditional probability of token $\\boldsymbol{\\mathsf{x}}_t$ depends on a fixed context $\\boldsymbol{\\mathsf{x}}_{[t - \\tau: t-1]}.$ If we want to incorporate the possible effect of tokens earlier than the given context, we need to increase the context size $\\tau$. For the *n*-gram model, this would increase the parameters exponentially in $\\tau$. Using embeddings, the MLP network the number of parameters grows as $O(\\tau)$. Finally, using convolutions this decreases to $O(\\log \\tau).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199176f8",
   "metadata": {
    "papermill": {
     "duration": 0.001542,
     "end_time": "2024-11-24T16:39:09.892460",
     "exception": false,
     "start_time": "2024-11-24T16:39:09.890918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Alternatively, instead of modeling the next token directly in terms of previous tokens, we can use a latent variable that, in principle, stores *all* previous information up to the previous time step:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\mathsf x}_{t} \\mid \\boldsymbol{\\mathsf x}_{1}, \\ldots, \\boldsymbol{\\mathsf x}_{t-1}) \\approx p(\\boldsymbol{\\mathsf x}_{t} \\mid \\boldsymbol{\\mathsf h}_{t-1})\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mathsf h}_{t-1}$ is a *hidden state* that stores information up to the time step $t - 1.$ The hidden state is updated based on the current input and the previous state: \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mathsf h}_{t} = f(\\boldsymbol{\\mathsf x}_{t}, \\boldsymbol{\\mathsf h}_{t-1}),\n",
    "$$\n",
    "\n",
    "so that $\\boldsymbol{\\mathsf h}_{t} = F(\\boldsymbol{\\mathsf x}_{1}, \\ldots, \\boldsymbol{\\mathsf x}_{t}, \\boldsymbol{\\mathsf h}_{0})$ for some $\\boldsymbol{\\mathsf h}_{0}$ where $F$ involves recursively applying $f.$ Note that for a sufficiently powerful function $f$, the latent variable model above is not an approximation, since $\\boldsymbol{\\mathsf h}_{t}$ can simply store all $\\boldsymbol{\\mathsf x}_{1}, \\ldots, \\boldsymbol{\\mathsf x}_{t}$ it has observed so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034ff5f",
   "metadata": {
    "papermill": {
     "duration": 0.001583,
     "end_time": "2024-11-24T16:39:09.895682",
     "exception": false,
     "start_time": "2024-11-24T16:39:09.894099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3baf14",
   "metadata": {
    "papermill": {
     "duration": 0.001699,
     "end_time": "2024-11-24T16:39:09.898943",
     "exception": false,
     "start_time": "2024-11-24T16:39:09.897244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "RNNs use the same parameters at each time step, i.e. it is assumed that the dynamics is *stationary*. Practically, this means that the parameter count does not grow as the sequence length increases, and that the parameters have to time index.\n",
    "The following implements what is called the **Simple RNN**. Here, the state update is calculated using essentially a linear layer with the embedding and hidden state vectors are concatenated as input. \n",
    "\n",
    "Let each token be represented by vectors $\\boldsymbol{\\mathsf{x}}_t \\in \\mathbb{R}^{d}$ and let $\\boldsymbol{\\mathsf{h}}_0 = \\boldsymbol{0}.$ Then,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\mathsf{h}}_t &= \\varphi(\\boldsymbol{\\mathsf{x}}_t \\boldsymbol{\\mathsf{U}} + \\boldsymbol{\\mathsf{h}}_{t-1} \\boldsymbol{\\mathsf{W}} + \\boldsymbol{\\mathsf{b}}) \\\\\n",
    "\\boldsymbol{\\mathsf{y}}_t &= \\boldsymbol{\\mathsf{h}}_t \\boldsymbol{\\mathsf{V}} + \\boldsymbol{\\mathsf{c}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mathsf{U}} \\in \\mathbb{R}^{d \\times h}$, $\\boldsymbol{\\mathsf{W}} \\in \\mathbb{R}^{h \\times h}$, and $\\boldsymbol{\\mathsf{b}} \\in \\mathbb{R}^{h}.$ Here $h$ is the dimensionality of the hidden state. For the outputs, we also have $\\boldsymbol{\\mathsf{V}} \\in \\mathbb{R}^{h \\times q}$ and $\\boldsymbol{\\mathsf{c}} \\in \\mathbb{R}^{q}$ where $q$ is the dimensionality of the output. This computation can be seen in {numref}`04-simple-rnn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f8be1",
   "metadata": {
    "papermill": {
     "duration": 0.001662,
     "end_time": "2024-11-24T16:39:09.902427",
     "exception": false,
     "start_time": "2024-11-24T16:39:09.900765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Remark.** Note that $\\boldsymbol{\\mathsf{x}}_t$ can be one-hot vectors with $\\boldsymbol{\\mathsf{U}}$ with shape $(\\mathcal{|V|}, h)$ acting as the embedding matrix for the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708f1f1",
   "metadata": {
    "papermill": {
     "duration": 0.001626,
     "end_time": "2024-11-24T16:39:09.906275",
     "exception": false,
     "start_time": "2024-11-24T16:39:09.904649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "```{figure} ../../../img/nn/04-simple-rnn.svg\n",
    "---\n",
    "width: 600px\n",
    "name: 04-simple-rnn\n",
    "align: center\n",
    "---\n",
    "Computational graph of an unrolled simple RNN. [Source](https://www.d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9c08e",
   "metadata": {
    "papermill": {
     "duration": 0.002092,
     "end_time": "2024-11-24T16:39:09.910029",
     "exception": false,
     "start_time": "2024-11-24T16:39:09.907937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, we implement the recurrent layer. To implement batch computation, an input $\\boldsymbol{\\mathsf{X}}$ has shape $(B, T, d).$ That is, a batch of $B$ sequences of length $T$, consisting of vectors in $\\mathbb{R}^{d}.$ Elements of a batch are computed in independently, ideally in parallel. At each step, the layer returns the state vector of shape $(B, h).$ These are stacked to get a tensor of shape $(B, T, h)$ consistent with the input. Note that we only implement the state update equation as output at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e584caa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:39:09.914298Z",
     "iopub.status.busy": "2024-11-24T16:39:09.913945Z",
     "iopub.status.idle": "2024-11-24T16:39:11.296745Z",
     "shell.execute_reply": "2024-11-24T16:39:11.296317Z"
    },
    "papermill": {
     "duration": 1.386337,
     "end_time": "2024-11-24T16:39:11.297980",
     "exception": false,
     "start_time": "2024-11-24T16:39:09.911643",
     "status": "completed"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n",
       "\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">SimpleRNN</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dim_inputs</span><span class=\"p\">,</span> <span class=\"n\">dim_hidden</span><span class=\"p\">):</span>\n",
       "        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dim_hidden</span> <span class=\"o\">=</span> <span class=\"n\">dim_hidden</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dim_inputs</span> <span class=\"o\">=</span> <span class=\"n\">dim_inputs</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Parameter</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">dim_hidden</span><span class=\"p\">,</span> <span class=\"n\">dim_hidden</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">dim_hidden</span><span class=\"p\">))</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">U</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Parameter</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">dim_inputs</span><span class=\"p\">,</span> <span class=\"n\">dim_hidden</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">dim_inputs</span><span class=\"p\">))</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Parameter</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">dim_hidden</span><span class=\"p\">))</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">state</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># (B, T, d) -&gt; (T, B, d)</span>\n",
       "        <span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n",
       "        <span class=\"k\">assert</span> <span class=\"n\">d</span> <span class=\"o\">==</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dim_inputs</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">state</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dim_hidden</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "            <span class=\"k\">assert</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">dim_hidden</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"n\">outs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
       "        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">T</span><span class=\"p\">):</span>\n",
       "            <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">@</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">U</span> <span class=\"o\">+</span> <span class=\"n\">state</span> <span class=\"o\">@</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">b</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">outs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"n\">outs</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">(</span><span class=\"n\">outs</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">outs</span> <span class=\"o\">=</span> <span class=\"n\">outs</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">outs</span><span class=\"p\">,</span> <span class=\"n\">state</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn} \\PY{k}{as} \\PY{n+nn}{nn}\n",
       "\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{SimpleRNN}\\PY{p}{(}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Module}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{dim\\PYZus{}inputs}\\PY{p}{,} \\PY{n}{dim\\PYZus{}hidden}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb}{super}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{dim\\PYZus{}hidden} \\PY{o}{=} \\PY{n}{dim\\PYZus{}hidden}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{dim\\PYZus{}inputs} \\PY{o}{=} \\PY{n}{dim\\PYZus{}inputs}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{W} \\PY{o}{=} \\PY{n}{nn}\\PY{o}{.}\\PY{n}{Parameter}\\PY{p}{(}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{n}{dim\\PYZus{}hidden}\\PY{p}{,} \\PY{n}{dim\\PYZus{}hidden}\\PY{p}{)} \\PY{o}{/} \\PY{n}{np}\\PY{o}{.}\\PY{n}{sqrt}\\PY{p}{(}\\PY{n}{dim\\PYZus{}hidden}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{U} \\PY{o}{=} \\PY{n}{nn}\\PY{o}{.}\\PY{n}{Parameter}\\PY{p}{(}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{n}{dim\\PYZus{}inputs}\\PY{p}{,} \\PY{n}{dim\\PYZus{}hidden}\\PY{p}{)} \\PY{o}{/} \\PY{n}{np}\\PY{o}{.}\\PY{n}{sqrt}\\PY{p}{(}\\PY{n}{dim\\PYZus{}inputs}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{b} \\PY{o}{=} \\PY{n}{nn}\\PY{o}{.}\\PY{n}{Parameter}\\PY{p}{(}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{zeros}\\PY{p}{(}\\PY{n}{dim\\PYZus{}hidden}\\PY{p}{)}\\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{forward}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{x}\\PY{p}{,} \\PY{n}{state}\\PY{o}{=}\\PY{k+kc}{None}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{x} \\PY{o}{=} \\PY{n}{x}\\PY{o}{.}\\PY{n}{transpose}\\PY{p}{(}\\PY{l+m+mi}{0}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} (B, T, d) \\PYZhy{}\\PYZgt{} (T, B, d)}\n",
       "        \\PY{n}{T}\\PY{p}{,} \\PY{n}{B}\\PY{p}{,} \\PY{n}{d} \\PY{o}{=} \\PY{n}{x}\\PY{o}{.}\\PY{n}{shape}\n",
       "        \\PY{k}{assert} \\PY{n}{d} \\PY{o}{==} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{dim\\PYZus{}inputs}\n",
       "        \\PY{k}{if} \\PY{n}{state} \\PY{o+ow}{is} \\PY{k+kc}{None}\\PY{p}{:}\n",
       "            \\PY{n}{state} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{zeros}\\PY{p}{(}\\PY{n}{B}\\PY{p}{,} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{dim\\PYZus{}hidden}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{x}\\PY{o}{.}\\PY{n}{device}\\PY{p}{)}\n",
       "        \\PY{k}{else}\\PY{p}{:}\n",
       "            \\PY{k}{assert} \\PY{n}{state}\\PY{o}{.}\\PY{n}{shape} \\PY{o}{==} \\PY{p}{(}\\PY{n}{B}\\PY{p}{,} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{dim\\PYZus{}hidden}\\PY{p}{)}\n",
       "\n",
       "        \\PY{n}{outs} \\PY{o}{=} \\PY{p}{[}\\PY{p}{]}\n",
       "        \\PY{k}{for} \\PY{n}{t} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{T}\\PY{p}{)}\\PY{p}{:}\n",
       "            \\PY{n}{state} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{tanh}\\PY{p}{(}\\PY{n}{x}\\PY{p}{[}\\PY{n}{t}\\PY{p}{]} \\PY{o}{@} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{U} \\PY{o}{+} \\PY{n}{state} \\PY{o}{@} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{W} \\PY{o}{+} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{b}\\PY{p}{)}\n",
       "            \\PY{n}{outs}\\PY{o}{.}\\PY{n}{append}\\PY{p}{(}\\PY{n}{state}\\PY{p}{)}\n",
       "\n",
       "        \\PY{n}{outs} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{stack}\\PY{p}{(}\\PY{n}{outs}\\PY{p}{)}\n",
       "        \\PY{n}{outs} \\PY{o}{=} \\PY{n}{outs}\\PY{o}{.}\\PY{n}{transpose}\\PY{p}{(}\\PY{l+m+mi}{0}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n}{outs}\\PY{p}{,} \\PY{n}{state}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "import torch\n",
       "import numpy as np\n",
       "import torch.nn as nn\n",
       "\n",
       "\n",
       "class SimpleRNN(nn.Module):\n",
       "    def __init__(self, dim_inputs, dim_hidden):\n",
       "        super().__init__()\n",
       "        self.dim_hidden = dim_hidden\n",
       "        self.dim_inputs = dim_inputs\n",
       "        self.W = nn.Parameter(torch.randn(dim_hidden, dim_hidden) / np.sqrt(dim_hidden))\n",
       "        self.U = nn.Parameter(torch.randn(dim_inputs, dim_hidden) / np.sqrt(dim_inputs))\n",
       "        self.b = nn.Parameter(torch.zeros(dim_hidden))\n",
       "\n",
       "    def forward(self, x, state=None):\n",
       "        x = x.transpose(0, 1)  # (B, T, d) -> (T, B, d)\n",
       "        T, B, d = x.shape\n",
       "        assert d == self.dim_inputs\n",
       "        if state is None:\n",
       "            state = torch.zeros(B, self.dim_hidden, device=x.device)\n",
       "        else:\n",
       "            assert state.shape == (B, self.dim_hidden)\n",
       "\n",
       "        outs = []\n",
       "        for t in range(T):\n",
       "            state = torch.tanh(x[t] @ self.U + state @ self.W + self.b)\n",
       "            outs.append(state)\n",
       "\n",
       "        outs = torch.stack(outs)\n",
       "        outs = outs.transpose(0, 1)\n",
       "        return outs, state"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%save\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, dim_inputs, dim_hidden):\n",
    "        super().__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_inputs = dim_inputs\n",
    "        self.W = nn.Parameter(torch.randn(dim_hidden, dim_hidden) / np.sqrt(dim_hidden))\n",
    "        self.U = nn.Parameter(torch.randn(dim_inputs, dim_hidden) / np.sqrt(dim_inputs))\n",
    "        self.b = nn.Parameter(torch.zeros(dim_hidden))\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        x = x.transpose(0, 1)  # (B, T, d) -> (T, B, d)\n",
    "        T, B, d = x.shape\n",
    "        assert d == self.dim_inputs\n",
    "        if state is None:\n",
    "            state = torch.zeros(B, self.dim_hidden, device=x.device)\n",
    "        else:\n",
    "            assert state.shape == (B, self.dim_hidden)\n",
    "\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            state = torch.tanh(x[t] @ self.U + state @ self.W + self.b)\n",
    "            outs.append(state)\n",
    "\n",
    "        outs = torch.stack(outs)\n",
    "        outs = outs.transpose(0, 1)\n",
    "        return outs, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca2d3d",
   "metadata": {
    "papermill": {
     "duration": 0.001827,
     "end_time": "2024-11-24T16:39:11.301986",
     "exception": false,
     "start_time": "2024-11-24T16:39:11.300159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Shapes test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73338dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:39:11.306547Z",
     "iopub.status.busy": "2024-11-24T16:39:11.306206Z",
     "iopub.status.idle": "2024-11-24T16:39:11.342684Z",
     "shell.execute_reply": "2024-11-24T16:39:11.342287Z"
    },
    "papermill": {
     "duration": 0.040203,
     "end_time": "2024-11-24T16:39:11.344031",
     "exception": false,
     "start_time": "2024-11-24T16:39:11.303828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "B, T, d, h = 32, 10, 30, 5\n",
    "rnn = SimpleRNN(dim_inputs=d, dim_hidden=h)\n",
    "outs, state = rnn(torch.randn(B, T, d))\n",
    "assert outs.shape == (B, T, h)\n",
    "assert state.shape == (B, h)\n",
    "assert torch.abs(outs[:, -1, :] - state).max() < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a35ae4",
   "metadata": {
    "papermill": {
     "duration": 0.001778,
     "end_time": "2024-11-24T16:39:11.347929",
     "exception": false,
     "start_time": "2024-11-24T16:39:11.346151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "**Remark.** The PyTorch RNN module has a similar API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1fdc8a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:39:11.352404Z",
     "iopub.status.busy": "2024-11-24T16:39:11.352061Z",
     "iopub.status.idle": "2024-11-24T16:39:11.362113Z",
     "shell.execute_reply": "2024-11-24T16:39:11.361750Z"
    },
    "papermill": {
     "duration": 0.01371,
     "end_time": "2024-11-24T16:39:11.363372",
     "exception": false,
     "start_time": "2024-11-24T16:39:11.349662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "B, T, d, h = 32, 10, 30, 5\n",
    "rnn = nn.RNN(d, h, batch_first=True)\n",
    "x = torch.randn(B, T, d)\n",
    "outs, state = rnn(x)\n",
    "\n",
    "assert outs.shape == (B, T, h)\n",
    "assert state.shape == (1, B, h)\n",
    "assert torch.abs(outs[:, -1, :] - state).max() < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a202b05",
   "metadata": {
    "papermill": {
     "duration": 0.001804,
     "end_time": "2024-11-24T16:39:11.367646",
     "exception": false,
     "start_time": "2024-11-24T16:39:11.365842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.872864,
   "end_time": "2024-11-24T16:39:11.789649",
   "environment_variables": {},
   "exception": null,
   "input_path": "04d-rnn.ipynb",
   "output_path": "04d-rnn.ipynb",
   "parameters": {},
   "start_time": "2024-11-24T16:39:08.916785",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}