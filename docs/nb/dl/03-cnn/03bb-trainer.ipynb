{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8402960d",
   "metadata": {
    "papermill": {
     "duration": 0.004082,
     "end_time": "2024-10-07T02:24:55.381732",
     "exception": false,
     "start_time": "2024-10-07T02:24:55.377650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(dl/03-cnn/03bb-trainer)=\n",
    "# Trainer engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bcf0d7",
   "metadata": {
    "papermill": {
     "duration": 0.005944,
     "end_time": "2024-10-07T02:24:55.390681",
     "exception": false,
     "start_time": "2024-10-07T02:24:55.384737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To separate concerns during model training, we define a **trainer engine**. For example, this defines an `eval_context` to automatically set the model to eval mode at entry, and back to the default train mode at exit. This is useful for layers such as BN and Dropout which have different behaviors at train and test times. LR schedulers and callbacks are also implemented. Currently, these are called at the end of each training step (it is easy to extend this class to implement epoch end callbacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfa8b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:24:55.398654Z",
     "iopub.status.busy": "2024-10-07T02:24:55.398159Z",
     "iopub.status.idle": "2024-10-07T02:24:57.414780Z",
     "shell.execute_reply": "2024-10-07T02:24:57.414095Z"
    },
    "papermill": {
     "duration": 2.023605,
     "end_time": "2024-10-07T02:24:57.417352",
     "exception": false,
     "start_time": "2024-10-07T02:24:55.393747",
     "status": "completed"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from chapter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42787bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:24:57.422785Z",
     "iopub.status.busy": "2024-10-07T02:24:57.422504Z",
     "iopub.status.idle": "2024-10-07T02:24:57.551567Z",
     "shell.execute_reply": "2024-10-07T02:24:57.551016Z"
    },
    "papermill": {
     "duration": 0.134583,
     "end_time": "2024-10-07T02:24:57.554188",
     "exception": false,
     "start_time": "2024-10-07T02:24:57.419605",
     "status": "completed"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">tqdm.notebook</span> <span class=\"kn\">import</span> <span class=\"n\">tqdm</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">contextlib</span> <span class=\"kn\">import</span> <span class=\"n\">contextmanager</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n",
       "\n",
       "\n",
       "<span class=\"nd\">@contextmanager</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">eval_context</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Temporarily set to eval mode inside context.&quot;&quot;&quot;</span>\n",
       "    <span class=\"n\">is_train</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">training</span>\n",
       "    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n",
       "    <span class=\"k\">try</span><span class=\"p\">:</span>\n",
       "        <span class=\"k\">yield</span>\n",
       "    <span class=\"k\">finally</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">is_train</span><span class=\"p\">)</span>\n",
       "\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">Trainer</span><span class=\"p\">:</span>\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span>\n",
       "        <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optim</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[],</span>\n",
       "        <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">DEVICE</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span>\n",
       "    <span class=\"p\">):</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optim</span> <span class=\"o\">=</span> <span class=\"n\">optim</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">device</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_log</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">:</span> <span class=\"p\">[],</span> <span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[],</span> <span class=\"s2\">&quot;loss_avg&quot;</span><span class=\"p\">:</span> <span class=\"p\">[],</span> <span class=\"s2\">&quot;accs_avg&quot;</span><span class=\"p\">:</span> <span class=\"p\">[]}</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">valid_log</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">:</span> <span class=\"p\">[],</span> <span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[]}</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">verbose</span> <span class=\"o\">=</span> <span class=\"n\">verbose</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">scheduler</span> <span class=\"o\">=</span> <span class=\"n\">scheduler</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">callbacks</span> <span class=\"o\">=</span> <span class=\"n\">callbacks</span>\n",
       "    \n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n",
       "        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">))</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">batch</span>\n",
       "        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">y</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">train_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">preds</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">forward</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">accs</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">preds</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">preds</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n",
       "        <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">:</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">:</span> <span class=\"n\">accs</span><span class=\"p\">}</span>\n",
       "\n",
       "    <span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">inference_mode</span><span class=\"p\">()</span>\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">valid_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">preds</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">forward</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">accs</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">preds</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">preds</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">reduction</span><span class=\"o\">=</span><span class=\"s2\">&quot;sum&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">:</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">:</span> <span class=\"n\">accs</span><span class=\"p\">}</span>\n",
       "    \n",
       "    <span class=\"k\">def</span> <span class=\"nf\">run</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">train_loader</span><span class=\"p\">,</span> <span class=\"n\">valid_loader</span><span class=\"p\">,</span> <span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n",
       "        <span class=\"k\">for</span> <span class=\"n\">e</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">)):</span>\n",
       "            <span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">train_loader</span><span class=\"p\">:</span>\n",
       "                <span class=\"c1\"># optim and lr step</span>\n",
       "                <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_step</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n",
       "                <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">scheduler</span><span class=\"p\">:</span>\n",
       "                    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">scheduler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n",
       "\n",
       "                <span class=\"c1\"># step callbacks</span>\n",
       "                <span class=\"k\">for</span> <span class=\"n\">callback</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"p\">:</span>\n",
       "                    <span class=\"n\">callback</span><span class=\"p\">()</span>\n",
       "\n",
       "                <span class=\"c1\"># logs @ train step</span>\n",
       "                <span class=\"n\">steps_per_epoch</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">)</span>\n",
       "                <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mf\">0.05</span> <span class=\"o\">*</span> <span class=\"n\">steps_per_epoch</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">window_size</span> <span class=\"k\">else</span> <span class=\"n\">window_size</span>\n",
       "                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_log</span><span class=\"p\">[</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">())</span>\n",
       "                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_log</span><span class=\"p\">[</span><span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">())</span>\n",
       "                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_log</span><span class=\"p\">[</span><span class=\"s2\">&quot;loss_avg&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_log</span><span class=\"p\">[</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">][</span><span class=\"o\">-</span><span class=\"n\">w</span><span class=\"p\">:]))</span>\n",
       "                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_log</span><span class=\"p\">[</span><span class=\"s2\">&quot;accs_avg&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_log</span><span class=\"p\">[</span><span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">][</span><span class=\"o\">-</span><span class=\"n\">w</span><span class=\"p\">:]))</span>\n",
       "\n",
       "            <span class=\"c1\"># logs @ epoch</span>\n",
       "            <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">evaluate</span><span class=\"p\">(</span><span class=\"n\">valid_loader</span><span class=\"p\">)</span>\n",
       "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">valid_log</span><span class=\"p\">[</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">])</span>\n",
       "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">valid_log</span><span class=\"p\">[</span><span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">])</span>\n",
       "            <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">verbose</span><span class=\"p\">:</span>\n",
       "                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;[Epoch: </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">:</span><span class=\"s2\">&gt;0</span><span class=\"si\">{</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">)))</span><span class=\"si\">}</span><span class=\"s2\">d</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">epochs</span><span class=\"si\">}</span><span class=\"s2\">]    loss: </span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_log</span><span class=\"p\">[</span><span class=\"s1\">&#39;loss_avg&#39;</span><span class=\"p\">][</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"si\">:</span><span class=\"s2\">.4f</span><span class=\"si\">}</span><span class=\"s2\">  acc: </span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">train_log</span><span class=\"p\">[</span><span class=\"s1\">&#39;accs_avg&#39;</span><span class=\"p\">][</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"si\">:</span><span class=\"s2\">.4f</span><span class=\"si\">}</span><span class=\"s2\">    val_loss: </span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">valid_log</span><span class=\"p\">[</span><span class=\"s1\">&#39;loss&#39;</span><span class=\"p\">][</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"si\">:</span><span class=\"s2\">.4f</span><span class=\"si\">}</span><span class=\"s2\">  val_acc: </span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">valid_log</span><span class=\"p\">[</span><span class=\"s1\">&#39;accs&#39;</span><span class=\"p\">][</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"si\">:</span><span class=\"s2\">.4f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">evaluate</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">data_loader</span><span class=\"p\">):</span>\n",
       "        <span class=\"k\">with</span> <span class=\"n\">eval_context</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">):</span>\n",
       "            <span class=\"n\">valid_loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n",
       "            <span class=\"n\">valid_accs</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n",
       "            <span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">data_loader</span><span class=\"p\">:</span>\n",
       "                <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">valid_step</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n",
       "                <span class=\"n\">valid_loss</span> <span class=\"o\">+=</span> <span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n",
       "                <span class=\"n\">valid_accs</span> <span class=\"o\">+=</span> <span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n",
       "\n",
       "        <span class=\"k\">return</span> <span class=\"p\">{</span>\n",
       "            <span class=\"s2\">&quot;loss&quot;</span><span class=\"p\">:</span> <span class=\"n\">valid_loss</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">data_loader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">),</span>\n",
       "            <span class=\"s2\">&quot;accs&quot;</span><span class=\"p\">:</span> <span class=\"n\">valid_accs</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">data_loader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n",
       "        <span class=\"p\">}</span>\n",
       "\n",
       "    <span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">inference_mode</span><span class=\"p\">()</span>\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">):</span>\n",
       "        <span class=\"k\">with</span> <span class=\"n\">eval_context</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">):</span>\n",
       "            <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{tqdm}\\PY{n+nn}{.}\\PY{n+nn}{notebook} \\PY{k+kn}{import} \\PY{n}{tqdm}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{contextlib} \\PY{k+kn}{import} \\PY{n}{contextmanager}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data} \\PY{k+kn}{import} \\PY{n}{DataLoader}\n",
       "\n",
       "\n",
       "\\PY{n+nd}{@contextmanager}\n",
       "\\PY{k}{def} \\PY{n+nf}{eval\\PYZus{}context}\\PY{p}{(}\\PY{n}{model}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{    }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Temporarily set to eval mode inside context.\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "    \\PY{n}{is\\PYZus{}train} \\PY{o}{=} \\PY{n}{model}\\PY{o}{.}\\PY{n}{training}\n",
       "    \\PY{n}{model}\\PY{o}{.}\\PY{n}{eval}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{k}{try}\\PY{p}{:}\n",
       "        \\PY{k}{yield}\n",
       "    \\PY{k}{finally}\\PY{p}{:}\n",
       "        \\PY{n}{model}\\PY{o}{.}\\PY{n}{train}\\PY{p}{(}\\PY{n}{is\\PYZus{}train}\\PY{p}{)}\n",
       "\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{Trainer}\\PY{p}{:}\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,}\n",
       "        \\PY{n}{model}\\PY{p}{,} \\PY{n}{optim}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{scheduler}\\PY{o}{=}\\PY{k+kc}{None}\\PY{p}{,} \\PY{n}{callbacks}\\PY{o}{=}\\PY{p}{[}\\PY{p}{]}\\PY{p}{,}\n",
       "        \\PY{n}{device}\\PY{o}{=}\\PY{n}{DEVICE}\\PY{p}{,} \\PY{n}{verbose}\\PY{o}{=}\\PY{k+kc}{True}\n",
       "    \\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model} \\PY{o}{=} \\PY{n}{model}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{device}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{optim} \\PY{o}{=} \\PY{n}{optim}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{device} \\PY{o}{=} \\PY{n}{device}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{loss\\PYZus{}fn} \\PY{o}{=} \\PY{n}{loss\\PYZus{}fn}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}log} \\PY{o}{=} \\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{p}{[}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{p}{[}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss\\PYZus{}avg}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{p}{[}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs\\PYZus{}avg}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{p}{[}\\PY{p}{]}\\PY{p}{\\PYZcb{}}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{valid\\PYZus{}log} \\PY{o}{=} \\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{p}{[}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{p}{[}\\PY{p}{]}\\PY{p}{\\PYZcb{}}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{verbose} \\PY{o}{=} \\PY{n}{verbose}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{scheduler} \\PY{o}{=} \\PY{n}{scheduler}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{callbacks} \\PY{o}{=} \\PY{n}{callbacks}\n",
       "    \n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}call\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{k}{return} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model}\\PY{p}{(}\\PY{n}{x}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{device}\\PY{p}{)}\\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{forward}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{batch}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{x}\\PY{p}{,} \\PY{n}{y} \\PY{o}{=} \\PY{n}{batch}\n",
       "        \\PY{n}{x} \\PY{o}{=} \\PY{n}{x}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{device}\\PY{p}{)}\n",
       "        \\PY{n}{y} \\PY{o}{=} \\PY{n}{y}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{device}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\\PY{p}{,} \\PY{n}{y}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{train\\PYZus{}step}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{batch}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{preds}\\PY{p}{,} \\PY{n}{y} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{forward}\\PY{p}{(}\\PY{n}{batch}\\PY{p}{)}\n",
       "        \\PY{n}{accs} \\PY{o}{=} \\PY{p}{(}\\PY{n}{preds}\\PY{o}{.}\\PY{n}{argmax}\\PY{p}{(}\\PY{n}{dim}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{==} \\PY{n}{y}\\PY{p}{)}\\PY{o}{.}\\PY{n}{float}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{mean}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{loss} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{preds}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)}\n",
       "        \\PY{n}{loss}\\PY{o}{.}\\PY{n}{backward}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{zero\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{loss}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{accs}\\PY{p}{\\PYZcb{}}\n",
       "\n",
       "    \\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{inference\\PYZus{}mode}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{k}{def} \\PY{n+nf}{valid\\PYZus{}step}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{batch}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{preds}\\PY{p}{,} \\PY{n}{y} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{forward}\\PY{p}{(}\\PY{n}{batch}\\PY{p}{)}\n",
       "        \\PY{n}{accs} \\PY{o}{=} \\PY{p}{(}\\PY{n}{preds}\\PY{o}{.}\\PY{n}{argmax}\\PY{p}{(}\\PY{n}{dim}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{==} \\PY{n}{y}\\PY{p}{)}\\PY{o}{.}\\PY{n}{float}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{sum}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{loss} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{preds}\\PY{p}{,} \\PY{n}{y}\\PY{p}{,} \\PY{n}{reduction}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{sum}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{loss}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{accs}\\PY{p}{\\PYZcb{}}\n",
       "    \n",
       "    \\PY{k}{def} \\PY{n+nf}{run}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{epochs}\\PY{p}{,} \\PY{n}{train\\PYZus{}loader}\\PY{p}{,} \\PY{n}{valid\\PYZus{}loader}\\PY{p}{,} \\PY{n}{window\\PYZus{}size}\\PY{o}{=}\\PY{k+kc}{None}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{k}{for} \\PY{n}{e} \\PY{o+ow}{in} \\PY{n}{tqdm}\\PY{p}{(}\\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{epochs}\\PY{p}{)}\\PY{p}{)}\\PY{p}{:}\n",
       "            \\PY{k}{for} \\PY{n}{batch} \\PY{o+ow}{in} \\PY{n}{train\\PYZus{}loader}\\PY{p}{:}\n",
       "                \\PY{c+c1}{\\PYZsh{} optim and lr step}\n",
       "                \\PY{n}{output} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}step}\\PY{p}{(}\\PY{n}{batch}\\PY{p}{)}\n",
       "                \\PY{k}{if} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{scheduler}\\PY{p}{:}\n",
       "                    \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{scheduler}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "                \\PY{c+c1}{\\PYZsh{} step callbacks}\n",
       "                \\PY{k}{for} \\PY{n}{callback} \\PY{o+ow}{in} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{callbacks}\\PY{p}{:}\n",
       "                    \\PY{n}{callback}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "                \\PY{c+c1}{\\PYZsh{} logs @ train step}\n",
       "                \\PY{n}{steps\\PYZus{}per\\PYZus{}epoch} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{train\\PYZus{}loader}\\PY{p}{)}\n",
       "                \\PY{n}{w} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{l+m+mf}{0.05} \\PY{o}{*} \\PY{n}{steps\\PYZus{}per\\PYZus{}epoch}\\PY{p}{)} \\PY{k}{if} \\PY{o+ow}{not} \\PY{n}{window\\PYZus{}size} \\PY{k}{else} \\PY{n}{window\\PYZus{}size}\n",
       "                \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{append}\\PY{p}{(}\\PY{n}{output}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\n",
       "                \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{append}\\PY{p}{(}\\PY{n}{output}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\n",
       "                \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss\\PYZus{}avg}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{append}\\PY{p}{(}\\PY{n}{np}\\PY{o}{.}\\PY{n}{mean}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{n}{w}\\PY{p}{:}\\PY{p}{]}\\PY{p}{)}\\PY{p}{)}\n",
       "                \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs\\PYZus{}avg}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{append}\\PY{p}{(}\\PY{n}{np}\\PY{o}{.}\\PY{n}{mean}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{n}{w}\\PY{p}{:}\\PY{p}{]}\\PY{p}{)}\\PY{p}{)}\n",
       "\n",
       "            \\PY{c+c1}{\\PYZsh{} logs @ epoch}\n",
       "            \\PY{n}{output} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{evaluate}\\PY{p}{(}\\PY{n}{valid\\PYZus{}loader}\\PY{p}{)}\n",
       "            \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{valid\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{append}\\PY{p}{(}\\PY{n}{output}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "            \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{valid\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{append}\\PY{p}{(}\\PY{n}{output}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "            \\PY{k}{if} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{verbose}\\PY{p}{:}\n",
       "                \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{[Epoch: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{e}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}0}\\PY{l+s+si}{\\PYZob{}}\\PY{n+nb}{int}\\PY{p}{(}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n+nb}{str}\\PY{p}{(}\\PY{n}{epochs}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epochs}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{]    loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{loss\\PYZus{}avg}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{l+s+si}{:}\\PY{l+s+s2}{.4f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{  acc: }\\PY{l+s+si}{\\PYZob{}}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{train\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{accs\\PYZus{}avg}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{l+s+si}{:}\\PY{l+s+s2}{.4f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{    val\\PYZus{}loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{valid\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{loss}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{l+s+si}{:}\\PY{l+s+s2}{.4f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{  val\\PYZus{}acc: }\\PY{l+s+si}{\\PYZob{}}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{valid\\PYZus{}log}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{accs}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{l+s+si}{:}\\PY{l+s+s2}{.4f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{evaluate}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{data\\PYZus{}loader}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{k}{with} \\PY{n}{eval\\PYZus{}context}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model}\\PY{p}{)}\\PY{p}{:}\n",
       "            \\PY{n}{valid\\PYZus{}loss} \\PY{o}{=} \\PY{l+m+mf}{0.0}\n",
       "            \\PY{n}{valid\\PYZus{}accs} \\PY{o}{=} \\PY{l+m+mf}{0.0}\n",
       "            \\PY{k}{for} \\PY{n}{batch} \\PY{o+ow}{in} \\PY{n}{data\\PYZus{}loader}\\PY{p}{:}\n",
       "                \\PY{n}{output} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{valid\\PYZus{}step}\\PY{p}{(}\\PY{n}{batch}\\PY{p}{)}\n",
       "                \\PY{n}{valid\\PYZus{}loss} \\PY{o}{+}\\PY{o}{=} \\PY{n}{output}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\n",
       "                \\PY{n}{valid\\PYZus{}accs} \\PY{o}{+}\\PY{o}{=} \\PY{n}{output}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "        \\PY{k}{return} \\PY{p}{\\PYZob{}}\n",
       "            \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{valid\\PYZus{}loss} \\PY{o}{/} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{data\\PYZus{}loader}\\PY{o}{.}\\PY{n}{dataset}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{accs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{valid\\PYZus{}accs} \\PY{o}{/} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{data\\PYZus{}loader}\\PY{o}{.}\\PY{n}{dataset}\\PY{p}{)}\n",
       "        \\PY{p}{\\PYZcb{}}\n",
       "\n",
       "    \\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{inference\\PYZus{}mode}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{k}{def} \\PY{n+nf}{predict}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{x}\\PY{p}{:} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{Tensor}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{k}{with} \\PY{n}{eval\\PYZus{}context}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model}\\PY{p}{)}\\PY{p}{:}\n",
       "            \\PY{k}{return} \\PY{n+nb+bp}{self}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "from tqdm.notebook import tqdm\n",
       "from contextlib import contextmanager\n",
       "from torch.utils.data import DataLoader\n",
       "\n",
       "\n",
       "@contextmanager\n",
       "def eval_context(model):\n",
       "    \"\"\"Temporarily set to eval mode inside context.\"\"\"\n",
       "    is_train = model.training\n",
       "    model.eval()\n",
       "    try:\n",
       "        yield\n",
       "    finally:\n",
       "        model.train(is_train)\n",
       "\n",
       "\n",
       "class Trainer:\n",
       "    def __init__(self,\n",
       "        model, optim, loss_fn, scheduler=None, callbacks=[],\n",
       "        device=DEVICE, verbose=True\n",
       "    ):\n",
       "        self.model = model.to(device)\n",
       "        self.optim = optim\n",
       "        self.device = device\n",
       "        self.loss_fn = loss_fn\n",
       "        self.train_log = {\"loss\": [], \"accs\": [], \"loss_avg\": [], \"accs_avg\": []}\n",
       "        self.valid_log = {\"loss\": [], \"accs\": []}\n",
       "        self.verbose = verbose\n",
       "        self.scheduler = scheduler\n",
       "        self.callbacks = callbacks\n",
       "    \n",
       "    def __call__(self, x):\n",
       "        return self.model(x.to(self.device))\n",
       "\n",
       "    def forward(self, batch):\n",
       "        x, y = batch\n",
       "        x = x.to(self.device)\n",
       "        y = y.to(self.device)\n",
       "        return self.model(x), y\n",
       "\n",
       "    def train_step(self, batch):\n",
       "        preds, y = self.forward(batch)\n",
       "        accs = (preds.argmax(dim=1) == y).float().mean()\n",
       "        loss = self.loss_fn(preds, y)\n",
       "        loss.backward()\n",
       "        self.optim.step()\n",
       "        self.optim.zero_grad()\n",
       "        return {\"loss\": loss, \"accs\": accs}\n",
       "\n",
       "    @torch.inference_mode()\n",
       "    def valid_step(self, batch):\n",
       "        preds, y = self.forward(batch)\n",
       "        accs = (preds.argmax(dim=1) == y).float().sum()\n",
       "        loss = self.loss_fn(preds, y, reduction=\"sum\")\n",
       "        return {\"loss\": loss, \"accs\": accs}\n",
       "    \n",
       "    def run(self, epochs, train_loader, valid_loader, window_size=None):\n",
       "        for e in tqdm(range(epochs)):\n",
       "            for batch in train_loader:\n",
       "                # optim and lr step\n",
       "                output = self.train_step(batch)\n",
       "                if self.scheduler:\n",
       "                    self.scheduler.step()\n",
       "\n",
       "                # step callbacks\n",
       "                for callback in self.callbacks:\n",
       "                    callback()\n",
       "\n",
       "                # logs @ train step\n",
       "                steps_per_epoch = len(train_loader)\n",
       "                w = int(0.05 * steps_per_epoch) if not window_size else window_size\n",
       "                self.train_log[\"loss\"].append(output[\"loss\"].item())\n",
       "                self.train_log[\"accs\"].append(output[\"accs\"].item())\n",
       "                self.train_log[\"loss_avg\"].append(np.mean(self.train_log[\"loss\"][-w:]))\n",
       "                self.train_log[\"accs_avg\"].append(np.mean(self.train_log[\"accs\"][-w:]))\n",
       "\n",
       "            # logs @ epoch\n",
       "            output = self.evaluate(valid_loader)\n",
       "            self.valid_log[\"loss\"].append(output[\"loss\"])\n",
       "            self.valid_log[\"accs\"].append(output[\"accs\"])\n",
       "            if self.verbose:\n",
       "                print(f\"[Epoch: {e+1:>0{int(len(str(epochs)))}d}/{epochs}]    loss: {self.train_log['loss_avg'][-1]:.4f}  acc: {self.train_log['accs_avg'][-1]:.4f}    val_loss: {self.valid_log['loss'][-1]:.4f}  val_acc: {self.valid_log['accs'][-1]:.4f}\")\n",
       "\n",
       "    def evaluate(self, data_loader):\n",
       "        with eval_context(self.model):\n",
       "            valid_loss = 0.0\n",
       "            valid_accs = 0.0\n",
       "            for batch in data_loader:\n",
       "                output = self.valid_step(batch)\n",
       "                valid_loss += output[\"loss\"].item()\n",
       "                valid_accs += output[\"accs\"].item()\n",
       "\n",
       "        return {\n",
       "            \"loss\": valid_loss / len(data_loader.dataset),\n",
       "            \"accs\": valid_accs / len(data_loader.dataset)\n",
       "        }\n",
       "\n",
       "    @torch.inference_mode()\n",
       "    def predict(self, x: torch.Tensor):\n",
       "        with eval_context(self.model):\n",
       "            return self(x)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%save\n",
    "from tqdm.notebook import tqdm\n",
    "from contextlib import contextmanager\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def eval_context(model):\n",
    "    \"\"\"Temporarily set to eval mode inside context.\"\"\"\n",
    "    is_train = model.training\n",
    "    model.eval()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        model.train(is_train)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "        model, optim, loss_fn, scheduler=None, callbacks=[],\n",
    "        device=DEVICE, verbose=True\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.optim = optim\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_log = {\"loss\": [], \"accs\": [], \"loss_avg\": [], \"accs_avg\": []}\n",
    "        self.valid_log = {\"loss\": [], \"accs\": []}\n",
    "        self.verbose = verbose\n",
    "        self.scheduler = scheduler\n",
    "        self.callbacks = callbacks\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.model(x.to(self.device))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, y = batch\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        return self.model(x), y\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        preds, y = self.forward(batch)\n",
    "        accs = (preds.argmax(dim=1) == y).float().mean()\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "        return {\"loss\": loss, \"accs\": accs}\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def valid_step(self, batch):\n",
    "        preds, y = self.forward(batch)\n",
    "        accs = (preds.argmax(dim=1) == y).float().sum()\n",
    "        loss = self.loss_fn(preds, y, reduction=\"sum\")\n",
    "        return {\"loss\": loss, \"accs\": accs}\n",
    "    \n",
    "    def run(self, epochs, train_loader, valid_loader, window_size=None):\n",
    "        for e in tqdm(range(epochs)):\n",
    "            for batch in train_loader:\n",
    "                # optim and lr step\n",
    "                output = self.train_step(batch)\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                # step callbacks\n",
    "                for callback in self.callbacks:\n",
    "                    callback()\n",
    "\n",
    "                # logs @ train step\n",
    "                steps_per_epoch = len(train_loader)\n",
    "                w = int(0.05 * steps_per_epoch) if not window_size else window_size\n",
    "                self.train_log[\"loss\"].append(output[\"loss\"].item())\n",
    "                self.train_log[\"accs\"].append(output[\"accs\"].item())\n",
    "                self.train_log[\"loss_avg\"].append(np.mean(self.train_log[\"loss\"][-w:]))\n",
    "                self.train_log[\"accs_avg\"].append(np.mean(self.train_log[\"accs\"][-w:]))\n",
    "\n",
    "            # logs @ epoch\n",
    "            output = self.evaluate(valid_loader)\n",
    "            self.valid_log[\"loss\"].append(output[\"loss\"])\n",
    "            self.valid_log[\"accs\"].append(output[\"accs\"])\n",
    "            if self.verbose:\n",
    "                print(f\"[Epoch: {e+1:>0{int(len(str(epochs)))}d}/{epochs}]    loss: {self.train_log['loss_avg'][-1]:.4f}  acc: {self.train_log['accs_avg'][-1]:.4f}    val_loss: {self.valid_log['loss'][-1]:.4f}  val_acc: {self.valid_log['accs'][-1]:.4f}\")\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        with eval_context(self.model):\n",
    "            valid_loss = 0.0\n",
    "            valid_accs = 0.0\n",
    "            for batch in data_loader:\n",
    "                output = self.valid_step(batch)\n",
    "                valid_loss += output[\"loss\"].item()\n",
    "                valid_accs += output[\"accs\"].item()\n",
    "\n",
    "        return {\n",
    "            \"loss\": valid_loss / len(data_loader.dataset),\n",
    "            \"accs\": valid_accs / len(data_loader.dataset)\n",
    "        }\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, x: torch.Tensor):\n",
    "        with eval_context(self.model):\n",
    "            return self(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8ab7f",
   "metadata": {
    "papermill": {
     "duration": 0.002255,
     "end_time": "2024-10-07T02:24:57.559161",
     "exception": false,
     "start_time": "2024-10-07T02:24:57.556906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `predict` method is suited for inference over *one* transformed mini-batch. A model call over a large input tensor may cause memory error. The model does not generate a computational graph to conserve memory and calls the model with layers in eval mode. For large batches, one should use `batch_predict` which is the same but takes in a data loader with transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3190b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:24:57.566487Z",
     "iopub.status.busy": "2024-10-07T02:24:57.566184Z",
     "iopub.status.idle": "2024-10-07T02:24:59.379384Z",
     "shell.execute_reply": "2024-10-07T02:24:59.379009Z"
    },
    "papermill": {
     "duration": 1.818373,
     "end_time": "2024-10-07T02:24:59.380625",
     "exception": false,
     "start_time": "2024-10-07T02:24:57.562252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__call__    0.000\n",
      "predict     0.500\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(3, 10), nn.Dropout(1.0))\n",
    "trainer = Trainer(model, optim=None, scheduler=None, loss_fn=None)\n",
    "\n",
    "# inference mode using eval_context\n",
    "x = torch.ones(size=(1, 3), requires_grad=True)\n",
    "print(f\"__call__    {(trainer(x) > 0).float().mean():.3f}\")\n",
    "print(f\"predict     {(trainer.predict(x) > 0).float().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fcdbe6",
   "metadata": {
    "papermill": {
     "duration": 0.002026,
     "end_time": "2024-10-07T02:24:59.385335",
     "exception": false,
     "start_time": "2024-10-07T02:24:59.383309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Checking computational graph generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a75784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:24:59.389688Z",
     "iopub.status.busy": "2024-10-07T02:24:59.389474Z",
     "iopub.status.idle": "2024-10-07T02:24:59.393506Z",
     "shell.execute_reply": "2024-10-07T02:24:59.393084Z"
    },
    "papermill": {
     "duration": 0.007575,
     "end_time": "2024-10-07T02:24:59.394664",
     "exception": false,
     "start_time": "2024-10-07T02:24:59.387089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__call__    True\n",
      "predict     False\n"
     ]
    }
   ],
   "source": [
    "y = trainer(x)\n",
    "z = trainer.predict(x)\n",
    "print(\"__call__   \", y.requires_grad)\n",
    "print(\"predict    \", z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70d00e",
   "metadata": {
    "papermill": {
     "duration": 0.001735,
     "end_time": "2024-10-07T02:24:59.398359",
     "exception": false,
     "start_time": "2024-10-07T02:24:59.396624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.546444,
   "end_time": "2024-10-07T02:25:00.022324",
   "environment_variables": {},
   "exception": null,
   "input_path": "03bb-trainer.ipynb",
   "output_path": "03bb-trainer.ipynb",
   "parameters": {},
   "start_time": "2024-10-07T02:24:54.475880",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
