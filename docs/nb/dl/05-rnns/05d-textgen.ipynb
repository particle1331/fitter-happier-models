{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e169dd1",
   "metadata": {
    "papermill": {
     "duration": 0.006669,
     "end_time": "2024-12-31T05:32:55.884767",
     "exception": false,
     "start_time": "2024-12-31T05:32:55.878098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def72d65",
   "metadata": {
    "papermill": {
     "duration": 0.003185,
     "end_time": "2024-12-31T05:32:55.892540",
     "exception": false,
     "start_time": "2024-12-31T05:32:55.889355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Recall that the state vector is initialized as zero. So we use a **warmup context** or a **prompt** to allow the RNN cell to update its state iteratively by processing one character at a time from the warmup text. Then, the algorithm simulates the prediction process of our RNN language model, but instead of using a predefined input sequence, it uses the *previous output* as the next input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345110a1",
   "metadata": {
    "papermill": {
     "duration": 0.002482,
     "end_time": "2024-12-31T05:32:55.898090",
     "exception": false,
     "start_time": "2024-12-31T05:32:55.895608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "```{figure} ../../../img/nn/04-rnn-textgen.png\n",
    "---\n",
    "width: 500px\n",
    "name: 04-rnn-textgen\n",
    "align: center\n",
    "---\n",
    "An input sequence is used to get a final state vector (this is the warmup stage, i.e. the state goes from zero to some nonzero vector). The final character and state during warmup is used to predict the next character. This process is repeated until the number of predicted tokens is reached.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd26308a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T05:32:55.903852Z",
     "iopub.status.busy": "2024-12-31T05:32:55.903563Z",
     "iopub.status.idle": "2024-12-31T05:32:56.550997Z",
     "shell.execute_reply": "2024-12-31T05:32:56.550653Z"
    },
    "papermill": {
     "duration": 0.652099,
     "end_time": "2024-12-31T05:32:56.552423",
     "exception": false,
     "start_time": "2024-12-31T05:32:55.900324",
     "status": "completed"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from chapter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c068c",
   "metadata": {
    "papermill": {
     "duration": 0.001693,
     "end_time": "2024-12-31T05:32:56.556024",
     "exception": false,
     "start_time": "2024-12-31T05:32:56.554331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the trained RNN language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9832e1fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T05:32:56.560434Z",
     "iopub.status.busy": "2024-12-31T05:32:56.560225Z",
     "iopub.status.idle": "2024-12-31T05:32:56.612700Z",
     "shell.execute_reply": "2024-12-31T05:32:56.612356Z"
    },
    "papermill": {
     "duration": 0.055592,
     "end_time": "2024-12-31T05:32:56.613659",
     "exception": false,
     "start_time": "2024-12-31T05:32:56.558067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"  # faster for RNN inference\n",
    "WEIGHTS_PATH = \"./artifacts/rnn_lm.pkl\"\n",
    "_, vocab = TimeMachine().build()\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "model = LanguageModel(RNN)(VOCAB_SIZE, 64, VOCAB_SIZE)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=DEVICE));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432733cf",
   "metadata": {
    "papermill": {
     "duration": 0.001056,
     "end_time": "2024-12-31T05:32:56.615922",
     "exception": false,
     "start_time": "2024-12-31T05:32:56.614866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Text generation utils and algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d130f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T05:32:56.618552Z",
     "iopub.status.busy": "2024-12-31T05:32:56.618436Z",
     "iopub.status.idle": "2024-12-31T05:32:56.646131Z",
     "shell.execute_reply": "2024-12-31T05:32:56.645830Z"
    },
    "papermill": {
     "duration": 0.030491,
     "end_time": "2024-12-31T05:32:56.647423",
     "exception": false,
     "start_time": "2024-12-31T05:32:56.616932",
     "status": "completed"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">TextGenerator</span><span class=\"p\">:</span>\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">vocab</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cpu&quot;</span><span class=\"p\">):</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">vocab</span> <span class=\"o\">=</span> <span class=\"n\">vocab</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">device</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">_inp</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">indices</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Preprocess indices (T,) to (T, 1, V) mini-batch shape with batch_size=1.&quot;&quot;&quot;</span>\n",
       "        <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">one_hot</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">indices</span><span class=\"p\">),</span> <span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">()</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"nd\">@staticmethod</span>\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">sample_token</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">temp</span><span class=\"p\">:</span> <span class=\"nb\">float</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Sample based on logits with softmax temperature.&quot;&quot;&quot;</span>\n",
       "        <span class=\"c1\"># higher temp =&gt; more uniform, i.e. exp ~ 1</span>\n",
       "        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">logits</span> <span class=\"o\">/</span> <span class=\"n\">temp</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">multinomial</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">num_samples</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">num_preds</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">temp</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Simulate character generation one at a time.&quot;&quot;&quot;</span>\n",
       "\n",
       "        <span class=\"c1\"># Iterate over warmup text. RNN cell outputs final state</span>\n",
       "        <span class=\"n\">warmup_indices</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">prompt</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">())]</span>\n",
       "        <span class=\"n\">outs</span><span class=\"p\">,</span> <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_inp</span><span class=\"p\">(</span><span class=\"n\">warmup_indices</span><span class=\"p\">),</span> <span class=\"n\">return_state</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"c1\"># Next token sampling and state update</span>\n",
       "        <span class=\"n\">indices</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
       "        <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_preds</span><span class=\"p\">):</span>\n",
       "            <span class=\"n\">i</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">sample_token</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"o\">=</span><span class=\"n\">outs</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">temp</span><span class=\"o\">=</span><span class=\"n\">temp</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">indices</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">outs</span><span class=\"p\">,</span> <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_inp</span><span class=\"p\">([</span><span class=\"n\">i</span><span class=\"p\">]),</span> <span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"n\">return_state</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"k\">return</span> <span class=\"s2\">&quot;&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">to_tokens</span><span class=\"p\">(</span><span class=\"n\">warmup_indices</span> <span class=\"o\">+</span> <span class=\"n\">indices</span><span class=\"p\">))</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{functional} \\PY{k}{as} \\PY{n+nn}{F}\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{TextGenerator}\\PY{p}{:}\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{vocab}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cpu}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model} \\PY{o}{=} \\PY{n}{model}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{device}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{vocab} \\PY{o}{=} \\PY{n}{vocab}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{device} \\PY{o}{=} \\PY{n}{device}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{\\PYZus{}inp}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{indices}\\PY{p}{:} \\PY{n+nb}{list}\\PY{p}{[}\\PY{n+nb}{int}\\PY{p}{]}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Preprocess indices (T,) to (T, 1, V) mini\\PYZhy{}batch shape with batch\\PYZus{}size=1.\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \\PY{n}{n} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{vocab}\\PY{p}{)}\n",
       "        \\PY{n}{x} \\PY{o}{=} \\PY{n}{F}\\PY{o}{.}\\PY{n}{one\\PYZus{}hot}\\PY{p}{(}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{tensor}\\PY{p}{(}\\PY{n}{indices}\\PY{p}{)}\\PY{p}{,} \\PY{n}{n}\\PY{p}{)}\\PY{o}{.}\\PY{n}{float}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n}{x}\\PY{o}{.}\\PY{n}{view}\\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{n}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{device}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n+nd}{@staticmethod}\n",
       "    \\PY{k}{def} \\PY{n+nf}{sample\\PYZus{}token}\\PY{p}{(}\\PY{n}{logits}\\PY{p}{,} \\PY{n}{temp}\\PY{p}{:} \\PY{n+nb}{float}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Sample based on logits with softmax temperature.\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \\PY{c+c1}{\\PYZsh{} higher temp =\\PYZgt{} more uniform, i.e. exp \\PYZti{} 1}\n",
       "        \\PY{n}{p} \\PY{o}{=} \\PY{n}{F}\\PY{o}{.}\\PY{n}{softmax}\\PY{p}{(}\\PY{n}{logits} \\PY{o}{/} \\PY{n}{temp}\\PY{p}{,} \\PY{n}{dim}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{multinomial}\\PY{p}{(}\\PY{n}{p}\\PY{p}{,} \\PY{n}{num\\PYZus{}samples}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{)}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{predict}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{prompt}\\PY{p}{:} \\PY{n+nb}{str}\\PY{p}{,} \\PY{n}{num\\PYZus{}preds}\\PY{p}{:} \\PY{n+nb}{int}\\PY{p}{,} \\PY{n}{temp}\\PY{o}{=}\\PY{l+m+mf}{1.0}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Simulate character generation one at a time.\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Iterate over warmup text. RNN cell outputs final state}\n",
       "        \\PY{n}{warmup\\PYZus{}indices} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{vocab}\\PY{p}{[}\\PY{n+nb}{list}\\PY{p}{(}\\PY{n}{prompt}\\PY{o}{.}\\PY{n}{lower}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\\PY{p}{]}\n",
       "        \\PY{n}{outs}\\PY{p}{,} \\PY{n}{state} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}inp}\\PY{p}{(}\\PY{n}{warmup\\PYZus{}indices}\\PY{p}{)}\\PY{p}{,} \\PY{n}{return\\PYZus{}state}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Next token sampling and state update}\n",
       "        \\PY{n}{indices} \\PY{o}{=} \\PY{p}{[}\\PY{p}{]}\n",
       "        \\PY{k}{for} \\PY{n}{\\PYZus{}} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{num\\PYZus{}preds}\\PY{p}{)}\\PY{p}{:}\n",
       "            \\PY{n}{i} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{sample\\PYZus{}token}\\PY{p}{(}\\PY{n}{logits}\\PY{o}{=}\\PY{n}{outs}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{p}{,} \\PY{n}{temp}\\PY{o}{=}\\PY{n}{temp}\\PY{p}{)}\n",
       "            \\PY{n}{indices}\\PY{o}{.}\\PY{n}{append}\\PY{p}{(}\\PY{n}{i}\\PY{p}{)}\n",
       "            \\PY{n}{outs}\\PY{p}{,} \\PY{n}{state} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}inp}\\PY{p}{(}\\PY{p}{[}\\PY{n}{i}\\PY{p}{]}\\PY{p}{)}\\PY{p}{,} \\PY{n}{state}\\PY{p}{,} \\PY{n}{return\\PYZus{}state}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{k}{return} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{vocab}\\PY{o}{.}\\PY{n}{to\\PYZus{}tokens}\\PY{p}{(}\\PY{n}{warmup\\PYZus{}indices} \\PY{o}{+} \\PY{n}{indices}\\PY{p}{)}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "import torch\n",
       "import torch.nn.functional as F\n",
       "\n",
       "class TextGenerator:\n",
       "    def __init__(self, model, vocab, device=\"cpu\"):\n",
       "        self.model = model.to(device)\n",
       "        self.vocab = vocab\n",
       "        self.device = device\n",
       "\n",
       "    def _inp(self, indices: list[int]):\n",
       "        \"\"\"Preprocess indices (T,) to (T, 1, V) mini-batch shape with batch_size=1.\"\"\"\n",
       "        n = len(self.vocab)\n",
       "        x = F.one_hot(torch.tensor(indices), n).float()\n",
       "        return x.view(-1, 1, n).to(self.device)\n",
       "\n",
       "    @staticmethod\n",
       "    def sample_token(logits, temp: float):\n",
       "        \"\"\"Sample based on logits with softmax temperature.\"\"\"\n",
       "        # higher temp => more uniform, i.e. exp ~ 1\n",
       "        p = F.softmax(logits / temp, dim=1)\n",
       "        return torch.multinomial(p, num_samples=1).item()\n",
       "\n",
       "    def predict(self, prompt: str, num_preds: int, temp=1.0):\n",
       "        \"\"\"Simulate character generation one at a time.\"\"\"\n",
       "\n",
       "        # Iterate over warmup text. RNN cell outputs final state\n",
       "        warmup_indices = self.vocab[list(prompt.lower())]\n",
       "        outs, state = self.model(self._inp(warmup_indices), return_state=True)\n",
       "\n",
       "        # Next token sampling and state update\n",
       "        indices = []\n",
       "        for _ in range(num_preds):\n",
       "            i = self.sample_token(logits=outs[-1], temp=temp)\n",
       "            indices.append(i)\n",
       "            outs, state = self.model(self._inp([i]), state, return_state=True)\n",
       "        \n",
       "        return \"\".join(self.vocab.to_tokens(warmup_indices + indices))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%save\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, model, vocab, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "\n",
    "    def _inp(self, indices: list[int]):\n",
    "        \"\"\"Preprocess indices (T,) to (T, 1, V) mini-batch shape with batch_size=1.\"\"\"\n",
    "        n = len(self.vocab)\n",
    "        x = F.one_hot(torch.tensor(indices), n).float()\n",
    "        return x.view(-1, 1, n).to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_token(logits, temp: float):\n",
    "        \"\"\"Sample based on logits with softmax temperature.\"\"\"\n",
    "        # higher temp => more uniform, i.e. exp ~ 1\n",
    "        p = F.softmax(logits / temp, dim=1)\n",
    "        return torch.multinomial(p, num_samples=1).item()\n",
    "\n",
    "    def predict(self, prompt: str, num_preds: int, temp=1.0):\n",
    "        \"\"\"Simulate character generation one at a time.\"\"\"\n",
    "\n",
    "        # Iterate over warmup text. RNN cell outputs final state\n",
    "        warmup_indices = self.vocab[list(prompt.lower())]\n",
    "        outs, state = self.model(self._inp(warmup_indices), return_state=True)\n",
    "\n",
    "        # Next token sampling and state update\n",
    "        indices = []\n",
    "        for _ in range(num_preds):\n",
    "            i = self.sample_token(logits=outs[-1], temp=temp)\n",
    "            indices.append(i)\n",
    "            outs, state = self.model(self._inp([i]), state, return_state=True)\n",
    "        \n",
    "        return \"\".join(self.vocab.to_tokens(warmup_indices + indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba1dfa",
   "metadata": {
    "papermill": {
     "duration": 0.001622,
     "end_time": "2024-12-31T05:32:56.650633",
     "exception": false,
     "start_time": "2024-12-31T05:32:56.649011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Sanity test.** Completing 'thank you':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a7b406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T05:32:56.654284Z",
     "iopub.status.busy": "2024-12-31T05:32:56.654118Z",
     "iopub.status.idle": "2024-12-31T05:32:56.670558Z",
     "shell.execute_reply": "2024-12-31T05:32:56.670226Z"
    },
    "papermill": {
     "duration": 0.019336,
     "end_time": "2024-12-31T05:32:56.671484",
     "exception": false,
     "start_time": "2024-12-31T05:32:56.652148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textgen = TextGenerator(model, vocab, device=\"cpu\")\n",
    "s = [textgen.predict(\"thank y\", num_preds=2, temp=0.4) for i in range(20)]\n",
    "(np.array(s) == \"thank you\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e112370",
   "metadata": {
    "papermill": {
     "duration": 0.001308,
     "end_time": "2024-12-31T05:32:56.674358",
     "exception": false,
     "start_time": "2024-12-31T05:32:56.673050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Example.** The network can generate output given warmup prompt of arbitrary length. Here we also look at the effect of temperature on the generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3d06dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T05:32:56.678246Z",
     "iopub.status.busy": "2024-12-31T05:32:56.678097Z",
     "iopub.status.idle": "2024-12-31T05:32:56.715311Z",
     "shell.execute_reply": "2024-12-31T05:32:56.714981Z"
    },
    "papermill": {
     "duration": 0.040422,
     "end_time": "2024-12-31T05:32:56.716328",
     "exception": false,
     "start_time": "2024-12-31T05:32:56.675906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warmup = \"mr williams i underst\"\n",
    "text = []\n",
    "temp = []\n",
    "for i in range(1, 6):\n",
    "    t = 0.20 * i\n",
    "    s = textgen.predict(warmup, num_preds=100, temp=t)\n",
    "    text.append(s)\n",
    "    temp.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e2cc94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T05:32:56.720181Z",
     "iopub.status.busy": "2024-12-31T05:32:56.720050Z",
     "iopub.status.idle": "2024-12-31T05:32:57.383372Z",
     "shell.execute_reply": "2024-12-31T05:32:57.382940Z"
    },
    "papermill": {
     "duration": 0.668663,
     "end_time": "2024-12-31T05:32:57.386478",
     "exception": false,
     "start_time": "2024-12-31T05:32:56.717815",
     "status": "completed"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_91821_row0_col0, #T_91821_row0_col1, #T_91821_row1_col0, #T_91821_row1_col1, #T_91821_row2_col0, #T_91821_row2_col1, #T_91821_row3_col0, #T_91821_row3_col1, #T_91821_row4_col0, #T_91821_row4_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_91821\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_91821_level0_col0\" class=\"col_heading level0 col0\" >temp</th>\n",
       "      <th id=\"T_91821_level0_col1\" class=\"col_heading level0 col1\" >text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_91821_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_91821_row0_col0\" class=\"data row0 col0\" >0.2</td>\n",
       "      <td id=\"T_91821_row0_col1\" class=\"data row0 col1\" >mr williams i understord the morlocks and in the little in the more and the man the stars in the stare and the man i saw </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91821_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_91821_row1_col0\" class=\"data row1 col0\" >0.4</td>\n",
       "      <td id=\"T_91821_row1_col1\" class=\"data row1 col1\" >mr williams i understanding i had behind and me with a conture i had seemed to me in the still a morlocks the part the li</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91821_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_91821_row2_col0\" class=\"data row2 col0\" >0.6</td>\n",
       "      <td id=\"T_91821_row2_col1\" class=\"data row2 col1\" >mr williams i understand the reced my have i fancied faint they was and that i saw then the interth the black and i could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91821_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_91821_row3_col0\" class=\"data row3 col0\" >0.8</td>\n",
       "      <td id=\"T_91821_row3_col1\" class=\"data row3 col1\" >mr williams i understlucked consteam of up of the and of pitite one see in time this to whice and tildge lay at the hand </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91821_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_91821_row4_col0\" class=\"data row4 col0\" >1.0</td>\n",
       "      <td id=\"T_91821_row4_col1\" class=\"data row4 col1\" >mr williams i understeads my pare incould be receppiname from so under as i linked dedest the even was soon i saw one cub</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x122f6a740>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df = pd.DataFrame({\"temp\": [f\"{t:.1f}\" for t in temp], \"text\": text})\n",
    "df = df.style.set_properties(**{\"text-align\": \"left\"})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3ad44",
   "metadata": {
    "papermill": {
     "duration": 0.003177,
     "end_time": "2024-12-31T05:32:57.392928",
     "exception": false,
     "start_time": "2024-12-31T05:32:57.389751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'Time machine' mentioned! Σ(°ロ°) Here we can see that the higher the temperature, the text looks more random. On the other hand, with lower temp, the softmax becomes more like argmax. The sampling algorithm gets the largest probability token which makes it prone to cycles. \n",
    "\n",
    "**Remark.** It would be nice if text generation does some backtracking, e.g. looking at the probability of the text when we add a new character, as well as characters that will follow the added character. We will see in future chapters how this can be done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc682eec",
   "metadata": {
    "papermill": {
     "duration": 0.002117,
     "end_time": "2024-12-31T05:32:57.397765",
     "exception": false,
     "start_time": "2024-12-31T05:32:57.395648",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.870278,
   "end_time": "2024-12-31T05:32:57.823953",
   "environment_variables": {},
   "exception": null,
   "input_path": "05d-textgen.ipynb",
   "output_path": "05d-textgen.ipynb",
   "parameters": {},
   "start_time": "2024-12-31T05:32:54.953675",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}