{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f74c81",
   "metadata": {
    "papermill": {
     "duration": 0.005585,
     "end_time": "2024-12-29T15:30:11.263731",
     "exception": false,
     "start_time": "2024-12-29T15:30:11.258146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(dl/05-rnns/05f-modern-rnns)=\n",
    "## Modern RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b9e34",
   "metadata": {
    "papermill": {
     "duration": 0.002524,
     "end_time": "2024-12-29T15:30:11.270484",
     "exception": false,
     "start_time": "2024-12-29T15:30:11.267960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Recall that the key quantity to calculate RNN gradients is $\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{H}}_t}$ that contain terms $\\left(\\boldsymbol{\\mathsf{W}}^\\top\\right)^{\\kappa}$ for path length $\\kappa$ from the current time step $t,$ where $\\boldsymbol{\\mathsf{W}}$ is the matrix responsible for transforming the latent state vector. This explodes or vanishes with increasing $\\kappa$ depending on whether the norm of the principal eigenvalue of $\\boldsymbol{\\mathsf{W}}$ is greater than or equal to 1. Hence, RNNs are said to have problems with modeling long-term dependencies between tokens. The consequence of this in practice is that RNNs are limited in context size. \n",
    "\n",
    "Exploding gradients can be solved in practice by gradient clipping or truncated BPTT. On the other hand, the problem of vanishing gradients requires nontrivial architectural changes. We will consider **LSTM** {cite}`lstm`, **GRU** {cite}`gru`, as well as **deep RNNs** and **Bidirectional RNNs** {cite}`birnn` which increase model complexity. In the next chapter, we will apply these architectures to sequence-to-sequence tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203521b7",
   "metadata": {
    "papermill": {
     "duration": 0.002256,
     "end_time": "2024-12-29T15:30:11.275431",
     "exception": false,
     "start_time": "2024-12-29T15:30:11.273175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In terms of code, we implement the following classes in order: `RNN`, `LSTM`, and `GRU`, as well as wrappers `Deep` and `Bidirectional` that modify units to the respective architecture, but with the same API. Note that we can also swap with PyTorch implementations, e.g.\n",
    "\n",
    "```python\n",
    "Deep(Bidirectional(nn.LSTM))(EMBED_DIM, HIDDEN_DIM, num_layers=3, batch_first=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7b1ff",
   "metadata": {
    "papermill": {
     "duration": 0.001828,
     "end_time": "2024-12-29T15:30:11.279227",
     "exception": false,
     "start_time": "2024-12-29T15:30:11.277399",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.027938,
   "end_time": "2024-12-29T15:30:11.400902",
   "environment_variables": {},
   "exception": null,
   "input_path": "05f-modern-rnns.ipynb",
   "output_path": "05f-modern-rnns.ipynb",
   "parameters": {},
   "start_time": "2024-12-29T15:30:10.372964",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}