{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(00-backprop)=\n",
    "# Backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we look at the **backpropagation algorithm** for efficient gradient computation on computational graphs. Backpropagation involves local message passing of outputs in the forward pass, and gradients in the backward pass. The resulting time complexity is linear in the number of size of the network, i.e. the total number of weights and neurons for neural networks. Neural networks are computational graphs with nodes for differentiable operations. This fact allows scaling training large neural networks. We will implement a minimal scalar-valued **autograd engine** and a neural net library on top it to train a small regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP on computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network can be modelled as a **directed acyclic graph** (DAG) of nodes that implements a function $f$, i.e. all computation flows from an input $\\boldsymbol{\\mathsf{x}}$ to an output node $f(\\boldsymbol{\\mathsf{x}})$ with no cycles. \n",
    "During training, this is extended to implement the calculation of the loss.\n",
    "Recall that our goal is to obtain parameter node values $\\hat{\\boldsymbol{\\Theta}}$ after optimization (e.g. with SGD) such that the $f_{\\hat{\\boldsymbol{\\Theta}}}$ minimizes the expected value of a loss function $\\ell.$ Backpropagation allows us to efficiently compute $\\nabla_{\\boldsymbol{\\Theta}} \\ell$ for SGD after $(\\boldsymbol{\\mathsf{x}}, y) \\in \\mathcal{B}$ is passed to the input nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/nn/03-comp-graph.png\n",
    "---\n",
    "width: 80%\n",
    "name: compute\n",
    "align: center\n",
    "---\n",
    "Computational graph of a dense layer. Note that parameter nodes (yellow) always have zero fan-in.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward pass.** Forward pass computes $f_{\\boldsymbol{\\Theta}}(\\boldsymbol{\\mathsf{x}}).$ All compute nodes are executed starting from the input nodes (which evaluates to the input vector $\\boldsymbol{\\mathsf x}$). This passed to its child nodes, and so on up to the loss node. The output value of each node is stored to avoid recomputation for child nodes that depend on the same node. This also preserves the network state for backward pass. Finally, forward pass builds the computational graph which is stored in memory. It follows that forward pass for one input is roughly $O(E)$ in time and memory where $E$ is the number of edges of the graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward pass.** Backward computes gradients starting from the loss node $\\ell$ down \n",
    "to the input nodes $\\boldsymbol{\\mathsf{x}}.$ \n",
    "The gradient of $\\ell$ with respect to itself is $1$. This serves as the base step.\n",
    "For any other node $u$ in the graph, we can assume that the **global gradient**\n",
    "${\\partial \\ell}/{\\partial v}$ is cached for each node $v \\in N_u$, where $N_u$ are all nodes \n",
    "in the graph that depend on $u$. On the other hand, the **local gradient**\n",
    "${\\partial v}/{\\partial u}$ between adjacent nodes is specified \n",
    "analytically based on the functional\n",
    "dependence of $v$ upon $u.$ These are computed at runtime given current node values\n",
    "cached during forward pass.\n",
    "\n",
    "The global gradient with respect to node $u$ can then be inductively calculated using the chain \n",
    "rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial u} = \\sum_{v \\in N_u} \\frac{\\partial \\ell}{\\partial v} \\frac{\\partial v}{\\partial u}.\n",
    "$$\n",
    "\n",
    "This can be visualized as gradients flowing from the loss node to each network node. \n",
    "The flow of gradients will end on parameter and input nodes which depend on no other\n",
    "nodes. These are called **leaf nodes**. It follows that the algorithm terminates.\n",
    "\n",
    "```{figure} ../../../img/backward-1.svg\n",
    "---\n",
    "width: 80%\n",
    "name: backward-1\n",
    "align: center\n",
    "---\n",
    "Computing the global gradient for a single node. Note that gradient type is distinguished by color: **local** (red) and **global** (blue).\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be visualized as gradients flowing to each network node from the loss node. The flow of gradients will end on parameter and input nodes which have zero fan-in. Global gradients are stored in each compute node in the `grad` attribute for use by the next layer, along with node values obtained during forward pass which are used in local gradient computation. Memory can be released after the weights are updated. On the other hand, there is no need to store local gradients as these are computed as needed. Backward pass can be implemented roughly as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Node:\n",
    "    ...\n",
    "\n",
    "    def sorted_graph(self):\n",
    "        \"\"\"Return toposorted comp graph with self as root.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def backward(self):\n",
    "        self.grad = 1.0\n",
    "        for node in self.sorted_graph():\n",
    "            for parent in node._parents:\n",
    "                parent.grad += node.grad * node._local_grad(parent)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node has to wait for all incoming gradients from dependent nodes before passing the gradient to its parents. This is done by topologically sorting the nodes based on dependency with `self` as root (i.e. the node calling `backward` is always treated as the terminal node). \n",
    "The contributions of each child node are then accumulated based on the chain rule, where\n",
    "`node.grad` is the global gradient which is equal to `∂self / ∂node`, while the local gradient `node._local_grad(parent)` is equal to `∂node / ∂parent`. By construction, each child node occurs before any of its parent nodes, thus the full gradient of a child node is calculated before it is sent to its parent nodes ({numref}`03-parent-child-nodes`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/nn/03-parent-child-nodes.png\n",
    "---\n",
    "width: 100%\n",
    "name: 03-parent-child-nodes\n",
    "---\n",
    "Equivalent ways of computing the global gradient. On the left, the global gradient is computed by tracking the dependencies from $u$ to each of its child node during forward pass. This is our formal statement before. Algorithmically, we start from each node in the upper layer, and we contribute one term in the sum to each parent node. Eventually, all terms in the chain rule is accumulated and the parent node fires, sending gradients to its parent nodes in the previous layer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the topologically sorted list of nodes from a terminal node, we use [depth-first search](https://www.geeksforgeeks.org/topological-sorting/). The following example is shown in {numref}`00-toposort`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v ['f']\n",
      "v ['f', 'd']\n",
      "v ['f', 'd', 'x']\n",
      "t ['x']\n",
      "v ['f', 'd', 'x', 'c']\n",
      "v ['f', 'd', 'x', 'c', 'a']\n",
      "t ['x', 'a']\n",
      "v ['f', 'd', 'x', 'c', 'a', 'b']\n",
      "t ['x', 'a', 'b']\n",
      "t ['x', 'a', 'b', 'c']\n",
      "t ['x', 'a', 'b', 'c', 'd']\n",
      "t ['x', 'a', 'b', 'c', 'd', 'f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['f', 'd', 'c', 'b', 'a', 'x']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parents = {\n",
    "    \"a\": [],\n",
    "    \"b\": [],\n",
    "    \"x\": [],\n",
    "    \"c\": [\"a\", \"b\"],\n",
    "    \"d\": [\"x\", \"c\"],\n",
    "    \"e\": [\"c\"],\n",
    "    \"f\": [\"d\"]\n",
    "}\n",
    "\n",
    "def sorted_graph(self):\n",
    "    \"\"\"Return toposorted comp graph with self as root.\"\"\"\n",
    "    topo = []\n",
    "    visited = list()\n",
    "    def dfs(node):\n",
    "        if node not in visited:\n",
    "            visited.append(node)\n",
    "            print(\"v\", visited)\n",
    "            for parent in parents[node]:\n",
    "                dfs(parent)\n",
    "            topo.append(node)\n",
    "            print(\"t\", topo)\n",
    "    dfs(self)\n",
    "    return reversed(topo)\n",
    "\n",
    "list(sorted_graph(\"f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/nn/00-toposort.png\n",
    "---\n",
    "width: 100%\n",
    "name: 00-toposort\n",
    "---\n",
    "Graph encoded in the `parents` dictionary above. Note `d` which `f` has no dependence on is excluded.\n",
    "Visited nodes (shaded in red) starts from the terminal node backwards into the graph. Then, the nodes are pushed forwards once all leaf nodes (no parents, shaded yellow) are visited.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Properties of backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some characteristics of backprop which explains why it is ubiquitous in deep learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Modularity.** Backprop is a useful tool for reasoning about gradient flow and can suggest ways to improve training or network design. Moreover, since it only requires local gradients between nodes, it allows modularity when designing deep neural networks. \n",
    "In other words, we can (in principle) arbitrarily connect layers of computation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Runtime.** Each edge is the DAG is passed exactly once ({numref}`03-parent-child-nodes`). Hence, the time complexity for finding global gradients is $O(n_\\mathsf{E})$ where $n_\\mathsf{E}$ is the number of edges in the graph, where we assume that each compute node and local gradient evaluation is constant time. For fully-connected networks, $n_\\mathsf{E} = n_\\mathsf{M} + n_\\mathsf{V}$ where $n_\\mathsf{M}$ is the number of weights and $n_\\mathsf{V}$ is the number of activations. It follows that one backward pass for an instance is proportional to the network size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Memory.** Each training step naively requires $O(2 n_\\mathsf{E})$ memory since we store both gradients and values. This can be improved by releasing the gradients and activations of non-leaf nodes in the previous layer once a layer finishes computing its gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **GPU parallelism.** Note that forward computation can generally be parallelized in the batch dimension and often times in the layer dimension. This can leverage massive parallelism in the GPU significantly decreasing runtime by trading off memory. The same is true for backward pass which can also be expressed in terms of matrix multiplications! {eq}`backprop-output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## References\n",
    "\n",
    "{cite}`timviera` {cite}`backprop-offconvex` {cite}`pytorch-autograd` {cite}`micrograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bf457f4026a6353447ea08cc5de431bf2d4a57657f54daac3cf4f903fa850ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
