{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(00-backprop)=\n",
    "# Backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we look at the **backpropagation algorithm** for efficient gradient computation on computational graphs. Backpropagation involves local message passing of outputs in the forward pass, and gradients in the backward pass. The resulting time complexity is linear in the number of size of the network, i.e. the total number of weights and neurons for neural networks. Neural networks are computational graphs with nodes for differentiable operations. This fact allows scaling training large neural networks. We will implement a minimal scalar-valued **autograd engine** and a neural net library on top it to train a small regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP on computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network can be modelled as a **directed acyclic graph** (DAG) of nodes that implements a function $f$, i.e. all computation flows from an input $\\boldsymbol{\\mathsf{x}}$ to an output node $f(\\boldsymbol{\\mathsf{x}})$ with no cycles. \n",
    "During training, this is extended to implement the calculation of the loss.\n",
    "Recall that our goal is to obtain parameter node values $\\hat{\\boldsymbol{\\Theta}}$ after optimization (e.g. with SGD) such that the $f_{\\hat{\\boldsymbol{\\Theta}}}$ minimizes the expected value of a loss function $\\ell.$ Backpropagation allows us to efficiently compute $\\nabla_{\\boldsymbol{\\Theta}} \\ell$ for SGD after $(\\boldsymbol{\\mathsf{x}}, y) \\in \\mathcal{B}$ is passed to the input nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/nn/03-comp-graph.png\n",
    "---\n",
    "width: 80%\n",
    "name: compute\n",
    "---\n",
    "Computational graph of a dense layer. Note that parameter nodes (yellow) always have zero fan-in.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward pass.** Forward pass computes $f_{\\boldsymbol{\\Theta}}(\\boldsymbol{\\mathsf{x}}).$ All compute nodes are executed starting from the input nodes (which evaluates to the input vector $\\boldsymbol{\\mathsf x}$). This passed to its child nodes, and so on up to the loss node. The output value of each node is stored to avoid recomputation for child nodes that depend on the same node. This also preserves the network state for backward pass. Finally, forward pass builds the computational graph which is stored in memory. It follows that forward pass for one input is roughly $O(E)$ in time and memory where $E$ is the number of edges of the graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward pass.** Backward computes gradients starting from the loss node $\\ell$ down \n",
    "to the input nodes $\\boldsymbol{\\mathsf{x}}.$ \n",
    "The gradient of $\\ell$ with respect to itself is $1$. This serves as the base step.\n",
    "For any other node $u$ in the graph, we can assume that the **global gradient**\n",
    "${\\partial \\ell}/{\\partial v}$ is cached for each node $v \\in N_u$, where $N_u$ are all nodes \n",
    "in the graph that depend on $u$. On the other hand, the **local gradient**\n",
    "${\\partial v}/{\\partial u}$ between adjacent nodes is specified \n",
    "analytically based on the functional\n",
    "dependence of $v$ upon $u.$ These are computed at runtime given current node values\n",
    "cached during forward pass.\n",
    "\n",
    "The global gradient with respect to node $u$ can then be inductively calculated using the chain \n",
    "rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial u} = \\sum_{v \\in N_u} \\frac{\\partial \\ell}{\\partial v} \\frac{\\partial v}{\\partial u}.\n",
    "$$\n",
    "\n",
    "This can be visualized as gradients flowing from the loss node to each network node. \n",
    "The flow of gradients will end on parameter and input nodes which depend on no other\n",
    "nodes. These are called **leaf nodes**. It follows that the algorithm terminates.\n",
    "\n",
    "```{figure} ../../../img/backward-1.svg\n",
    "---\n",
    "width: 80%\n",
    "name: backward-1\n",
    "---\n",
    "Computing the global gradient for a single node. Note that gradient type is distinguished by color: **local** (red) and **global** (blue).\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be visualized as gradients flowing to each network node from the loss node. The flow of gradients will end on parameter and input nodes which have zero fan-in. Global gradients are stored in each compute node in the `grad` attribute for use by the next layer, along with node values obtained during forward pass which are used in local gradient computation. Memory can be released after the weights are updated. On the other hand, there is no need to store local gradients as these are computed as needed. Backward pass can be implemented roughly as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Node:\n",
    "    ...\n",
    "\n",
    "    def comp_graph(self):\n",
    "        \"\"\"Return toposorted comp graph with self as root.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def backward(self):\n",
    "        self.grad = 1.0\n",
    "        for node in self.comp_graph():\n",
    "            for parent in node._parents:\n",
    "                parent.grad += node.grad * node._local_grad(parent)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node has to wait for all incoming gradients from dependent nodes before passing the gradient to its parents. This is done by topologically sorting the nodes based on dependency with `self` as root (i.e. the node calling `backward` is always treated as the terminal node). \n",
    "The contributions of each child node are then accumulated based on the chain rule, where\n",
    "`node.grad` is the global gradient which is equal to `∂self / ∂node`, while the local gradient `node._local_grad(parent)` is equal to `∂node / ∂parent`. By construction, each child node occurs before any of its parent nodes, thus the full gradient of a child node is calculated before it is sent to its parent nodes ({numref}`03-parent-child-nodes`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/nn/03-parent-child-nodes.png\n",
    "---\n",
    "width: 100%\n",
    "name: 03-parent-child-nodes\n",
    "---\n",
    "Equivalent ways of computing the global gradient. On the left, the global gradient is computed by tracking the dependencies from $u$ to each of its child node during forward pass. This is our formal statement before. Algorithmically, we start from each node in the upper layer, and we contribute one term in the sum to each parent node. Eventually, all terms in the chain rule is accumulated and the parent node fires, sending gradients to its parent nodes in the previous layer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the topologically sorted list of nodes from a terminal node, we use [depth-first search](https://www.geeksforgeeks.org/topological-sorting/). The following example is shown in {numref}`00-toposort`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f', 'd', 'c', 'b', 'a']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parents = {\n",
    "    \"a\": [],\n",
    "    \"b\": [],\n",
    "    \"c\": [\"a\", \"b\"],\n",
    "    \"d\": [\"c\"],\n",
    "    \"e\": [\"c\"],\n",
    "    \"f\": [\"d\"]\n",
    "}\n",
    "\n",
    "def comp_graph(self):\n",
    "    \"\"\"Return toposorted comp graph with self as root.\"\"\"\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def dfs(node):\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            for parent in parents[node]:\n",
    "                dfs(parent)\n",
    "            topo.append(node)\n",
    "    dfs(self)\n",
    "    return reversed(topo)\n",
    "\n",
    "list(comp_graph(\"f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/nn/00-toposort.png\n",
    "---\n",
    "width: 100%\n",
    "name: 00-toposort\n",
    "---\n",
    "Graph encoded in the `parents` dictionary above. Note `d` which `f` has no dependence on is excluded.\n",
    "Visited nodes (shaded in red) starts from the terminal node backwards into the graph. Then, the nodes are pushed forwards once all leaf nodes (no parents) are visited.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Properties of backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some characteristics of backprop which explains why it is ubiquitous in deep learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Modularity.** Backprop is a useful tool for reasoning about gradient flow and can suggest ways to improve training or network design. Moreover, since it only requires local gradients between nodes, it allows modularity when designing deep neural networks. \n",
    "In other words, we can (in principle) arbitrarily connect layers of computation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Runtime.** Each edge is the DAG is passed exactly once ({numref}`03-parent-child-nodes`). Hence, the time complexity for finding global gradients is $O(n_\\mathsf{E})$ where $n_\\mathsf{E}$ is the number of edges in the graph, where we assume that each compute node and local gradient evaluation is constant time. For fully-connected networks, $n_\\mathsf{E} = n_\\mathsf{M} + n_\\mathsf{V}$ where $n_\\mathsf{M}$ is the number of weights and $n_\\mathsf{V}$ is the number of activations. It follows that one backward pass for an instance is proportional to the network size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Memory.** Each training step naively requires $O(2 n_\\mathsf{E})$ memory since we store both gradients and values. This can be improved by releasing the gradients and activations of non-leaf nodes in the previous layer once a layer finishes computing its gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **GPU parallelism.** Note that forward computation can generally be parallelized in the batch dimension and often times in the layer dimension. This can leverage massive parallelism in the GPU significantly decreasing runtime by trading off memory. The same is true for backward pass which can also be expressed in terms of matrix multiplications! {eq}`backprop-output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## References\n",
    "\n",
    "- {cite}`timviera`\n",
    "- {cite}`backprop-offconvex`\n",
    "- {cite}`pytorch-autograd`\n",
    "- {cite}`micrograd`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Testing with `autograd`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `autograd` package allows automatic differentiation by building computational graphs on the fly every time we pass data through our model. Autograd tracks which data combined through which operations to produce the output. This allows us to take derivatives over ordinary imperative code. This functionality is consistent with the memory and time requirements outlined above for BP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scalars.** Here we calculate $\\mathsf{y} = \\boldsymbol{\\mathsf x}^\\top \\boldsymbol{\\mathsf x} = \\sum_i {\\boldsymbol{\\mathsf{x}}_i}^2$ where the initialized tensor $\\boldsymbol{\\mathsf{x}}$ initially has no gradient (i.e. `None`). Calling backward on $\\mathsf{y}$ results in gradients being stored on the leaf tensor $\\boldsymbol{\\mathsf{x}}.$ Note that unlike our implementation, there is no need to set `y.grad = 1.0`. Moreover, doing so would result in an error as $\\mathsf{y}$ is not a [leaf node](https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html) in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float, requires_grad=True)\n",
    "print(x.grad)\n",
    "\n",
    "y = x.reshape(1, -1) @ x \n",
    "y.backward() \n",
    "print((x.grad == 2*x).all().item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectors.** Let $\\boldsymbol{\\mathsf y} = g(\\boldsymbol{\\mathsf x})$ and let $\\boldsymbol{{\\mathsf v}}$ be a vector having the same length as $\\boldsymbol{\\mathsf y}.$ Then `y.backward(v)` calculates\n",
    "$\\sum_i {\\boldsymbol{\\mathsf v}}_i \\frac{\\partial {\\boldsymbol{\\mathsf y}}_i}{\\partial {\\boldsymbol{\\mathsf x}}_j}$\n",
    "resulting in a vector of same length as $\\boldsymbol{\\mathsf{x}}$ stored in `x.grad`. Note that the terms on the right are the local gradients. Setting ${\\boldsymbol{\\mathsf v}} = \\frac{\\partial \\mathcal{L} }{\\partial \\boldsymbol{\\mathsf y}}$ gives us the vector $\\frac{\\partial \\mathcal{L} }{\\partial \\boldsymbol{\\mathsf x}}.$ Below $\\boldsymbol{\\mathsf y}(\\boldsymbol{\\mathsf x}) = [x_0, x_1].$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(size=(4,), dtype=torch.float, requires_grad=True)\n",
    "v = torch.rand(size=(2,), dtype=torch.float)\n",
    "y = x[:2]\n",
    "\n",
    "# Computing the Jacobian by hand\n",
    "J = torch.tensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0]], dtype=torch.float\n",
    ")\n",
    "\n",
    "# Confirming the above formula\n",
    "y.backward(v)\n",
    "(x.grad == v @ J).all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark.** Memory and compute is wasted for running code that should not involve backpropagation. Hence, we wrap this part our code in a `torch.no_grad()` context (or run it inside a function decorated with `@torch.no_grad()`) so that a computation graph is not built. \n",
    "\n",
    "A related method is `.detach()` used to return a tensor detached from the current graph. The result will therefore not require gradients. It is important to note that the detached tensor still shares the same storage with the original one, so that in-place modifications on either tensor takes effect for both and can result in subtle bugs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write our tests with `autograd` to check the correctness of our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Node' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mNode\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.0\u001b[39m)\n\u001b[1;32m      2\u001b[0m z \u001b[38;5;241m=\u001b[39m Node(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m Node(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m x\n\u001b[1;32m      3\u001b[0m q \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mrelu() \u001b[38;5;241m+\u001b[39m z \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39mtanh()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Node' is not defined"
     ]
    }
   ],
   "source": [
    "x = Node(-4.0)\n",
    "z = Node(2) * x + Node(2) - x\n",
    "q = z.relu() + z * x.tanh()\n",
    "h = (z * z).relu()\n",
    "y = (-h + q + q * x).tanh()\n",
    "y.grad = 1.0\n",
    "y.backward()\n",
    "\n",
    "x_node, y_node, z_node = x, y, z\n",
    "draw_graph(y_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute error: 7.48e-08\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(-4.0, requires_grad=True)\n",
    "z = 2 * x + 2 - x\n",
    "q = z.relu() + z * x.tanh()\n",
    "h = (z * z).relu()\n",
    "y = (-h + q + q * x).tanh()\n",
    "\n",
    "z.retain_grad()\n",
    "y.retain_grad()\n",
    "y.backward()\n",
    "\n",
    "x_torch, y_torch, z_torch = x, y, z\n",
    "\n",
    "# forward\n",
    "errors = []\n",
    "errors.append(abs(x_node.data - x_torch.item()))\n",
    "errors.append(abs(y_node.data - y_torch.item()))\n",
    "errors.append(abs(z_node.data - z_torch.item()))\n",
    "\n",
    "# backward\n",
    "errors.append(abs(x_node.grad - x_torch.grad.item()))\n",
    "errors.append(abs(y_node.grad - y_torch.grad.item()))\n",
    "errors.append(abs(z_node.grad - z_torch.grad.item()))\n",
    "\n",
    "print(f\"Max absolute error: {max(errors):.2e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(00-backprop:appendix-backpropagation-equations-for-mlps)=\n",
    "## Appendix: BP equations for MLPs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closed-form expressions for input and output gradients \n",
    "is more efficient to compute since \n",
    "no explicit passing of gradients between nodes is performed.\n",
    "Moreover, such expressions allow us to see how to express it in terms of matrix \n",
    "operations which are computed in parallel.\n",
    "In this section, we derive BP equations specifically for MLPs.\n",
    "Recall that a dense layer with weights $\\boldsymbol{\\mathsf{w}}_j \\in \\mathbb{R}^d$ \n",
    "and bias ${b}_j \\in \\mathbb{R}$ computes given an input $\\boldsymbol{\\mathsf{x}} \\in \\mathbb{R}^d$\n",
    "the following equations for $j = 1, \\ldots, h$ where $h$ is the layer width:\n",
    "```{math}\n",
    ":label: fully-connected-layer\n",
    "\\begin{aligned}\n",
    "    z_j &= \\boldsymbol{\\mathsf{x}} \\cdot \\boldsymbol{\\mathsf{w}}_j + {b}_j \\\\\n",
    "    y_j &= \\phi\\left( z_j \\right) \\\\\n",
    "\\end{aligned}\n",
    "```\n",
    "Given global gradients \n",
    "$\\partial \\ell / \\partial y_j$\n",
    "that flow into the layer, we compute the global gradients of the nodes \n",
    "$\\boldsymbol{\\mathsf{z}}$, $\\boldsymbol{\\mathsf{w}}$, $\\boldsymbol{\\mathsf{b}}$, and $\\boldsymbol{\\mathsf{x}}$\n",
    "in the layer. \n",
    "As discussed above, this can be done by tracking backward dependencies in \n",
    "the computational graph ({numref}`fully-connected-backprop.drawio`).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../img/fully-connected-backprop.drawio.svg\n",
    "---\n",
    "name: fully-connected-backprop.drawio\n",
    "width: 50%\n",
    "---\n",
    "Node dependencies in compute nodes of a fully connected layer. All nodes $\\boldsymbol{\\mathsf{z}}_k$ depend on the node $\\boldsymbol{\\mathsf{y}}_j.$\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there can be cross-dependencies for activations such as softmax. \n",
    "For typical activations $\\phi,$ the [Jacobian](https://mathworld.wolfram.com/Jacobian.html)\n",
    "$\\mathsf{J}^{\\phi}_{kj} = \\frac{\\partial y_k}{\\partial z_j}$ \n",
    "reduces to a diagonal matrix. Following backward dependencies \n",
    "for the compute nodes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{math}\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell}{\\partial {z}_j} &= \\sum_k \\frac{\\partial \\ell}{\\partial {y}_k}  \\frac{\\partial {y}_k}{\\partial {z}_j} =  \\sum_k \\frac{\\partial \\ell}{\\partial {y}_k} \\mathsf{J}^{\\phi}_{kj} \\\\\n",
    "\\frac{\\partial \\ell}{\\partial {x}_i} &= \\sum_j \\frac{\\partial \\ell}{\\partial {z}_j} \\frac{\\partial {z}_j}{\\partial {x}_i} = \\sum_j \\frac{\\partial \\ell}{\\partial {z}_j} {w}_{ij} = \\sum_j \\frac{\\partial \\ell}{\\partial {z}_j} {w}_{ji}^{\\top}.\n",
    "\\end{align}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the second equation uses the gradients calculated in the first equation. Next, we compute gradients for the weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{math}\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell}{\\partial{w}_{ij}} \n",
    "&= \\frac{\\partial \\ell}{\\partial{z}_{j}} \\frac{\\partial{z}_{j}}{\\partial{w}_{ij}} \n",
    "= {x}_{i} \\frac{\\partial \\ell}{\\partial{z}_{j}} \\label{eq:gradient_weight} \\\\\n",
    "\\frac{\\partial \\ell}{\\partial{b}_{j}} \n",
    "&= \\frac{\\partial \\ell}{\\partial{z}_{j}}  \\frac{\\partial{z}_{j}} {\\partial{b}_{j}} \n",
    "= \\frac{\\partial \\ell}{\\partial{z}_{j}}. \\label{eq:gradient_bias}\n",
    "\\end{align}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the dependence of the weight gradient on the input makes it sensitive to scaling.\n",
    "This can introduce an effective scalar factor to the learning rate that is specific to \n",
    "each input dimension, making SGD diverge at early stages of training. This motivates\n",
    "network input normalization and layer output normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $B$ be the batch size. Processing a batch of inputs in parallel, in principle, creates a graph \n",
    "consisting of $B$ copies of the original computational graph that share the same parameters. \n",
    "The outputs of these combine to form the loss node $\\mathcal{L} = \\frac{1}{B}\\sum_b \\ell_b.$ For usual activations, $\\boldsymbol{\\mathsf{J}}^\\phi = \\text{diag}(\\phi^\\prime(\\boldsymbol{\\mathsf{z}}))$. The output and input gradients can be written in the following matrix notation for fast computation:\n",
    "\n",
    "```{math}\n",
    ":label: backprop-output\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Z}}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Y}}} \\odot {\\phi}^{\\prime}(\\boldsymbol{\\mathsf{Z}})\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "```{math}\n",
    ":label: backprop-input\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{X}}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Z}}}\\, \\boldsymbol{\\mathsf{W}}^\\top \\hspace{18pt}\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "These correspond to the equations above since there is no dependence across batch instances. Note that the stacked output tensors $\\boldsymbol{\\mathsf{Z}}$ and $\\boldsymbol{\\mathsf{Y}}$ have shape $(B, h)$ where $h$ is the layer width and $B$ is the batch size. The stacked input tensor $\\boldsymbol{\\mathsf{X}}$ has shape $(B, d)$ where $d$ is the input dimension. Finally, the weight tensor $\\boldsymbol{\\mathsf{W}}$ has shape $(d, h).$ For the weights, the contribution of the entire batch have to be accumulated ({numref}`weight-backprop.drawio`):\n",
    "\n",
    "```{math}\n",
    ":label: backprop-weights\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial{\\boldsymbol{\\mathsf{W}}}} \n",
    "= \\boldsymbol{\\mathsf{X}}^\\top \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Z}}}. \\hspace{30pt}\n",
    "\\end{align}\n",
    "```\n",
    "\n",
    "```{math}\n",
    ":label: backprop-bias\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial{\\boldsymbol{\\mathsf{b}}}} \n",
    "= [1, \\ldots, 1] \\, \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mathsf{Z}}}.\n",
    "\\end{align}\n",
    "```\n",
    "\n",
    "**Remark.** One way to remember these equations is that the shapes must check out.\n",
    "\n",
    "<br>\n",
    "\n",
    "```{figure} ../../img/weight-backprop.drawio.svg\n",
    "---\n",
    "name: weight-backprop.drawio\n",
    "width: 30%\n",
    "---\n",
    "Node dependencies for a weight node. The nodes $\\boldsymbol{\\mathsf{z}}_{bj}$ depend on $\\boldsymbol{\\mathsf{w}}_{ij}$ for $b = 1, \\ldots, B.$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we compute the gradient across the **cross-entropy loss**.\n",
    "This can be calculated using backpropagation, but we will derive it \n",
    "symbolically to get a closed-form formula. Recall that cross-entropy loss computes\n",
    "for logits $\\boldsymbol{\\mathsf{s}}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell \n",
    "&= -\\log \\frac{\\exp({s_{y}})}{\\sum_{k=1}^m \\exp({{s}_{k}})} \\\\\n",
    "&= - s_{y} + \\log \\sum_{k=1}^m \\exp({s_k}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Calculating the derivatives, we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\ell}{\\partial s_j} \n",
    "&= - \\delta_{j}^y + \\frac{\\exp({s_j})}{\\sum_{k=1}^m \\exp({s_k})} \\\\ \\\\\n",
    "&= - \\delta_{j}^y + \\text{softmax}(s_j) = \n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "p_j \\quad \\quad\\;\\;\\; \\text{if $\\;j \\neq y$}\\\\\n",
    "p_y - 1 \\quad \\text{else}\\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\delta_{j}^y$ is the [Kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta).\n",
    "This makes sense: having output values in nodes that do not correspond \n",
    "to the true class only contributes to increasing the loss. This effect is particularly strong \n",
    "when the model is confidently wrong such that $p_y \\approx 0$ on the true class and\n",
    "$p_{j^*} \\approx 1$ where $j^* = \\text{arg max}_j\\, s_j$\n",
    "is the predicted wrong class.\n",
    "On the other hand,\n",
    "increasing values in the node for the true class results in decreasing loss for all nodes.\n",
    "In this case, \n",
    "$\\text{softmax}(\\boldsymbol{\\mathsf{s}}) \\approx \\mathbf{1}_y,$ and\n",
    "${\\partial \\ell}/{\\partial \\boldsymbol{\\mathsf{s}}}$ becomes close to the zero vector, \n",
    "so that $\\nabla_{\\boldsymbol{\\Theta}} \\ell$ is also close to zero.\n",
    "\n",
    "The gradient of the logits ${\\boldsymbol{\\mathsf{S}}}$ can be written in matrix form\n",
    "where $\\mathcal{L} = \\frac{1}{B}\\sum_b \\ell_b$:\n",
    "\n",
    "```{math}\n",
    ":label: backprop-cross-entropy\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\boldsymbol{\\mathsf{S}}}} \n",
    "&= - \\frac{1}{B} \\left( \\boldsymbol{\\delta} - \\text{softmax}({\\boldsymbol{\\mathsf{S}}}) \\right).\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "**Remark.** Examples with similar features but different labels can contribute to a smoothing between \n",
    "the labels of the predicted probability vector. This is nice since we can use the probability\n",
    "value as a measure of confidence. We should also expect a noisy loss curve in the presence of significant label noise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the cross-entropy for a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 32\n",
    "N = 27\n",
    "\n",
    "# forward pass\n",
    "t = torch.randint(low=0, high=N, size=(B,))\n",
    "x = torch.randn(B, 128, requires_grad=True)\n",
    "w = torch.randn(128, N, requires_grad=True)\n",
    "b = torch.randn(N,      requires_grad=True)\n",
    "z = x @ w + b\n",
    "y = torch.tanh(z)\n",
    "\n",
    "for node in [x, w, b, z, y]:\n",
    "    node.retain_grad()\n",
    "\n",
    "# backprop batch loss\n",
    "loss = -torch.log(F.softmax(y, dim=1)[range(B), t]).sum() / B\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the gradient of the logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"822.994534pt\" height=\"423.99625pt\" viewBox=\"0 0 822.994534 423.99625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-12-27T03:31:24.683165</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 423.99625 \n",
       "L 822.994534 423.99625 \n",
       "L 822.994534 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 380.220738 \n",
       "L 350.09187 380.220738 \n",
       "L 350.09187 13.419262 \n",
       "L 40.603125 13.419262 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p1d1598d2bc)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"imagee70cddc41b\" transform=\"scale(1 -1) translate(0 -367.2)\" x=\"40.603125\" y=\"-13.020738\" width=\"309.6\" height=\"367.2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m4630411e89\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"46.334398\" y=\"380.220738\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(43.153148 394.819176) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"103.647129\" y=\"380.220738\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(100.465879 394.819176) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"160.959859\" y=\"380.220738\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(154.597359 394.819176) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"218.27259\" y=\"380.220738\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(211.91009 394.819176) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"275.585321\" y=\"380.220738\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(269.222821 394.819176) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"332.898051\" y=\"380.220738\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(326.535551 394.819176) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- $N$ -->\n",
       "     <g transform=\"translate(191.597498 408.497301) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Oblique-4e\" d=\"M 1081 4666 \n",
       "L 1931 4666 \n",
       "L 3219 666 \n",
       "L 4000 4666 \n",
       "L 4616 4666 \n",
       "L 3706 0 \n",
       "L 2853 0 \n",
       "L 1569 4025 \n",
       "L 788 0 \n",
       "L 172 0 \n",
       "L 1081 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-4e\" transform=\"translate(0 0.09375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m8eca88243b\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"40.603125\" y=\"19.150535\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(27.240625 22.949754) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"40.603125\" y=\"76.463266\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(27.240625 80.262484) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"40.603125\" y=\"133.775996\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(20.878125 137.575215) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"40.603125\" y=\"191.088727\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(20.878125 194.887946) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"40.603125\" y=\"248.401458\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(20.878125 252.200676) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"40.603125\" y=\"305.714188\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(20.878125 309.513407) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"40.603125\" y=\"363.026919\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(20.878125 366.826138) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- $B$ -->\n",
       "     <g transform=\"translate(14.798438 200.27) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Oblique-42\" d=\"M 1081 4666 \n",
       "L 2694 4666 \n",
       "Q 3350 4666 3675 4422 \n",
       "Q 4000 4178 4000 3688 \n",
       "Q 4000 3238 3720 2911 \n",
       "Q 3441 2584 2988 2516 \n",
       "Q 3375 2428 3569 2181 \n",
       "Q 3763 1934 3763 1522 \n",
       "Q 3763 819 3242 409 \n",
       "Q 2722 0 1819 0 \n",
       "L 172 0 \n",
       "L 1081 4666 \n",
       "z\n",
       "M 1234 2228 \n",
       "L 903 519 \n",
       "L 1919 519 \n",
       "Q 2491 519 2800 781 \n",
       "Q 3109 1044 3109 1522 \n",
       "Q 3109 1891 2904 2059 \n",
       "Q 2700 2228 2247 2228 \n",
       "L 1234 2228 \n",
       "z\n",
       "M 1606 4147 \n",
       "L 1331 2741 \n",
       "L 2272 2741 \n",
       "Q 2775 2741 3058 2959 \n",
       "Q 3341 3178 3341 3566 \n",
       "Q 3341 3869 3150 4008 \n",
       "Q 2959 4147 2541 4147 \n",
       "L 1606 4147 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-42\" transform=\"translate(0 0.09375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 380.220738 \n",
       "L 40.603125 13.419262 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 350.09187 380.220738 \n",
       "L 350.09187 13.419262 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 380.220738 \n",
       "L 350.09187 380.220738 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 13.419262 \n",
       "L 350.09187 13.419262 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 495.810784 386.44 \n",
       "L 815.794534 386.44 \n",
       "L 815.794534 7.2 \n",
       "L 495.810784 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p2bcde3be29)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"imagea542a63649\" transform=\"scale(1 -1) translate(0 -379.44)\" x=\"495.810784\" y=\"-7\" width=\"320.4\" height=\"379.44\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"501.736409\" y=\"386.44\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(498.555159 401.038437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"560.992659\" y=\"386.44\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(557.811409 401.038437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"620.248909\" y=\"386.44\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(613.886409 401.038437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"679.505159\" y=\"386.44\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(673.142659 401.038437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"738.761409\" y=\"386.44\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(732.398909 401.038437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4630411e89\" x=\"798.017659\" y=\"386.44\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(791.655159 401.038437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_22\">\n",
       "     <!-- $N$ -->\n",
       "     <g transform=\"translate(652.052659 414.716562) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-4e\" transform=\"translate(0 0.09375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"495.810784\" y=\"13.125625\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(482.448284 16.924844) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"495.810784\" y=\"72.381875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(482.448284 76.181094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"495.810784\" y=\"131.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(476.085784 135.437344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"495.810784\" y=\"190.894375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(476.085784 194.693594) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"495.810784\" y=\"250.150625\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(476.085784 253.949844) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"495.810784\" y=\"309.406875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(476.085784 313.206094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8eca88243b\" x=\"495.810784\" y=\"368.663125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(476.085784 372.462344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_30\">\n",
       "     <!-- $B$ -->\n",
       "     <g transform=\"translate(470.006097 200.27) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-42\" transform=\"translate(0 0.09375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 495.810784 386.44 \n",
       "L 495.810784 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 815.794534 386.44 \n",
       "L 815.794534 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 495.810784 386.44 \n",
       "L 815.794534 386.44 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 495.810784 7.2 \n",
       "L 815.794534 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 369.434917 386.44 \n",
       "L 388.396917 386.44 \n",
       "L 388.396917 7.2 \n",
       "L 369.434917 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAABoAAAIPCAYAAABtzs7PAAABi0lEQVR4nO3XwQkDMAwEQTmk/5aTHvZhjJhpQOz9dGbmNxd8bhyZmfmec64cUpQpyhS9f2jfdIoyRZmiTFGmKFOUKcoUZYoyRZmizLOcKcoUZYoyRZmiTFGmKFOUKcoUZYreP7RvOkWZokxRpihTlCnKFGWKMkWZokxR5lnOFGWKMkWZokxRpihTlCnKFGWKMkXvH9o3naJMUaYoU5QpyhRlijJFmaJMUbavyHSZokxRpihTlCnKFGWKMkWZokxRpijzLGeKMkWZokxRpihTlCnKFGWKMkWZovcP7ZtOUaYoU5QpyhRlijJFmaJMUaYo21dkukxRpihTlCnKFGWKMkWZokxRpihTlHmWM0WZokxRpihTlCnKFGWKMkWZokzR+4f2TacoU5QpyhRlijJFmaJMUaYoU5QpyjzLmaJMUaYoU5QpyhRlijJFmaJMUabo/UP7plOUKcoUZYoyRZmiTFGmKFOUKcr2FZkuU5QpyhRlijJFmaJMUaYoU5QpyhRlnuVMUaYoU5TtK/oDk14KGCn/jE0AAAAASUVORK5CYII=\" id=\"imagea9bf074555\" transform=\"scale(1 -1) translate(0 -379.44)\" x=\"369.36\" y=\"-6.48\" width=\"18.72\" height=\"379.44\"/>\n",
       "   <g id=\"matplotlib.axis_5\"/>\n",
       "   <g id=\"matplotlib.axis_6\">\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <defs>\n",
       "       <path id=\"m38355832b3\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m38355832b3\" x=\"388.396917\" y=\"319.711608\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_31\">\n",
       "      <!-- −0.8 -->\n",
       "      <g transform=\"translate(395.396917 323.510827) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m38355832b3\" x=\"388.396917\" y=\"250.210909\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_32\">\n",
       "      <!-- −0.6 -->\n",
       "      <g transform=\"translate(395.396917 254.010127) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_17\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m38355832b3\" x=\"388.396917\" y=\"180.710209\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_33\">\n",
       "      <!-- −0.4 -->\n",
       "      <g transform=\"translate(395.396917 184.509428) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_18\">\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m38355832b3\" x=\"388.396917\" y=\"111.20951\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_34\">\n",
       "      <!-- −0.2 -->\n",
       "      <g transform=\"translate(395.396917 115.008729) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_19\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m38355832b3\" x=\"388.396917\" y=\"41.70881\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_35\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(395.396917 45.508029) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 369.434917 386.44 \n",
       "L 378.915917 386.44 \n",
       "L 388.396917 386.44 \n",
       "L 388.396917 7.2 \n",
       "L 378.915917 7.2 \n",
       "L 369.434917 7.2 \n",
       "L 369.434917 386.44 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p1d1598d2bc\">\n",
       "   <rect x=\"40.603125\" y=\"13.419262\" width=\"309.488745\" height=\"366.801476\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p2bcde3be29\">\n",
       "   <rect x=\"495.810784\" y=\"7.2\" width=\"319.98375\" height=\"379.24\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1200x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "c = ax[0].imshow(y.grad.detach().numpy() * B, cmap='gray')\n",
    "ax[1].imshow(-F.one_hot(t, num_classes=N).detach().numpy(), cmap='gray')\n",
    "ax[0].set_ylabel(\"$B$\")\n",
    "ax[1].set_ylabel(\"$B$\")\n",
    "ax[0].set_xlabel(\"$N$\")\n",
    "ax[1].set_xlabel(\"$N$\")\n",
    "plt.colorbar(c, ax=ax[0])\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure.** Gradient of the logits for given batch (left) and actual targets (right). Notice the sharp contribution to decreasing loss by increasing the logit of the correct class. Other nodes contribute to increasing the loss. It's hard to see, but incorrect pixels have positive values that sum to the pixel value in the target. Checking this to ensure correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0609e-09)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad.sum(dim=1, keepdim=True).abs().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the above equations were vectorized with the convention that the gradient with respect to a tensor $\\boldsymbol{\\mathsf{v}}$ has the same shape as $\\boldsymbol{\\mathsf{v}}.$ In PyTorch, `v.grad` is the global gradient with respect to `v` of the tensor that called `.backward()` (i.e. `loss` in our case). The following computation should give us an intuition of how gradients flow backwards through the neural net starting from the loss to all intermediate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 1 - y**2                                    # Jacobian\n",
    "δ_tk = F.one_hot(t, num_classes=N)              # Kronecker delta\n",
    "dy = - (1 / B) * (δ_tk - F.softmax(y, dim=1))   # logits grad\n",
    "dz = dy * J\n",
    "dx = dz @ w.T\n",
    "dw = x.T @ dz\n",
    "db = dz.sum(0, keepdim=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to {eq}`backprop-cross-entropy`, {eq}`backprop-output`, {eq}`backprop-input`, {eq}`backprop-weights`, and {eq}`backprop-bias` above. These equations can be checked using `autograd` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y   | exact: False | approx: True  | maxdiff: 1.86e-09\n",
      "z   | exact: False | approx: True  | maxdiff: 1.16e-10\n",
      "x   | exact: False | approx: True  | maxdiff: 1.86e-09\n",
      "w   | exact: False | approx: True  | maxdiff: 1.86e-09\n",
      "b   | exact: False | approx: True  | maxdiff: 2.33e-10\n"
     ]
    }
   ],
   "source": [
    "def compare(name, dt, t):\n",
    "    exact  = torch.all(dt == t.grad).item()\n",
    "    approx = torch.allclose(dt, t.grad, rtol=1e-5)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{name:<3s} | exact: {str(exact):5s} | approx: {str(approx):5s} | maxdiff: {maxdiff:.2e}')\n",
    "\n",
    "\n",
    "compare('y', dy, y)\n",
    "compare('z', dz, z)\n",
    "compare('x', dx, x)\n",
    "compare('w', dw, w)\n",
    "compare('b', db, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bf457f4026a6353447ea08cc5de431bf2d4a57657f54daac3cf4f903fa850ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
