{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic theory\n",
    "\n",
    "Here we will consider binary classification, although the method can be easily extended to the multiclass setting. For each data point $\\boldsymbol{\\mathsf{x}} \\in \\mathscr{X}$ the latent true label is denoted by $y \\in \\mathscr{Y} = \\{-1, +1\\}$. We have $n$ labeling functions $\\lambda_j\\colon \\mathscr{X} \\to \\mathscr{Y} \\cup \\{0\\}$ for $j = 1, \\ldots, n.$ In case that the LF is not applicable, then the LF returns $0$ for *abstain*. This explains why we augmented the target space with the zero label. So if we have data points $\\boldsymbol{\\mathsf{x}}_i$ for $i = 1, \\ldots, m$, then we get an $m \\times n$ matrix of LF outputs $\\Lambda_{ij} = \\lambda_j(\\boldsymbol{\\mathsf{x}}_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to estimate the distribution $p_{\\delta, \\gamma}(\\Lambda=\\lambda(\\boldsymbol{\\mathsf{x}}), Y=y)$ defined as the probability that LFs output $\\lambda(\\boldsymbol{\\mathsf{x}}) = (\\lambda_1(\\boldsymbol{\\mathsf{x}}), \\ldots, \\lambda_n(\\boldsymbol{\\mathsf{x}}))$ for a test instance $\\boldsymbol{\\mathsf{x}}$ with true label $y$. The parameters are chosen such that the marginal probability $p(\\Lambda_{ij})$ of the observed LF outputs is maximized.\n",
    "This parameters of the model are **coverage** $\\delta = (\\delta_1, \\ldots, \\delta_n)$ and **accuracy** $\\gamma = (\\gamma_1, \\ldots, \\gamma_n)$ of each LF. Once the parameters $\\hat{\\delta}$ and $\\hat{\\gamma}$ have been learned, we can train a **noise-aware** discriminative model by minimizing the ff. loss function:\n",
    "\n",
    "$$\n",
    "\\mathscr{L}(\\Theta) = \n",
    "\\frac{1}{m}\\sum_{i=1}^m \\sum_{y=-1,+1} \n",
    "\\ell(f_\\Theta(\\boldsymbol{\\mathsf{x}}), y) \\cdot \n",
    "{p_{\\hat{\\delta}, \\hat{\\gamma}}(Y = y \\mid \\Lambda = \\lambda(\\boldsymbol{\\mathsf{x}}))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\ell$ is the instance loss. This looks like the usual loss except that the contribution for each target is summed over weighted by the probability of the target given the labeling of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each LF $\\lambda_j$ we assign two parameters $(\\delta_j, \\gamma_j)$ corresponding to its coverage and accuracy. Coverage $\\delta_j$ is defined as the probability of labeling an input, and accuracy as $\\gamma_j$ as the probability of labeling it correctly. This assumes that the LFs have the same distribution for each label (e.g. not more accurate when the label is positive). Moreover, we assume that LF outputs are independent of each other. Hence,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_{\\delta, \\gamma}(\\Lambda = \\lambda(\\boldsymbol{\\mathsf{x}})) \n",
    "&= \\sum_{y=-1,+1}  p_{\\delta, \\gamma}(\\Lambda = \\lambda(\\boldsymbol{\\mathsf{x}}), Y = y) \\\\\n",
    "&= \\sum_{y=-1,+1} p(Y = y) \\cdot p_{\\delta, \\gamma}(\\Lambda = \\lambda(\\boldsymbol{\\mathsf{x}}) \\mid Y = y) \\\\\n",
    "&= \\sum_{y=-1,+1} p(Y = y) \\cdot \\prod_{j=1}^n \\begin{cases} \n",
    "1 - \\delta_j \\quad& \\phantom{y}\\lambda_j(\\boldsymbol{\\mathsf{x}}) &= \\phantom{-}0 \\\\\n",
    "\\delta_j\\gamma_j \\quad& y \\lambda_j(\\boldsymbol{\\mathsf{x}}) &= +1 \\\\\n",
    "\\delta_j(1 - \\gamma_j) \\quad& y \\lambda_j(\\boldsymbol{\\mathsf{x}}) &= -1 \\\\\n",
    "\\end{cases}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal therefore is to find parameters that maximize the observed LF outputs for our dataset:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(\\hat{\\delta}, \\hat{\\gamma}) \n",
    "&= \\underset{\\delta,\\gamma}{\\text{arg min}}\\sum_{i=1}^m -\\log \\; p_{\\delta, \\gamma}(\\Lambda_{i}) \\\\\n",
    "&= \\underset{\\delta,\\gamma}{\\text{arg min}}\\sum_{i=1}^m -\\log \n",
    "\\left( \n",
    "    \\sum_{y=-1,+1} p_y \n",
    "    \\cdot \n",
    "    \\prod_{j=1}^n \n",
    "    \\begin{cases} \n",
    "        1 - \\delta_j            \\quad& \\phantom{y}\\Lambda_{ij} &= \\phantom{-}0 \\\\\n",
    "        \\delta_j\\gamma_j        \\quad&          y \\Lambda_{ij} &= +1 \\\\\n",
    "        \\delta_j(1 - \\gamma_j)  \\quad&          y \\Lambda_{ij} &= -1\n",
    "    \\end{cases} \n",
    "\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Remark.** The assumption that accuracy is independent of true label is strong. Recall that the distributions in the rows of a confusion matrix for a classifier are not generally the same for each true label. This is fixed by having a separate set of LF parameters for each true label. For the multi-class case with $K$ classes, we have to learn parameters for $K-1$ entries of each row of the confusion matrix of every LF. For the sake of simplicity, we stick with the idealized case for binary classification.\n",
    "\n",
    "We implement the above equations using some clever indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7000, 0.7000, 0.7000, 0.7000],\n",
       "        [0.2100, 0.1800, 0.2400, 0.2700],\n",
       "        [0.0900, 0.1200, 0.0600, 0.0300]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)\n",
    "\n",
    "delta = torch.tensor([0.3, 0.3, 0.3, 0.3])  # coverage\n",
    "gamma = torch.tensor([0.7, 0.6, 0.8, 0.9])  # accuracy\n",
    "\n",
    "n = 4\n",
    "L = torch.tensor([\n",
    "    [-1,  0, +1, +1],\n",
    "    [+1, -1, -1,  0],\n",
    "])\n",
    "\n",
    "params = torch.stack([\n",
    "    1 - delta,              # abstained\n",
    "    delta * gamma,          # accurate\n",
    "    delta * (1 - gamma)     # inaccurate\n",
    "], dim=0)\n",
    "\n",
    "params  # Note sum along dim=0 is 1, i.e. sum of all p(λ | y) for λ = -1, 0, 1 is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the empirical LF matrix to pick out the appropriate weight given its value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.7000, 0.2400, 0.2700],\n",
       "        [0.2100, 0.1200, 0.0600, 0.7000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(L | y = +1)\n",
    "params[L, torch.arange(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that non-abstained probabilities will flip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2100, 0.7000, 0.0600, 0.0300],\n",
       "        [0.0900, 0.1800, 0.2400, 0.7000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(L | y = -1)\n",
    "params[-L, torch.arange(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p_{y=-1} = 0.7$ and $p_{y=+1} = 0.3$. The marginal probability of the LF outputs for each instance is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0022, 0.0019])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py = [0.0, 0.5, 0.5]    # zero-index = dummy\n",
    "p_pos = py[+1] * (params[+L, torch.arange(n)]).prod(dim=1) \n",
    "p_neg = py[-1] * (params[-L, torch.arange(n)]).prod(dim=1) \n",
    "p = p_pos + p_neg\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we generally have $m \\gg 1$ terms with valuees in $[0, 1].$ So we use $\\log$ to convert the product to a sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     p(Λ): 4.107915628992487e-06\n",
      "-log p(Λ): 12.402594566345215\n"
     ]
    }
   ],
   "source": [
    "print(\"     p(Λ):\", p.prod().item())\n",
    "print(\"-log p(Λ):\", -torch.log(p).sum().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
