{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4c5954",
   "metadata": {
    "papermill": {
     "duration": 0.00178,
     "end_time": "2024-09-02T07:15:12.540990",
     "exception": false,
     "start_time": "2024-09-02T07:15:12.539210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(dl/01ba-nll)=\n",
    "# Negative log loss (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd31b55",
   "metadata": {
    "papermill": {
     "duration": 0.000986,
     "end_time": "2024-09-02T07:15:12.543136",
     "exception": false,
     "start_time": "2024-09-02T07:15:12.542150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The machine learning method follows four steps: defining a model, defining a loss function,\n",
    "choosing an optimizer, and running it on large compute (e.g. GPUs). A **loss function** \n",
    "acts a smooth surrogate to the true objective which may not be amenable to available optimization \n",
    "techniques. Hence, we can think of loss functions as a measure of model quality.\n",
    "The choice of loss function determines what the model parameters will optimize towards.\n",
    "\n",
    "```{figure} ../../../img/nn/02-loss-surface.png\n",
    "---\n",
    "name: 01c-loss-surface\n",
    "width: 60%\n",
    "align: center\n",
    "---\n",
    "Loss surface for a model with two weights. [Source](https://cs182sp21.github.io/static/slides/lec-4.pdf)\n",
    "```\n",
    "\n",
    "\n",
    "Here we derive a loss function based on the principle of [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLE), i.e. finding optimal parameters such that the dataset is most probable. Consider a parametric model of the target $p_{\\boldsymbol{\\Theta}}(y \\mid \\boldsymbol{\\mathsf{x}}).$ \n",
    "The **likelihood** of the [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) sample $\\mathcal{D} = \\{(\\boldsymbol{\\mathsf{x}}_i, y_i)\\}_{i=1}^N$ can be defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\boldsymbol{\\Theta}) \n",
    "= \\left({\\prod_{i=1}^N {p_{\\boldsymbol{\\Theta}}(y_i \\mid \\boldsymbol{\\mathsf{x}}_i)}}\\right)^{\\frac{1}{N}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This can be thought of as the probability assigned by the parametric model on the sample.\n",
    "The iid assumption is important. It also means that the model gets to focus on inputs \n",
    "that are more probable since they are better represented in the sample. \n",
    "Probabilities are\n",
    "small numbers in $[0, 1]$ and we are multiplying lots of them, so applying the logarithm which is monotonic\n",
    "and converts the product into a sum is a good idea:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log L(\\boldsymbol{\\Theta}) \n",
    "&= \\frac{1}{N}\\sum_{i=1}^N \\log p_{\\boldsymbol{\\Theta}}(y_i \\mid \\boldsymbol{\\mathsf{x}}_i).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f976364",
   "metadata": {
    "papermill": {
     "duration": 0.000731,
     "end_time": "2024-09-02T07:15:12.544798",
     "exception": false,
     "start_time": "2024-09-02T07:15:12.544067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "MLE then maximizes the log-likelihood with respect to the parameters $\\boldsymbol{\\Theta}.$ The idea is that a good model should make the data more probable. It is common practice in optimization literature to convert this to a minimization problem. The following then becomes our optimization problem:\n",
    "\n",
    "$$\\boldsymbol{\\Theta}^* = \\underset{\\boldsymbol{\\Theta}}{\\text{argmin}}\\,\\left( -\\frac{1}{N}\\sum_{i=1}^N \\log p_{\\boldsymbol{\\Theta}}(y_i \\mid \\boldsymbol{\\mathsf{x}}_i)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f231012",
   "metadata": {
    "papermill": {
     "duration": 0.000881,
     "end_time": "2024-09-02T07:15:12.547376",
     "exception": false,
     "start_time": "2024-09-02T07:15:12.546495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This allows us to define $\\ell = -\\log p_{\\boldsymbol{\\Theta}}(y \\mid \\boldsymbol{\\mathsf{x}}).$ In general, the loss function can be any nonnegative function whose value approaches zero whenever the prediction of the network the target value. Observe that:\n",
    "\n",
    "- $p_{\\boldsymbol{\\Theta}}(y \\mid \\boldsymbol{\\mathsf{x}}) \\to 1$ $\\implies$ $\\ell \\to 0$\n",
    "- $p_{\\boldsymbol{\\Theta}}(y \\mid \\boldsymbol{\\mathsf{x}}) \\to 0$ $\\implies$ $\\ell \\to \\infty$ \n",
    "\n",
    "Using an expectation of the loss over the underlying distribution allows the model to focus on errors based on its probability of occuring. For parameters $\\boldsymbol{\\Theta},$ we will approximate the **true risk** which is the expectation of $\\ell$ on the underlying distribution with the **empirical risk** calculated on the sample $\\mathcal{D}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\boldsymbol{\\Theta}) \n",
    "&= \\mathbb{E}_{\\boldsymbol{\\mathsf{x}},y}\\left[\\ell(y, f_{\\boldsymbol{\\Theta}}(\\boldsymbol{\\mathsf{x}}))\\right] \\\\\n",
    "&\\approx \\mathcal{L}_\\mathcal{D}(\\boldsymbol{\\Theta}) = \\frac{1}{|\\mathcal{D}|} \\sum_i \\ell(y_i, f_{\\boldsymbol{\\Theta}}(\\boldsymbol{\\mathsf{x}}_i)).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The optimization problem can be written more generally as\n",
    "$\\boldsymbol{\\Theta}^* = \\underset{\\boldsymbol{\\Theta}}{\\text{argmin}}\\, \\mathcal{L}_\\mathcal{D}(\\boldsymbol{\\Theta})\n",
    "$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.945649,
   "end_time": "2024-09-02T07:15:12.665706",
   "environment_variables": {},
   "exception": null,
   "input_path": "01ba-nll.ipynb",
   "output_path": "01ba-nll.ipynb",
   "parameters": {},
   "start_time": "2024-09-02T07:15:11.720057",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}