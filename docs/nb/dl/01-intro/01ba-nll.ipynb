{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4c5954",
   "metadata": {
    "papermill": {
     "duration": 0.005263,
     "end_time": "2024-09-23T08:57:41.248401",
     "exception": false,
     "start_time": "2024-09-23T08:57:41.243138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(dl/01ba-nll)=\n",
    "# Negative log loss (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd31b55",
   "metadata": {
    "papermill": {
     "duration": 0.000954,
     "end_time": "2024-09-23T08:57:41.251896",
     "exception": false,
     "start_time": "2024-09-23T08:57:41.250942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Machine learning training requires four steps: defining a model, defining a loss function,\n",
    "choosing an optimizer, and running it on large compute (e.g. GPUs). A **loss function** \n",
    "acts a smooth surrogate to the true objective which may not be amenable to available optimization \n",
    "techniques. Hence, we can think of loss functions as a measure of model quality.\n",
    "The choice of loss function determines what the model parameters will optimize towards.\n",
    "\n",
    "```{figure} ../../../img/nn/02-loss-surface.png\n",
    "---\n",
    "name: 01c-loss-surface\n",
    "width: 60%\n",
    "align: center\n",
    "---\n",
    "Loss surface for a model with two weights. [Source](https://cs182sp21.github.io/static/slides/lec-4.pdf)\n",
    "```\n",
    "\n",
    "\n",
    "Here we derive a loss function based on the principle of [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLE), i.e. finding optimal parameters such that the dataset is most probable. Consider a parametric model of the target $p_{\\boldsymbol{\\Theta}}(y \\mid \\boldsymbol{\\mathsf{x}}).$ \n",
    "The **likelihood** of the [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) sample $\\mathcal{D} = \\{(\\boldsymbol{\\mathsf{x}}_i, y_i)\\}_{i=1}^N$ can be defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\boldsymbol{\\Theta}) \n",
    "= \\left({\\prod_{i=1}^N {p_{\\boldsymbol{\\Theta}}(y_i \\mid \\boldsymbol{\\mathsf{x}}_i)}}\\right)^{\\frac{1}{N}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This can be thought of as the probability assigned by the parametric model on the sample.\n",
    "The iid assumption is important. Note that maximizing likelihood means that the model will focus more on inputs that are more probable since they are better represented in the sample. \n",
    "Probabilities are\n",
    "small numbers in $[0, 1]$ and we are multiplying lots of them, so applying the logarithm which is monotonic\n",
    "and converts the product into a sum is a good idea:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log L(\\boldsymbol{\\Theta}) \n",
    "&= \\frac{1}{N}\\sum_{i=1}^N \\log p_{\\boldsymbol{\\Theta}}(y_i \\mid \\boldsymbol{\\mathsf{x}}_i).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f976364",
   "metadata": {
    "papermill": {
     "duration": 0.000786,
     "end_time": "2024-09-23T08:57:41.253526",
     "exception": false,
     "start_time": "2024-09-23T08:57:41.252740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "MLE then maximizes the log-likelihood with respect to the parameters $\\boldsymbol{\\Theta}.$ The idea is that a good model should make the data more probable. It is common practice in optimization literature to convert this to a minimization problem. The following then becomes our optimization problem:\n",
    "\n",
    "$$\\boldsymbol{\\Theta}^* = \\underset{\\boldsymbol{\\Theta}}{\\text{argmin}}\\,\\left( -\\frac{1}{N}\\sum_{i=1}^N \\log p_{\\boldsymbol{\\Theta}}(y_i \\mid \\boldsymbol{\\mathsf{x}}_i)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f231012",
   "metadata": {
    "papermill": {
     "duration": 0.000749,
     "end_time": "2024-09-23T08:57:41.255050",
     "exception": false,
     "start_time": "2024-09-23T08:57:41.254301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This allows us to define $\\ell = -\\log p_{\\boldsymbol{\\Theta}}(y \\mid \\boldsymbol{\\mathsf{x}}).$ In general, the loss function can be any nonnegative function whose value approaches zero whenever the prediction of the network the target value. Observe that:\n",
    "\n",
    "- $p_{\\boldsymbol{\\Theta}}(y \\mid \\boldsymbol{\\mathsf{x}}) \\to 1$ $\\implies$ $\\ell \\to 0$\n",
    "- $p_{\\boldsymbol{\\Theta}}(y \\mid \\boldsymbol{\\mathsf{x}}) \\to 0$ $\\implies$ $\\ell \\to \\infty$ \n",
    "\n",
    "Using an expectation of the loss over the underlying distribution allows the model to focus on errors based on its probability of occuring. For parameters $\\boldsymbol{\\Theta},$ we will approximate the **true risk** which is the expectation of $\\ell$ on the underlying distribution with the **empirical risk** calculated on the sample $\\mathcal{D}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\boldsymbol{\\Theta}) \n",
    "&= \\mathbb{E}_{\\boldsymbol{\\mathsf{x}},y}\\left[\\ell(y, f_{\\boldsymbol{\\Theta}}(\\boldsymbol{\\mathsf{x}}))\\right] \\\\\n",
    "&\\approx \\mathcal{L}_\\mathcal{D}(\\boldsymbol{\\Theta}) = \\frac{1}{|\\mathcal{D}|} \\sum_i \\ell(y_i, f_{\\boldsymbol{\\Theta}}(\\boldsymbol{\\mathsf{x}}_i)).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The optimization problem can be written more generally as\n",
    "$\\boldsymbol{\\Theta}^* = \\underset{\\boldsymbol{\\Theta}}{\\text{argmin}}\\, \\mathcal{L}_\\mathcal{D}(\\boldsymbol{\\Theta})\n",
    "$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77f570",
   "metadata": {
    "papermill": {
     "duration": 0.000753,
     "end_time": "2024-09-23T08:57:41.256585",
     "exception": false,
     "start_time": "2024-09-23T08:57:41.255832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cross entropy\n",
    "\n",
    "A commonly used metric in deep learning for classification tasks is **cross entropy** defined as \n",
    "\n",
    "$$H(p, \\hat{p}) = -\\mathbb{E}_{\\boldsymbol{\\mathsf{x}} \\sim p}[\\log \\hat{p}(\\boldsymbol{\\mathsf{x}})].$$\n",
    "\n",
    "$H(p, \\hat{p})$ increases[^1] as the model probability $\\hat{p}$ diverges from the true probability $p$\n",
    "with minimum value $-\\mathbb{E}_{p}[\\log {p}]$ when $\\hat{p} = p.$\n",
    "This is equivalent to NLL when $p$ are one-hot probability vectors, i.e. we calculate the average $-\\log \\hat{p}_y$ where $y$ is the target class. Thus, any classification model trained with cross-entropy on hard labels maximizes the likelihood of the training dataset.\n",
    "\n",
    "In practice, we usually convert class scores to probabilities using the softmax function. The PyTorch implementation [`F.cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) reflects this. The true probability vector $p$ can be of shape $(B,)$ where $B$ is the number of inputs where $p_i = 0, 1, \\ldots, K-1$ for $K$ classes (hard labels), or $(B, K)$ where $p_{ij} \\in [0, 1]$ containing probabilities for class $j$ (soft labels).\n",
    "\n",
    "[^1]: From [Gibbs' inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality), we have $H(p, q) \\geq H(p, p)$. The cross entropy measures the amount of \"information\" needed to describe outputs of the model. Recall that the input output pairs $(\\boldsymbol{\\mathsf{x}}, y)$ are generated by a random process. The cross entropy increases as more information is used to describe each outcome of this random process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1812f52a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T08:57:41.259106Z",
     "iopub.status.busy": "2024-09-23T08:57:41.258910Z",
     "iopub.status.idle": "2024-09-23T08:57:41.910501Z",
     "shell.execute_reply": "2024-09-23T08:57:41.910223Z"
    },
    "papermill": {
     "duration": 0.65408,
     "end_time": "2024-09-23T08:57:41.911411",
     "exception": false,
     "start_time": "2024-09-23T08:57:41.257331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4525)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "B = 32                               # no. of examples\n",
    "K = 3                                # no. of classes\n",
    "s = torch.randn(B, K)                # logits = unnormalized class scores\n",
    "p = torch.randint(0, K, size=(B,))   # hard labels -> one-hot true probas\n",
    "F.cross_entropy(s, target=p)         # expects logits, applies softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b1561",
   "metadata": {
    "papermill": {
     "duration": 0.000858,
     "end_time": "2024-09-23T08:57:41.913388",
     "exception": false,
     "start_time": "2024-09-23T08:57:41.912530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`F.cross_entropy` calculates cross-entropy with softmax probas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c64737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T08:57:41.915670Z",
     "iopub.status.busy": "2024-09-23T08:57:41.915453Z",
     "iopub.status.idle": "2024-09-23T08:57:41.920449Z",
     "shell.execute_reply": "2024-09-23T08:57:41.920176Z"
    },
    "papermill": {
     "duration": 0.007147,
     "end_time": "2024-09-23T08:57:41.921332",
     "exception": false,
     "start_time": "2024-09-23T08:57:41.914185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4525)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = F.softmax(s, dim=1)\n",
    "-torch.log(q[range(B), p]).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.697444,
   "end_time": "2024-09-23T08:57:42.138731",
   "environment_variables": {},
   "exception": null,
   "input_path": "01ba-nll.ipynb",
   "output_path": "01ba-nll.ipynb",
   "parameters": {},
   "start_time": "2024-09-23T08:57:40.441287",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}