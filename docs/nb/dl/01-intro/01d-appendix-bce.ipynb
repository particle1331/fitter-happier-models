{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: BCE loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce the **BCE loss** which you might encounter [in the wild](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html). This turns out to be just cross-entropy but for binary classification with scalar-valued models. Another goal of this section is to show that conceptually simple things in ML can be confusing due to implementation details.\n",
    "\n",
    "For binary classification, since $p_0 + p_1 = 1$, it suffices to compute the probability for the positive class $p_1$. Hence, we should be able to train a scalar valued NN to compute the probabilities. In this case, the cross-entropy loss can be calculated using $p_1$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell_{\\text{CE}} \n",
    "= -(1 - y)\\log (1 - p_1) - y \\log p_1\n",
    "\\; = \\begin{cases} \n",
    "    -\\log \\;(1 - p_1)  \\quad &{y = 0} \\\\ \n",
    "    -\\log \\; p_1 \\quad &{y = 1}.\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let $\\boldsymbol{\\mathsf{s}} = f(\\boldsymbol{\\mathsf{x}}) \\in \\mathbb{R}^2$ be the network output. Recall that\n",
    "the softmax probabilities are given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{p} = \\text{Softmax}(\\boldsymbol{\\mathsf{s}})\n",
    "&= \\left(\\frac{e^{s_0}}{e^{s_0} + e^{s_1}}, \\frac{e^{s_1}}{e^{s_0} + e^{s_1}}\\right) \\\\\n",
    "&= \\left(\\frac{1}{1 + e^{(s_1 - s_0)}}, \\frac{1}{1 + e^{-(s_1 - s_0)}}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then, the probability of the positive class can be written as:\n",
    "\n",
    "$$p_1 = \\text{Sigmoid}(\\Delta s)$$\n",
    "\n",
    "where $\\Delta s := s_1 - s_0.$ \n",
    "This can now be used to calculate the cross-entropy by using \n",
    "\n",
    "$$-\\log\\,\\text{Sigmoid}(\\Delta s) = \\log\\left(1 + e^{-\\Delta s}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is more numerically stable than calculating the two operations sequentially. \n",
    "Note that $\\Delta s = (\\boldsymbol{\\theta}_1  - \\boldsymbol{\\theta}_0)^\\top \\boldsymbol{\\mathsf{z}} + (b_1 - b_0)$ since the logits layer is linear.\n",
    "Thus, we can train an equivalent scalar-valued model with these fused weights that models $\\Delta s.$ This model predicts the positive class whenever $\\Delta s \\geq 0,$ i.e. $s_1 \\geq s_0.$ The scalar-valued model can then be converted to the two-valued model by assigning zero weights and bias to the negative class.\n",
    "\n",
    "\n",
    "**Remark.** This is another nice property of using the exponential to convert scores to probabilities, i.e. it converts a sum to a product, allowing fusing the weights of the logits layer to get one separating hyperplane."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
