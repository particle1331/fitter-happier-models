{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea9f6af",
   "metadata": {
    "papermill": {
     "duration": 0.005064,
     "end_time": "2024-09-23T08:59:17.587611",
     "exception": false,
     "start_time": "2024-09-23T08:59:17.582547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Appendix: BCE loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37b9c3",
   "metadata": {
    "papermill": {
     "duration": 0.002388,
     "end_time": "2024-09-23T08:59:17.593503",
     "exception": false,
     "start_time": "2024-09-23T08:59:17.591115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we introduce the **BCE loss** which you might encounter [in the wild](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html). This turns out to be just cross-entropy but for binary classification with scalar-valued models. Another goal of this section is to show that conceptually simple things in ML can be confusing due to implementation details.\n",
    "\n",
    "For binary classification, since $p_0 + p_1 = 1$, it suffices to compute the probability for the positive class $p_1$. Hence, we should be able to train a scalar valued NN to compute the probabilities. In this case, the cross-entropy loss can be calculated using $p_1$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell_{\\text{CE}} \n",
    "= -(1 - y)\\log (1 - p_1) - y \\log p_1\n",
    "\\; = \\begin{cases} \n",
    "    -\\log \\;(1 - p_1)  \\quad &{y = 0} \\\\ \n",
    "    -\\log \\; p_1 \\quad &{y = 1}.\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let $\\boldsymbol{\\mathsf{s}} = f(\\boldsymbol{\\mathsf{x}}) \\in \\mathbb{R}^2$ be the network output. Recall that\n",
    "the softmax probabilities are given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{p} = \\text{Softmax}(\\boldsymbol{\\mathsf{s}})\n",
    "&= \\left(\\frac{e^{s_0}}{e^{s_0} + e^{s_1}}, \\frac{e^{s_1}}{e^{s_0} + e^{s_1}}\\right) \\\\\n",
    "&= \\left(\\frac{1}{1 + e^{(s_1 - s_0)}}, \\frac{1}{1 + e^{-(s_1 - s_0)}}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then, the probability of the positive class can be written as:\n",
    "\n",
    "$$p_1 = \\text{Sigmoid}(\\Delta s)$$\n",
    "\n",
    "where $\\Delta s := s_1 - s_0.$ \n",
    "This can now be used to calculate the cross-entropy by using \n",
    "\n",
    "$$-\\log\\,\\text{Sigmoid}(\\Delta s) = \\log\\left(1 + e^{-\\Delta s}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0a1fa",
   "metadata": {
    "papermill": {
     "duration": 0.001695,
     "end_time": "2024-09-23T08:59:17.596967",
     "exception": false,
     "start_time": "2024-09-23T08:59:17.595272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "which is more numerically stable than calculating the two operations sequentially. \n",
    "Note that $\\Delta s = (\\boldsymbol{\\theta}_1  - \\boldsymbol{\\theta}_0)^\\top \\boldsymbol{\\mathsf{z}} + (b_1 - b_0)$ since the logits layer is linear.\n",
    "Thus, we can train an equivalent scalar-valued model with these fused weights that models $\\Delta s.$ This model predicts the positive class whenever $\\Delta s \\geq 0,$ i.e. $s_1 \\geq s_0.$ The scalar-valued model can then be converted to the two-valued model by assigning zero weights and bias to the negative class.\n",
    "\n",
    "\n",
    "**Remark.** This is another nice property of using the exponential to convert scores to probabilities, i.e. it converts a sum to a product, allowing fusing the weights of the logits layer to get one separating hyperplane."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.956387,
   "end_time": "2024-09-23T08:59:17.719267",
   "environment_variables": {},
   "exception": null,
   "input_path": "01d-appendix-bce.ipynb",
   "output_path": "01d-appendix-bce.ipynb",
   "parameters": {},
   "start_time": "2024-09-23T08:59:16.762880",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}