
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Backpropagation on DAGs &#8212; 𝗦𝘁𝗲𝗲𝗽𝗲𝘀𝘁 𝗔𝘀𝗰𝗲𝗻𝘁 ⛰️</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../_static/loss_surface.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Neural Networks with TensorFlow" href="seb3/tensorflow-nn.html" />
    <link rel="prev" title="Hyperparameter Tuning with Optuna" href="optuna.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/loss_surface.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">𝗦𝘁𝗲𝗲𝗽𝗲𝘀𝘁 𝗔𝘀𝗰𝗲𝗻𝘁 ⛰️</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  FUNDAMENTALS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="house-prices.html">
   Pipelines in Scikit-Learn
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="blending-stacking.html">
   Blending and Stacking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optuna.html">
   Hyperparameter Tuning with Optuna
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  DEEP LEARNING
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Backpropagation on DAGs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="seb3/tensorflow-nn.html">
   Neural Networks with TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="seb3/tensorflow-mechanics.html">
   Mechanics of TensorFlow
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  PYTORCH TUTORIALS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="pytorch-intro.html">
   Introduction to PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pytorch-activation.html">
   Activation Functions
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  REST APIs with FASTAPI
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="fastapi/ch3.html">
   Developing a RESTful API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fastapi/ch4.html">
   Managing Pydantic Data Models
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/backpropagation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/particle1331/steepest-ascent/issues/new?title=Issue%20on%20page%20%2Fnotebooks/backpropagation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/particle1331/steepest-ascent/master?urlpath=tree/docs/notebooks/backpropagation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-on-the-loss-surface">
   Gradient descent on the loss surface
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backpropagation-on-computational-graphs">
   Backpropagation on Computational Graphs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#autodifferentiation-with-pytorch-autograd">
   Autodifferentiation with PyTorch
   <code class="docutils literal notranslate">
    <span class="pre">
     autograd
    </span>
   </code>
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="backpropagation-on-dags">
<h1>Backpropagation on DAGs<a class="headerlink" href="#backpropagation-on-dags" title="Permalink to this headline">¶</a></h1>
<p>In this notebook, we look at <strong>backpropagation</strong> (BP) on <strong>directed acyclic computational graphs</strong> (DAG). Our main result is that a single training step (consisting of both a forward and a backward pass into the network) has a time complexity that is linear in the network size. In the last section, we take a closer look at the implementation of <code class="docutils literal notranslate"><span class="pre">.backward</span></code> in PyTorch.</p>
<p><strong>Readings</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/">Evaluating <span class="math notranslate nohighlight">\(\nabla f(x)\)</span> is as fast as <span class="math notranslate nohighlight">\(f(x)\)</span></a></p></li>
<li><p><a class="reference external" href="http://www.offconvex.org/2016/12/20/backprop/">Back-propagation, an introduction</a></p></li>
</ul>
<div class="section" id="gradient-descent-on-the-loss-surface">
<h2>Gradient descent on the loss surface<a class="headerlink" href="#gradient-descent-on-the-loss-surface" title="Permalink to this headline">¶</a></h2>
<p>Training neural networks is done by minimizing a <strong>loss function</strong> which is an almost everywhere differentiable function <span class="math notranslate nohighlight">\(\ell\colon \mathbb R \times \mathbb R \to [0, +\infty).\)</span> If we assume that an underlying distribution generates the data, then the the expected loss with the current weights is</p>
<div class="math notranslate nohighlight">
\[\mathcal L  (\mathbf w) = \mathbb E_{(\mathbf x, y)} \ell(y, f(\mathbf x; \mathbf w)).\]</div>
<p>But what motivates the choice of loss function? In practice, the function <span class="math notranslate nohighlight">\(\ell\)</span> is a surrogate objective that we want to minimize in order to minimize (on expectation) some metric that we care about (such as accuracy). In practical deep learning tasks where we have a dataset <span class="math notranslate nohighlight">\(\mathcal X\)</span> of independently sampled data points <span class="math notranslate nohighlight">\(\mathbf x_1, \ldots, \mathbf x_N\)</span>, we use the following approximation as our optimization objective:</p>
<div class="math notranslate nohighlight">
\[\mathcal L(\mathbf w; \mathcal X) = \frac{1}{N}\sum_{i=1}^N \ell (y, f(\mathbf x_i; \mathbf w)).\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Loss surfaces from samples</strong></p>
</div>
<p>Observe that this forms surface in <span class="math notranslate nohighlight">\(\mathbb R^d \times \mathbb R\)</span> where <span class="math notranslate nohighlight">\(d\)</span> is the number of parameters of the network, with the current parameter setting being a point <span class="math notranslate nohighlight">\((\mathbf w, \mathcal L(\mathcal X, \mathbf w))\)</span> on this surface which will generally vary with <span class="math notranslate nohighlight">\(\mathcal X.\)</span> However, we expect these surfaces to be similar for large <span class="math notranslate nohighlight">\(N\)</span> since the points sampled from the same distribution. To find the minimum of this surface, we use variants of <strong>gradient descent</strong> characterized by the update rule</p>
<div class="math notranslate nohighlight">
\[\mathbf w \leftarrow \mathbf w - \varepsilon \nabla_{\mathbf w} \mathcal L\]</div>
<p>where <span class="math notranslate nohighlight">\(-\nabla_{\mathbf w} \mathcal L\)</span> is the direction of steepest descent at <span class="math notranslate nohighlight">\(\mathbf w\)</span> and <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> is some positive number called the <strong>learning rate</strong>. Note that we can compute the gradient as the average of gradients <span class="math notranslate nohighlight">\(\nabla_\mathbf w \mathcal L (\mathbf x_i, \mathbf w)\)</span> at the point <span class="math notranslate nohighlight">\(\mathbf w\)</span> on the loss surface generated by the data point <span class="math notranslate nohighlight">\(\mathbf x_i.\)</span></p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>The need for efficient BP.</strong></p>
</div>
<p>Since <span class="math notranslate nohighlight">\(\nabla_\mathbf w \mathcal L\)</span> consists of partial derivatives for each weight in the network, this can be really large — millions, or even billions for SOTA models. How do we compute these derivatives efficiently? Because we have to compute the gradient at the current state of the network, we would have to perform a forward pass to compute all parameter values given <span class="math notranslate nohighlight">\(\mathbf w\)</span> up to the final node. This is followed by a backward pass where we compute compute every partial derivative by a clever use of the chain rule, recursively computing the partial derivative from the output to the input layer. Both forward and backward passes will have to be implemented efficiently for this to be usable in practice.</p>
</div>
<div class="section" id="backpropagation-on-computational-graphs">
<h2>Backpropagation on Computational Graphs<a class="headerlink" href="#backpropagation-on-computational-graphs" title="Permalink to this headline">¶</a></h2>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Forward pass</strong></p>
</div>
<p>A neural network can be modelled as a <strong>directed acyclic graph</strong> (DAG) of compute and parameter nodes that implements a function <span class="math notranslate nohighlight">\(f\)</span> and can be extended to implement the calculation of the loss value for each training example and parameter values. To compute <span class="math notranslate nohighlight">\(f(\mathbf x),\)</span> the values for each node are calculated from bottom to top starting from the input nodes. Every value in the nodes is stored to avoid recomputing any known value. Assuming each activation and each arithmetic operation between weights, biases and compute nodes take constant time, then one forward pass takes <span class="math notranslate nohighlight">\( O(V + E)\)</span> calculations were <span class="math notranslate nohighlight">\(V\)</span> is the number of activations, and <span class="math notranslate nohighlight">\(E\)</span> is the number of trainable parameters of the network — i.e. the network size. Around this is also the memory complexity of the whole operation.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Backward pass</strong></p>
</div>
<p>During backward pass, we divide gradients into two groups: <strong>local gradients</strong> obtained when perturbing adjacent compute nodes <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(w\)</span>, and <strong>backpropagated gradients</strong> of the form <span class="math notranslate nohighlight">\({\frac{\partial{\mathcal L}}{\partial u}}\)</span> for a node <span class="math notranslate nohighlight">\({u}.\)</span> Our goal is to calculate the backpropagated gradient of the loss with respect to parameter nodes. Note that parameter nodes have zero fan-in. BP proceeds by recursively. First, <span class="math notranslate nohighlight">\(\frac{\partial{\mathcal L}}{\partial \mathcal L} = 1\)</span> is stored as gradient of the node which computes the loss value. Suppose <span class="math notranslate nohighlight">\({\frac{\partial{\mathcal L}}{\partial u}}\)</span> are stored for each compute node <span class="math notranslate nohighlight">\(u\)</span> in the upper layer, then after computing local gradients <span class="math notranslate nohighlight">\({\frac{\partial{u}}{\partial w}}\)</span>, the backpropagated gradients <span class="math notranslate nohighlight">\({\frac{\partial{\mathcal L}}{\partial w}}\)</span> for compute nodes <span class="math notranslate nohighlight">\(w\)</span> can be calculated via the chain rule:</p>
<div class="math notranslate nohighlight">
\[{ \frac{\partial\mathcal L}{\partial w} } = \sum_{ {u} }\left( {{\frac{\partial\mathcal L}{\partial u}}} \right)\left( {{\frac{\partial{u}}{\partial w}}} \right).\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>BP is a useful tool for understanding how derivatives flow through a model. This can be extremely helpful in reasoning about why some models are difficult to optimize. Classic examples are vanishing or exploding gradients as we go into deeper layers of the network.</p>
</div>
<p>Thus, continuing the “flow” of gradients to the current layer. The process ends on  nodes with zero fan-in. Note that the partial derivatives are evaluated on the current network state — these values are stored during forward pass which precedes backward pass. Analogously, all backpropagated gradients are stored in each compute node for use by the next layer. On the other hand, there is no need to store local gradients; these are computed as needed. Hence, it suffices to compute all gradients with respect to compute nodes to get all gradients with respect to the weights of the network.</p>
<div class="figure align-default" id="backprop-compgraph">
<a class="reference internal image-reference" href="../_images/backprop-compgraph.png"><img alt="../_images/backprop-compgraph.png" src="../_images/backprop-compgraph.png" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">BP on a generic comp. graph with fan out &gt; 1 on node <code>y</code>. Each backpropagated gradient computation is stored in the corresponding node. For node <code>y</code> to calculate the backpropagated gradient we have to sum over the two incoming gradients which can be implemented using matrix multiplication of the gradient vectors.</span><a class="headerlink" href="#backprop-compgraph" title="Permalink to this image">¶</a></p>
</div>
<p><span class="math notranslate nohighlight">\(\phantom{3}\)</span></p>
<p><strong>Backpropagation algorithm.</strong> Now that we know how to compute each backpropagated gradient implemented as <code class="docutils literal notranslate"><span class="pre">u.backward()</span></code> for node <code class="docutils literal notranslate"><span class="pre">u</span></code> which sends its gradient <span class="math notranslate nohighlight">\(\frac{\partial \mathcal L}{\partial u}\)</span> to all its parent nodes, i.e. nodes on the lower layer. We now write the complete algorithm:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Forward</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">compute</span><span class="p">:</span> 
        <span class="n">c</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">Backward</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">compute</span><span class="p">:</span> <span class="n">c</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>  <span class="n">c</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>  <span class="n">c</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">compute</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span> 
        <span class="n">c</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">SGD</span><span class="p">(</span><span class="n">eta</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
        <span class="n">w</span><span class="o">.</span><span class="n">value</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
<div class="admonition-bp-equations-for-mlps admonition">
<p class="admonition-title">BP equations for MLPs</p>
<p>Consider an MLP which is clearly a computational DAG. Let <span class="math notranslate nohighlight">\({z_j}^{[t]} = \sum_k {w_{jk}}^{[t]}{a_k}^{[t-1]}\)</span> and <span class="math notranslate nohighlight">\({a_j}^{[t]} = \phi^{[t]}({\mathbf z}^{[t]})\)</span> be the values of compute nodes at the <span class="math notranslate nohighlight">\(t\)</span>-th layer of the network. The backpropagated gradients for the compute nodes of the current layer are given by</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
        \dfrac{\partial \mathcal L}{\partial {a_j}^{[t]}} 
        &amp;= \sum_{k}\dfrac{\partial \mathcal L}{\partial {z_k}^{[t+1]}} \dfrac{\partial {z_k}^{[t+1]}}{\partial {a_j}^{[t]}} = \sum_{k}\dfrac{\partial \mathcal L}{\partial {z_k}^{[t+1]}} {w_{kj}}^{[t+1]}
    \end{aligned}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
    \dfrac{\partial \mathcal L}{\partial {z_j}^{[t]}} 
    &amp;= \sum_{l}\dfrac{\partial \mathcal L}{\partial {a_l}^{[t]}} \dfrac{\partial {a_l}^{[t]}}{\partial {z_j}^{[t]}}.
\end{aligned}\]</div>
<p>This sum typically reduces to a single term for activations such as ReLU, but not for activations which depend on multiple preactivations such as softmax. Similarly, the backpropagated gradients for the parameter nodes (weights and biases) are given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \dfrac{\partial \mathcal L}{\partial {w_{jk}}^{[t]}} 
    &amp;= \dfrac{\partial \mathcal L}{\partial {z_j}^{[t]}} \dfrac{\partial {z_j}^{[t]}}{\partial {w_{jk}}^{[t]}} = \dfrac{\partial \mathcal L}{\partial {z_j}^{[t]}} {a^{[t-1]}_k} \\
    \text{and}\qquad\dfrac{\partial \mathcal L}{\partial {b_{j}}^{[t]}} 
    &amp;= \dfrac{\partial \mathcal L}{\partial {z_j}^{[t]}} \dfrac{\partial {z_j}^{[t]}}{\partial {b_{j}}^{[t]}} = \dfrac{\partial \mathcal L}{\partial {z_j}^{[t]}}.
\end{aligned}\end{split}\]</div>
<p>Backpropagated gradients for compute nodes are stored until the weights are updated, e.g. <span class="math notranslate nohighlight">\(\frac{\partial \mathcal L}{\partial {z_k}^{[t+1]}}\)</span> are retrieved in the compute nodes of the <span class="math notranslate nohighlight">\(t+1\)</span>-layer to compute gradients in the <span class="math notranslate nohighlight">\(t\)</span>-layer. On the other hand, the local gradients <span class="math notranslate nohighlight">\(\frac{\partial {a_k}^{[t]}}{\partial {z_j}^{[t]}}\)</span> are computed directly using autodifferentiation and evaluated with the current network state obtained during forward pass.</p>
</div>
<br><p>We highlight two important properties of the algorithm which makes it the practical choice for training huge neural networks:</p>
<ul class="simple">
<li><p><strong>Modularity.</strong> The dependence only on nodes belonging to the upper layer suggests a modularity in the computation, e.g. we can connect DAG subnetworks with possibly distinct network architectures by only connecting nodes that are exposed between layers.</p></li>
</ul>
<br>
<ul class="simple">
<li><p><strong>Bottleneck and complexity.</strong> Assuming each computation of a local derivative takes constant time (e.g. with autodifferentiation), then backward pass requires <span class="math notranslate nohighlight">\(O(V + E)\)</span> computations. This takes into account gradient flows across <span class="math notranslate nohighlight">\(E\)</span> trainable parameters (including biases) and <span class="math notranslate nohighlight">\(V\)</span> gradient flows across activations. It follows that fast matrix multiplication, e.g. by having dedicated hardware such as GPUs, must be developed to make neural networks train fast. Finally, since gradients are stored in nodes, the memory complexity should also depend in the network size (as well as the size of the gradients).</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\phantom{3}\)</span></p>
<p>The following two figures show BP on a logistic regression model.</p>
<div class="figure align-default" id="backprop-compgraph2">
<a class="reference internal image-reference" href="../_images/backprop-compgraph2.png"><img alt="../_images/backprop-compgraph2.png" src="../_images/backprop-compgraph2.png" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">Backprop with weights for a single layer neural network with sigmoid activation and cross-entropy loss. Observe the gradient flowing from node <code>L</code> to the node <code>w0</code>.</span><a class="headerlink" href="#backprop-compgraph2" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="backprop-compgraph3">
<a class="reference internal image-reference" href="../_images/backprop-compgraph3.png"><img alt="../_images/backprop-compgraph3.png" src="../_images/backprop-compgraph3.png" style="width: 25em;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">Backprop with weights for a single layer neural network with sigmoid activation and cross-entropy loss. Local gradients that require current values of the nodes while backpropagated gradients are accessed from the layer above. Node <code>u</code> which has fan-in &gt; 1 performs chain rule on the backpropagated gradients.</span><a class="headerlink" href="#backprop-compgraph3" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="autodifferentiation-with-pytorch-autograd">
<h2>Autodifferentiation with PyTorch <code class="docutils literal notranslate"><span class="pre">autograd</span></code><a class="headerlink" href="#autodifferentiation-with-pytorch-autograd" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">autograd</span></code> package allows automatic differentiation by building computational graphs on the fly every time we pass data through our model. Autograd tracks which data combined through which operations to produce the output. This allows us to take derivatives over ordinary imperative code. This functionality is consistent with the memory and time requirements outlined in above for BP.</p>
<br>
<p><strong>Backward for scalars.</strong> Let <span class="math notranslate nohighlight">\(y = \mathbf x^\top \mathbf x = \sum_i {x_i}^2.\)</span> In this example, we initialize a tensor <code class="docutils literal notranslate"><span class="pre">x</span></code> which initially has no gradient. Calling backward on <code class="docutils literal notranslate"><span class="pre">y</span></code> results in gradients being stored on the leaf tensor <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">x</span> 

<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> 
<span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(True)
</pre></div>
</div>
</div>
</div>
<p><strong>Backward for vectors.</strong> Let <span class="math notranslate nohighlight">\(\mathbf y = g(\mathbf x)\)</span> and let <span class="math notranslate nohighlight">\(\mathbf v\)</span> be a vector having the same length as <span class="math notranslate nohighlight">\(\mathbf y.\)</span> Then <code class="docutils literal notranslate"><span class="pre">y.backward(v)</span></code> implements</p>
<div class="math notranslate nohighlight">
\[\sum_i v_i \left(\frac{\partial y_i}{\partial x_j}\right)\]</div>
<p>resulting in a vector of same length as <code class="docutils literal notranslate"><span class="pre">x</span></code> that is stored in <code class="docutils literal notranslate"><span class="pre">x.grad</span></code>. Note that the terms on the right are the local gradients in backprop. Hence, if <code class="docutils literal notranslate"><span class="pre">v</span></code> contains backpropagated gradients of nodes that depend on <code class="docutils literal notranslate"><span class="pre">y</span></code>, then this operation gives us the backpropagated gradients with respect to <code class="docutils literal notranslate"><span class="pre">x</span></code>, i.e. setting <span class="math notranslate nohighlight">\(v_i = \frac{\partial \mathcal{L} }{\partial y_i}\)</span> gives us the vector <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L} }{\partial x_j}.\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># Computing the Jacobian by hand</span>
<span class="n">J</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span>
<span class="p">)</span>

<span class="c1"># Confirming the above formula</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">v</span> <span class="o">@</span> <span class="n">J</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(True)
</pre></div>
</div>
</div>
</div>
<p><strong>Locally disabling gradient tracking.</strong> To stop PyTorch from building computational graphs, we can put the code inside a <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">torch.no_grad()</span></code> block. In this mode, the result of every computation will have <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>, even when the inputs have <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>.
<br><br>
Another method is to use the <code class="docutils literal notranslate"><span class="pre">.detach()</span></code> method which returns a new tensor detached from the current graph but shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. Disabling gradient computation is useful when computing values, e.g. accuracy, whose gradients will not be backpropagated into the network.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="optuna.html" title="previous page">Hyperparameter Tuning with Optuna</a>
    <a class='right-next' id="next-link" href="seb3/tensorflow-nn.html" title="next page">Neural Networks with TensorFlow</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By 𝗥𝗼𝗻 𝗠𝗲𝗱𝗶𝗻𝗮. Powered by <a href="https://jupyterbook.org">Jupyter Book</a>.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>