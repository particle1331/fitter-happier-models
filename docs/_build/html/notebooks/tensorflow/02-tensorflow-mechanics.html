
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Mechanics of TensorFlow &#8212; 𝗜𝗻𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝗡𝗲𝘁𝘄𝗼𝗿𝗸𝘀</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../../_static/pone.0237978.g001.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Activation Functions" href="03-tensorflow-activations.html" />
    <link rel="prev" title="TensorFlow Datasets" href="01-tensorflow-nn.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/pone.0237978.g001.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">𝗜𝗻𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝗡𝗲𝘁𝘄𝗼𝗿𝗸𝘀</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  FUNDAMENTALS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/house-prices.html">
   Pipelines in
   <code class="docutils literal notranslate">
    <span class="pre">
     scikit-learn
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/blending-stacking.html">
   Blending and Stacking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/optuna.html">
   Model Tuning with Optuna
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/missing.html">
   Handling Missing Values
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DEEP LEARNING
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/backpropagation.html">
   Backpropagation on DAGs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01-tensorflow-nn.html">
   TensorFlow Datasets
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Mechanics of TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-tensorflow-activations.html">
   Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-tensorflow-optim-init.html">
   Initialization and Optimization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MODEL DEPLOYMENT
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../deployment/production-code.html">
   Packaging Production Code
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/tensorflow/02-tensorflow-mechanics.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/particle1331/inefficient-networks"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/particle1331/inefficient-networks/issues/new?title=Issue%20on%20page%20%2Fnotebooks/tensorflow/02-tensorflow-mechanics.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/particle1331/inefficient-networks/master?urlpath=tree/docs/notebooks/tensorflow/02-tensorflow-mechanics.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#static-graph-execution">
   Static graph execution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#specifying-input-signature-and-static-graph-tracing">
     Specifying input signature and static graph tracing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#timing-static-execution-runs">
     Timing static execution runs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensorflow-variable">
   TensorFlow
   <code class="docutils literal notranslate">
    <span class="pre">
     Variable
    </span>
   </code>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modifying-the-value-of-a-variable">
     Modifying the value of a variable
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initializing-a-tensorflow-module">
     Initializing a TensorFlow module
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variables-and-tf-functions">
     Variables and TF functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#automatic-differentiation">
   Automatic Differentiation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradients-of-the-loss-with-respect-to-weights">
     Gradients of the loss with respect to weights
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#higher-order-gradients">
       Higher-order gradients
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradients-with-respect-to-nontrainable-parameters">
     Gradients with respect to nontrainable parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#persisting-the-gradient-tape">
     Persisting the gradient tape
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applying-optimizer-step-to-update-model-parameters">
     Applying optimizer step to update model parameters
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#keras-api">
   Keras API
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stacking-layers-with-tf-keras-sequential">
     Stacking layers with
     <code class="docutils literal notranslate">
      <span class="pre">
       tf.keras.Sequential
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-and-initialization">
     Regularization and initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compiling-keras-models">
     Compiling Keras models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#api-for-losses">
       API for losses
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#solving-the-xor-problem">
     Solving the XOR problem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keras-functional-api">
     Keras’ Functional API
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keras-model-class">
     Keras’
     <code class="docutils literal notranslate">
      <span class="pre">
       Model
      </span>
     </code>
     class
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-custom-keras-layers">
     Creating custom Keras layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#saving-and-loading-models">
     Saving and loading models
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Mechanics of TensorFlow</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#static-graph-execution">
   Static graph execution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#specifying-input-signature-and-static-graph-tracing">
     Specifying input signature and static graph tracing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#timing-static-execution-runs">
     Timing static execution runs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensorflow-variable">
   TensorFlow
   <code class="docutils literal notranslate">
    <span class="pre">
     Variable
    </span>
   </code>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modifying-the-value-of-a-variable">
     Modifying the value of a variable
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initializing-a-tensorflow-module">
     Initializing a TensorFlow module
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variables-and-tf-functions">
     Variables and TF functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#automatic-differentiation">
   Automatic Differentiation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradients-of-the-loss-with-respect-to-weights">
     Gradients of the loss with respect to weights
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#higher-order-gradients">
       Higher-order gradients
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradients-with-respect-to-nontrainable-parameters">
     Gradients with respect to nontrainable parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#persisting-the-gradient-tape">
     Persisting the gradient tape
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applying-optimizer-step-to-update-model-parameters">
     Applying optimizer step to update model parameters
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#keras-api">
   Keras API
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stacking-layers-with-tf-keras-sequential">
     Stacking layers with
     <code class="docutils literal notranslate">
      <span class="pre">
       tf.keras.Sequential
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-and-initialization">
     Regularization and initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compiling-keras-models">
     Compiling Keras models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#api-for-losses">
       API for losses
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#solving-the-xor-problem">
     Solving the XOR problem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keras-functional-api">
     Keras’ Functional API
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keras-model-class">
     Keras’
     <code class="docutils literal notranslate">
      <span class="pre">
       Model
      </span>
     </code>
     class
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-custom-keras-layers">
     Creating custom Keras layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#saving-and-loading-models">
     Saving and loading models
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="mechanics-of-tensorflow">
<h1>Mechanics of TensorFlow<a class="headerlink" href="#mechanics-of-tensorflow" title="Permalink to this headline">¶</a></h1>
<p><img alt="Status" src="https://img.shields.io/static/v1.svg?label=Status&amp;message=Finished&amp;color=green" /></p>
<p>In this notebook, we take a deeper dive into lower-level features of TensorFlow. For example, accessing and modifying layer weights and weight gradients, performing automatic differentiation, creating custom layers, and so on. Knowing these tricks would allow us to write more advanced TensorFlow models and write custom functionality for our models and training pipeline.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">))</span>

<span class="c1"># plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span> 
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.7.0
[PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>⚠️ <strong>Attribution:</strong> This notebook builds on the <a class="reference external" href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch14">Chapter 14 notebook</a> of <span id="id1">[<a class="reference internal" href="../../intro.html#id7">RM19</a>]</span> which is <a class="reference external" href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt">released under MIT License</a>. For instance, we add timing static graph executions to see the conditions where we can expect a speedup in our code.</p>
</div>
<div class="section" id="static-graph-execution">
<h2>Static graph execution<a class="headerlink" href="#static-graph-execution" title="Permalink to this headline">¶</a></h2>
<p>Computations with eager execution are not as efficient
as the static graph execution in TensorFlow v1.x, as these can come with pure Python operations.
TensorFlow v2 provides a tool called <strong>AutoGraph</strong> that can automatically transform Python code into
TensorFlow’s graph code for faster execution. Fortunately for us, TensorFlow provides
a simple mechanism for compiling a normal Python function to do exactly this: <code class="docutils literal notranslate"><span class="pre">graph_function</span> <span class="pre">=</span> <span class="pre">tf.function(eager_function)</span></code> or using the <code class="docutils literal notranslate"><span class="pre">&#64;tf.function</span></code> decorator.</p>
<div class="section" id="specifying-input-signature-and-static-graph-tracing">
<h3>Specifying input signature and static graph tracing<a class="headerlink" href="#specifying-input-signature-and-static-graph-tracing" title="Permalink to this headline">¶</a></h3>
<p>Note that while TensorFlow graphs, strictly speaking, require static types and shapes,
<code class="docutils literal notranslate"><span class="pre">tf.function</span></code> readily supports such a dynamic typing capability (through separate static graphs will be created under the hood). For example, let’s call this function
with the following inputs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span>

<span class="n">f_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;Scalar Inputs:&#39;</span><span class="p">,</span> <span class="n">f_graph</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;Rank 1 Inputs:&#39;</span><span class="p">,</span> <span class="n">f_graph</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;Rank 2 Inputs:&#39;</span><span class="p">,</span> <span class="n">f_graph</span><span class="p">([[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">]]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Metal device set to: Apple M1

systemMemory: 8.00 GB
maxCacheSize: 2.67 GB

Scalar Inputs: 6
Rank 1 Inputs: [1, 2, 3]
Rank 2 Inputs: [[1], [2], [3]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:41:46.808741: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-03-13 07:41:46.809194: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)
2022-03-13 07:41:46.842530: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2022-03-13 07:41:46.858840: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2022-03-13 07:41:46.887922: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
</div>
</div>
<p>Here, TensorFlow uses a <strong>tracing mechanism</strong> to construct a graph based on the input arguments. For this tracing mechanism, TensorFlow generates a tuple of keys based on the input signatures
given for calling the function. The generated keys are as follows:</p>
<ul class="simple">
<li><p>For <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> arguments, the key is based on their shapes and <code class="docutils literal notranslate"><span class="pre">dtypes</span></code>.</p></li>
<li><p>For Python types, such as lists, their <code class="docutils literal notranslate"><span class="pre">id()</span></code> is used to generate cache keys.</p></li>
<li><p>For Python primitive values, the cache keys are based on the input values.</p></li>
</ul>
<p>Upon calling such a decorated function, TensorFlow will check whether a graph with
the corresponding key has already been generated. If such a graph does not exist,
TensorFlow will generate a new graph and store the new key.
If we want to limit the way a function can be called, we can specify its input signature
via a tuple of <code class="docutils literal notranslate"><span class="pre">tf.TensorSpec</span></code> objects when defining the function. For example, let’s
take the previous function and modify it so that only rank 1 tensors of
type <code class="docutils literal notranslate"><span class="pre">tf.int32</span></code> are allowed:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span>

<span class="n">f_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">input_signature</span><span class="o">=</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;Rank 1 Inputs:&#39;</span><span class="p">,</span> <span class="n">f_graph</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Rank 1 Inputs: [6]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:41:46.953430: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
</div>
</div>
<p>We get an error if we pass a tensor with different shape:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;Rank 1 Inputs:&#39;</span><span class="p">,</span> <span class="n">f_graph</span><span class="p">([[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">]]))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Python inputs incompatible with input_signature:
  inputs: (
    [[1]],
    [[2]],
    [[3]])
  input_signature: (
    TensorSpec(shape=(None,), dtype=tf.int32, name=None),
    TensorSpec(shape=(None,), dtype=tf.int32, name=None),
    TensorSpec(shape=(None,), dtype=tf.int32, name=None)).
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="timing-static-execution-runs">
<h3>Timing static execution runs<a class="headerlink" href="#timing-static-execution-runs" title="Permalink to this headline">¶</a></h3>
<p>In this section, we define a function that takes in an eager function <code class="docutils literal notranslate"><span class="pre">f</span></code> and plots the timings for evaluating <code class="docutils literal notranslate"><span class="pre">f</span></code> eagerly on <code class="docutils literal notranslate"><span class="pre">x</span></code> versus evaluating on its static graph version <code class="docutils literal notranslate"><span class="pre">tf.function(f)</span></code>. As discussed above, the static graph is built after “warming up” once on <code class="docutils literal notranslate"><span class="pre">x</span></code> with its particular shape and type through TensorFlow’s tracing mechanism.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">timeit</span> <span class="kn">import</span> <span class="n">timeit</span>

<span class="k">def</span> <span class="nf">compare_timings</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="c1"># Define functions</span>
    <span class="n">eager_function</span> <span class="o">=</span> <span class="n">f</span>
    <span class="n">graph_function</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="c1"># Timing</span>
    <span class="n">graph_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">graph_function</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">number</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">eager_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">eager_function</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">number</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;graph&#39;</span><span class="p">:</span> <span class="n">graph_time</span><span class="p">,</span>
        <span class="s1">&#39;eager&#39;</span><span class="p">:</span> <span class="n">eager_time</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>For further info on TF 2.x’s tracing mechanism, refer to <a class="reference external" href="https://www.tensorflow.org/guide/function#tracing">this guide</a>.</p>
</div>
<p>Note that if we fail to persist the static graph, we get the following warning, as well as practically an endless loop. The error message also highlights cases where we make inefficient use of tracing. (Recall the rules above for generating keys for static graphs based on the input.)</p>
<blockquote>
<div><p>WARNING:tensorflow:6 out of the last 6 calls to &lt;keras.engine.sequential.Sequential object at 0x28575dfa0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating &#64;tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your &#64;tf.function outside of the loop.</p>
</div></blockquote>
<p>Comparing static graph execution with eager execution on a dense network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span>

<span class="c1"># Model building</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;softmax&quot;</span><span class="p">))</span>

<span class="c1"># Define input + functions</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="n">mlp_times</span> <span class="o">=</span> <span class="n">compare_timings</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10000</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:41:47.305809: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
</div>
</div>
<p>Comparing timings with convolution operations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">AveragePooling2D</span>

<span class="c1"># Build model</span>
<span class="n">conv_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">conv_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">conv_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">AveragePooling2D</span><span class="p">())</span>
<span class="n">conv_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">conv_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">AveragePooling2D</span><span class="p">())</span>

<span class="c1"># Plot timings</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">conv_times</span> <span class="o">=</span> <span class="n">compare_timings</span><span class="p">(</span><span class="n">conv_model</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10000</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:42:02.230240: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
</div>
</div>
<p>Comparing timings on many extremely small operations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">small_dense_layer</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># Plot timings</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">small_times</span> <span class="o">=</span> <span class="n">compare_timings</span><span class="p">(</span><span class="n">small_dense_layer</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10000</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:42:12.145609: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mlp&#39;</span><span class="p">,</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span><span class="s1">&#39;small&#39;</span><span class="p">]</span>
<span class="n">eager</span> <span class="o">=</span> <span class="p">[</span><span class="nb">eval</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="s1">&#39;_times&#39;</span><span class="p">)[</span><span class="s1">&#39;eager&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span><span class="p">]</span>
<span class="n">graph</span> <span class="o">=</span> <span class="p">[</span><span class="nb">eval</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="s1">&#39;_times&#39;</span><span class="p">)[</span><span class="s1">&#39;graph&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">models</span><span class="p">))</span>


<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">eager</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;eager&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;graph&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">models</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Time (s)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Time for 10,000 executions.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/02-tensorflow-mechanics_26_0.svg" src="../../_images/02-tensorflow-mechanics_26_0.svg" /></div>
</div>
<p>The above results show that graph execution can be faster can be faster than eager code, especially for graphs with expensive operations. But for graphs with few expensive operations (like convolutions), you may not see much speedup or even worse with many cheap operations. Perhaps because there is overhead in the tracing mechanism for static graphs and alternating between TensorFlow and Python abstractions.</p>
</div>
</div>
<div class="section" id="tensorflow-variable">
<h2>TensorFlow <code class="docutils literal notranslate"><span class="pre">Variable</span></code><a class="headerlink" href="#tensorflow-variable" title="Permalink to this headline">¶</a></h2>
<p>A <code class="docutils literal notranslate"><span class="pre">Variable</span></code> is a special <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> object
that allows us to store and update the parameters of our models during training.
This can be created by just calling the <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> class on user-specified
initial values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;var_a&#39;</span><span class="p">)</span> <span class="c1"># float32 by default.</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;var_b&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Variable &#39;var_a:0&#39; shape=() dtype=float32, numpy=3.0&gt;
&lt;tf.Variable &#39;var_b:0&#39; shape=(3,) dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;
&lt;tf.Variable &#39;Variable:0&#39; shape=(1,) dtype=string, numpy=array([b&#39;c&#39;], dtype=object)&gt;
</pre></div>
</div>
</div>
</div>
<p>Note that initial value is required. TF variables have an attribute called <code class="docutils literal notranslate"><span class="pre">trainable</span></code>, which by default is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>. Higher-level APIs such as Keras will use this attribute to manage the trainable variables and non-trainable ones. You can define a non-trainable <code class="docutils literal notranslate"><span class="pre">Variable</span></code> as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">trainable</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<div class="section" id="modifying-the-value-of-a-variable">
<h3>Modifying the value of a variable<a class="headerlink" href="#modifying-the-value-of-a-variable" title="Permalink to this headline">¶</a></h3>
<p>The values of a <code class="docutils literal notranslate"><span class="pre">Variable</span></code> can be efficiently modified by running some operations
such as <code class="docutils literal notranslate"><span class="pre">.assign()</span></code>, <code class="docutils literal notranslate"><span class="pre">.assign_add()</span></code> and related methods. When the <code class="docutils literal notranslate"><span class="pre">read_value</span></code> argument is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> (default), these operations will automatically return the new values after updating the current values of the <code class="docutils literal notranslate"><span class="pre">Variable</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=0.0&gt;
</pre></div>
</div>
</div>
</div>
<p>Setting the <code class="docutils literal notranslate"><span class="pre">read_value</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> will suppress the automatic return of the updated value but the <code class="docutils literal notranslate"><span class="pre">Variable</span></code> will still be updated in place.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>None
-1
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="initializing-a-tensorflow-module">
<h3>Initializing a TensorFlow module<a class="headerlink" href="#initializing-a-tensorflow-module" title="Permalink to this headline">¶</a></h3>
<p>In practice, we usually define and initialize a <code class="docutils literal notranslate"><span class="pre">Variable</span></code> inside a <code class="docutils literal notranslate"><span class="pre">tf.Module</span></code> class. In the example below, we define two variables one trainable one and another non-trainable. These variables can be accessed using the <code class="docutils literal notranslate"><span class="pre">.variables</span></code> and <code class="docutils literal notranslate"><span class="pre">.trainable_variables</span></code> attribute of TF modules.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">GlorotNormal</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">init</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">init</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="n">m</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All module variables:&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">variables</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trainable variables:&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>All module variables: [TensorShape([2, 3]), TensorShape([1, 2])]
Trainable variables: [TensorShape([2, 3])]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:42:16.314299: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="variables-and-tf-functions">
<h3>Variables and TF functions<a class="headerlink" href="#variables-and-tf-functions" title="Permalink to this headline">¶</a></h3>
<p>Note that if we define a TF variable inside a pure Python function, then this variable will be initialized every time the function is called. Since the static graph will try to reuse the variable based on tracing and graph creation, TF prevents variable initialization inside a decorated TF function.</p>
<p>One way to avoid this problem is to define the <code class="docutils literal notranslate"><span class="pre">Variable</span></code> outside of the decorated
function and use it inside the function — this is not recommended with a global scope. Instead, you should define a class to manage this dependency in a separate namespace.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">3.0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span>

<span class="c1"># Testing</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">f</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>in user code:

    File &quot;/var/folders/jq/9vsvd9252_349lsng_5gc_jw0000gn/T/ipykernel_18558/2632006634.py&quot;, line 3, in f  *
        w = tf.Variable([3.0])

    ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.
</pre></div>
</div>
</div>
</div>
<p>Instead do:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Declare variable outside function &lt;- make sure to not pollute the global namespace</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">3.0</span><span class="p">])</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span>

<span class="c1"># Testing</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[3]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:42:16.571599: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="automatic-differentiation">
<h2>Automatic Differentiation<a class="headerlink" href="#automatic-differentiation" title="Permalink to this headline">¶</a></h2>
<p>TensorFlow supports automatic differentiation which implements symbolic differentiation for each operation defined in the language. For nested functions, TF provides a context called <code class="docutils literal notranslate"><span class="pre">GradientTape</span></code> for calculating gradients of these computed tensors with respect to its dependent nodes in the computation graph. This allows TensorFlow needs to remember what operations happen in what order during the forward pass. This list of operations is traversed backwards during <a class="reference external" href="https://particle1331.github.io/inefficient-networks/notebooks/fundamentals/backpropagation.html">backward pass</a> to compute the weight gradients.</p>
<div class="section" id="gradients-of-the-loss-with-respect-to-weights">
<h3>Gradients of the loss with respect to weights<a class="headerlink" href="#gradients-of-the-loss-with-respect-to-weights" title="Permalink to this headline">¶</a></h3>
<p>In order to compute these gradients, we have to “record” the computations via <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code>. Note that the shape of <code class="docutils literal notranslate"><span class="pre">tape.gradient(loss,</span> <span class="pre">w)</span></code> is the same as that of <code class="docutils literal notranslate"><span class="pre">w</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># scope outside tf.GradientTape</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="mf">1.4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="mf">2.1</span><span class="p">])</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">))</span>

<span class="n">grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;∂(loss)/∂w =&quot;</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c1"># symbolic</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂(loss)/∂w = -0.559999764
[-0.559999764]
</pre></div>
</div>
</div>
</div>
<div class="section" id="higher-order-gradients">
<h4>Higher-order gradients<a class="headerlink" href="#higher-order-gradients" title="Permalink to this headline">¶</a></h4>
<p>It turns out that TF supports stacking gradient tapes which allow us to compute <strong>second derivatives</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">outer_tape</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">inner_tape</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">))</span>
    <span class="n">grad_w</span> <span class="o">=</span> <span class="n">inner_tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">grad_wb</span> <span class="o">=</span> <span class="n">outer_tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">grad_w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;∂²(loss)/∂w∂b =&quot;</span><span class="p">,</span> <span class="n">grad_wb</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c1"># symbolic</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂²(loss)/∂w∂b = 2.8
[2.8]
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="gradients-with-respect-to-nontrainable-parameters">
<h3>Gradients with respect to nontrainable parameters<a class="headerlink" href="#gradients-with-respect-to-nontrainable-parameters" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">GradientTape</span></code> automatically supports the gradients for trainable variables.
For non-trainable variables<a class="footnote-reference brackets" href="#refadversarial" id="id2">1</a> and other <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> objects, we need to add
<code class="docutils literal notranslate"><span class="pre">tape.watch()</span></code> to monitor those as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># &lt;-</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">))</span>

<span class="n">grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;∂(loss)/∂x =&quot;</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">w</span><span class="p">))</span> <span class="c1"># check symbolic</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂(loss)/∂x = [-0.399999857]
[-0.399999857]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="persisting-the-gradient-tape">
<h3>Persisting the gradient tape<a class="headerlink" href="#persisting-the-gradient-tape" title="Permalink to this headline">¶</a></h3>
<p>Note that the tape will keep the resources only for a single gradient computation by default. So
after calling <code class="docutils literal notranslate"><span class="pre">tape.gradient()</span></code> once, the resources will be released and the tape will
be cleared. If we want to compute more than one gradient, we need to persist it (less memory efficient).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">))</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;∂(loss)/∂w =&quot;</span><span class="p">,</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;∂(loss)/∂x =&quot;</span><span class="p">,</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂(loss)/∂w = -0.559999764
A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">))</span>

<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;∂(loss)/∂w =&quot;</span><span class="p">,</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;∂(loss)/∂x =&quot;</span><span class="p">,</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span> <span class="c1"># grad_x has same shape as x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂(loss)/∂w = -0.559999764
∂(loss)/∂x = [-0.399999857]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="applying-optimizer-step-to-update-model-parameters">
<h3>Applying optimizer step to update model parameters<a class="headerlink" href="#applying-optimizer-step-to-update-model-parameters" title="Permalink to this headline">¶</a></h3>
<p>During SGD, we are computing gradients of a loss term with respect to model weights, which we use to update the weights according to some rule defined by an optimization algorithm. For Keras optimizers, we can do this by  using <code class="docutils literal notranslate"><span class="pre">.apply_gradients</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grad_w</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">grad_b</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;w =&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;b =&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;λ =&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;grad_[w, b] =&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">grad_w</span><span class="p">,</span> <span class="n">grad_b</span><span class="p">])</span>

<span class="c1"># Define keras optimizer; apply optimizer step</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">([</span><span class="n">grad_w</span><span class="p">,</span> <span class="n">grad_b</span><span class="p">],</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">]))</span>

<span class="c1"># Print updates</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;w - λ·∂(loss)/∂w ≟&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;b - λ·∂(loss)/∂b ≟&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = 1
b = 0.5
λ = 0.1
grad_[w, b] = [-0.559999764, -0.399999857]

w - λ·∂(loss)/∂w ≟ 1.056
b - λ·∂(loss)/∂b ≟ 0.539999962
</pre></div>
</div>
</div>
</div>
<p>Checks out nicely.</p>
</div>
</div>
<div class="section" id="keras-api">
<h2>Keras API<a class="headerlink" href="#keras-api" title="Permalink to this headline">¶</a></h2>
<p>Keras provides a user-friendly and
modular programming interface that allows easy prototyping and the building of
complex models in just a few lines of code which
in TensorFlow 2, has become the primary and recommended approach
for implementing models via the <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> library.  This has the advantage that it supports TensorFlow specific functionalities, such as <a class="reference external" href="https://particle1331.github.io/inefficient-networks/notebooks/tensorflow/01-tensorflow-nn.html">dataset pipelines using <code class="docutils literal notranslate"><span class="pre">tf.data</span></code></a>.</p>
<div class="section" id="stacking-layers-with-tf-keras-sequential">
<h3>Stacking layers with <code class="docutils literal notranslate"><span class="pre">tf.keras.Sequential</span></code><a class="headerlink" href="#stacking-layers-with-tf-keras-sequential" title="Permalink to this headline">¶</a></h3>
<p>For stacking layers that perform sequential transforms on input data, we typically use <code class="docutils literal notranslate"><span class="pre">tf.keras.Sequential()</span></code>. Such a model has an <code class="docutils literal notranslate"><span class="pre">add()</span></code> method to add individual Keras layers. Alternatively, if we have a list of layers <code class="docutils literal notranslate"><span class="pre">layers</span></code>, then we can define a model using <code class="docutils literal notranslate"><span class="pre">tf.keras.Sequential(layers)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="c1"># Create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>

<span class="c1"># Build model</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_4 (Dense)             (None, 16)                80        
                                                                 
 dense_5 (Dense)             (None, 32)                544       
                                                                 
 dense_6 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 657
Trainable params: 657
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>Once variables (or model parameters) are created, we can access both
trainable and non-trainable variables as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">name</span><span class="si">:</span><span class="s1">20s</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">trainable</span><span class="p">)</span><span class="si">:</span><span class="s1">7</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dense_4/kernel:0     True    (4, 16)
dense_4/bias:0       True    (16,)
dense_5/kernel:0     True    (16, 32)
dense_5/bias:0       True    (32,)
dense_6/kernel:0     True    (32, 1)
dense_6/bias:0       True    (1,)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="regularization-and-initialization">
<h3>Regularization and initialization<a class="headerlink" href="#regularization-and-initialization" title="Permalink to this headline">¶</a></h3>
<p>Layers can be configured using optional arguments to allow applying different activation
functions, choosing variable initializers, or choosing the type regularization to use.
Regularizers allow you to apply penalties on layer parameters or layer activity during optimization. These penalties are summed into the loss function that the network optimizes. For Keras, regularization penalties are applied on a per-layer basis. The <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer (as well as other layers such as <code class="docutils literal notranslate"><span class="pre">Conv1D</span></code>, <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code>, and <code class="docutils literal notranslate"><span class="pre">Conv3D</span></code>) exposes three keyword arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_regularizer</span></code>: Regularizer to apply a penalty on the layer’s kernel</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_regularizer</span></code>: Regularizer to apply a penalty on the layer’s bias</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activity_regularizer</span></code>: Regularizer to apply a penalty on the layer’s output</p></li>
</ul>
<br><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.keras.initializers</span> <span class="k">as</span> <span class="nn">init</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras.regularizers</span> <span class="k">as</span> <span class="nn">regularizers</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="k">as</span> <span class="nn">optim</span>


<span class="c1"># Create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> 
        <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">init</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">(),</span>
        <span class="n">bias_initializer</span><span class="o">=</span><span class="n">init</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">2.0</span><span class="p">),</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l1</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Build model</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_3&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_7 (Dense)             (None, 16)                80        
                                                                 
=================================================================
Total params: 80
Trainable params: 80
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="compiling-keras-models">
<h3>Compiling Keras models<a class="headerlink" href="#compiling-keras-models" title="Permalink to this headline">¶</a></h3>
<p>Compiling prepares the model for training via <code class="docutils literal notranslate"><span class="pre">model.fit()</span></code>. In this example, we will compile the model using the SGD optimizer, cross-entropy
loss for binary classification, and a specific list of metrics, including accuracy,
precision, and recall:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">(),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Precision</span><span class="p">(),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Recall</span><span class="p">()</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="api-for-losses">
<h4>API for losses<a class="headerlink" href="#api-for-losses" title="Permalink to this headline">¶</a></h4>
<p>APIs for cross-entropy loss are always tricky. For example, libraries always provide computation of the cross-entropy loss by providing the logits, instead of the class-membership probabilities. This is usually preferred due to numerical stability reasons and can be implemented by setting <code class="docutils literal notranslate"><span class="pre">from_logits=True</span></code>. You can see how this is possible by working out the math (some operations cancel out). The default behavior in Keras is <code class="docutils literal notranslate"><span class="pre">from_logits=False</span></code>, so that the outputs of the model are expected to be probabilities in <span class="math notranslate nohighlight">\([0, 1].\)</span></p>
<div class="figure align-default" id="loss-logits-keras">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch15/images/15_11.png"><img alt="https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch15/images/15_11.png" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch15/images/15_11.png" style="width: 45em;" /></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Keras API for loss functions. <span id="id3">[<a class="reference internal" href="../../intro.html#id7">RM19</a>]</span> (Chapter 15)</span><a class="headerlink" href="#loss-logits-keras" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="solving-the-xor-problem">
<h3>Solving the XOR problem<a class="headerlink" href="#solving-the-xor-problem" title="Permalink to this headline">¶</a></h3>
<p><strong>Dataset.</strong> The XOR is the smallest dataset that is not linearly separable (also the most historically interesting relative to its size <span id="id4">[<a class="reference internal" href="../../intro.html#id4">Min69</a>]</span>). Our version of the XOR dataset is generated by adding Gaussian noise to points <code class="docutils literal notranslate"><span class="pre">(-1,</span> <span class="pre">-1)</span></code>, <code class="docutils literal notranslate"><span class="pre">(-1,</span> <span class="pre">1)</span></code>, <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">-1)</span></code> and <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">1)</span></code>. Points generated from <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">1)</span></code> and <code class="docutils literal notranslate"><span class="pre">(-1,</span> <span class="pre">-1)</span></code> will be labeled <code class="docutils literal notranslate"><span class="pre">1</span></code> otherwise <code class="docutils literal notranslate"><span class="pre">0</span></code>. A dataset of size 200 points will be generated with half used for validation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> 
    <span class="n">y</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="c1"># Train-test split</span>
<span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">valid</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">100</span><span class="p">:]</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">valid</span><span class="p">,</span> <span class="p">:],</span> <span class="n">Y</span><span class="p">[</span><span class="n">valid</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">Y</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>

<span class="c1"># Visualize dataset</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/02-tensorflow-mechanics_81_0.svg" src="../../_images/02-tensorflow-mechanics_81_0.svg" /></div>
</div>
<br>
<p><strong>Network architecture.</strong> As a general rule of thumb, the more layers we have,
and the more neurons we have in each layer, the larger the capacity of the model
will be. While having more parameters means the network can fit more complex functions, larger models are usually harder to train (and are prone to overfitting). Model capacity can be increased by increasing:</p>
<ul class="simple">
<li><p><strong>Width.</strong>
The universal approximation theorem states that a feedforward NN with a single hidden
layer and a sufficiently large number of hidden units can approximate any continuous function
to arbitrary accuracy. But this doesn’t apply to the current task which is a classification problem. Indeed, we need a network of at least depth 2 to properly classify the dataset.
<br><br></p></li>
<li><p><strong>Depth.</strong> The advantage of making a network deeper rather than wider is
that fewer parameters are required to achieve a comparable model capacity.
However, a downside of deep (versus wide) models is that deep models are prone
to vanishing and exploding gradients, which make them harder to train.
<br><br></p></li>
</ul>
<p>As mentioned, from the geometry of the dataset, we have to use a network that has at least depth 2, so its not linear. However, since the dataset is small, we want the network to be not too wide (and not too deep), so the model does not overfit on the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Keras provides <code class="docutils literal notranslate"><span class="pre">.summary()</span></code> which prints a summary of the network architecture.
Since the number of parameters for the input layer depend on input size , we
need to specify the dimension of the input. This is done by calling <code class="docutils literal notranslate"><span class="pre">model.build</span></code>
on the expected input shape:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_4&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_8 (Dense)             (None, 4)                 12        
                                                                 
 dense_9 (Dense)             (None, 1)                 5         
                                                                 
=================================================================
Total params: 17
Trainable params: 17
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">None</span></code> is used as a placeholder for the first dimension of the input to make room for arbitrary batch sizes. Alternatively, we could have set <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> in the input layer so we can skip model build.</p>
<br>
<p><strong>Model training.</strong> Writing the <code class="docutils literal notranslate"><span class="pre">train()</span></code> function is boilerplate code.
Since the training loop can be complex, developing this each time can
potentially be a source of bugs, not to mention time-consuming.
TensorFlow Keras API provides a convenient <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> method that can be called
on an instantiated model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(),</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">()]</span>
<span class="p">)</span>

<span class="n">hist</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="c1"># fit accepts numpy arrays</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:42:18.394529: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2022-03-13 07:42:18.731409: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Refer to the <a class="reference external" href="https://keras.io/guides/">Keras developer guides</a>, if more precise control of the details of the training process is needed.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">fit</span></code> method handles the low-level details (regularization, callbacks, metrics, etc.) of training consistently across different implementations. Moreover, this is designed to be performant by exploiting static graph computation. Hence, it is recommended to use <code class="docutils literal notranslate"><span class="pre">fit</span></code> for most use-cases (as well as other built-ins such as <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code> for inference).</p>
<br>
<p><strong>Results.</strong> The <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method returns a dictionary containing data on how the model trained. We will use this to generate visualizations of the training process. To further evaluate the model, we also look at its decision boundaries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;loss&#39;, &#39;binary_accuracy&#39;, &#39;val_loss&#39;, &#39;val_binary_accuracy&#39;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlxtend.plotting</span> <span class="kn">import</span> <span class="n">plot_decision_regions</span>

<span class="k">def</span> <span class="nf">plot_training_history</span><span class="p">(</span><span class="n">hist</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">),</span> <span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">),</span> <span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;valid loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">),</span> <span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;binary_accuracy&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train acc.&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">),</span> <span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_binary_accuracy&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;valid acc.&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_valid</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int_</span><span class="p">),</span> <span class="n">clf</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax</span>

<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
<span class="n">plot_training_history</span><span class="p">(</span><span class="n">hist</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4/4 [==============================] - 0s 7ms/step - loss: 0.0385 - binary_accuracy: 1.0000
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:43:09.555487: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2022-03-13 07:43:10.005851: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
<img alt="../../_images/02-tensorflow-mechanics_93_2.svg" src="../../_images/02-tensorflow-mechanics_93_2.svg" /></div>
</div>
<p>To see model confidence, we can look at the prediction probability at each point in <span class="math notranslate nohighlight">\([-1, 1]^2.\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">to_rgba</span>

<span class="c1"># Plot valid set points</span>
<span class="k">def</span> <span class="nf">plot_decision_gradient</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_valid</span><span class="p">[</span><span class="n">y_valid</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_valid</span><span class="p">[</span><span class="n">y_valid</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_valid</span><span class="p">[</span><span class="n">y_valid</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_valid</span><span class="p">[</span><span class="n">y_valid</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Plot decision gradient</span>
    <span class="n">c0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">to_rgba</span><span class="p">(</span><span class="s2">&quot;C0&quot;</span><span class="p">))</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">to_rgba</span><span class="p">(</span><span class="s2">&quot;C1&quot;</span><span class="p">))</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

    <span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">model_inputs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">output_image</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">preds</span><span class="p">)</span> <span class="o">*</span> <span class="n">c0</span> <span class="o">+</span> <span class="n">preds</span> <span class="o">*</span> <span class="n">c1</span> <span class="c1"># blending</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">output_image</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>

<span class="c1"># Plotting</span>
<span class="n">plot_decision_gradient</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
</pre></div>
</div>
<img alt="../../_images/02-tensorflow-mechanics_95_1.svg" src="../../_images/02-tensorflow-mechanics_95_1.svg" /></div>
</div>
</div>
<div class="section" id="keras-functional-api">
<h3>Keras’ Functional API<a class="headerlink" href="#keras-functional-api" title="Permalink to this headline">¶</a></h3>
<p>Recall that using <code class="docutils literal notranslate"><span class="pre">Sequential</span></code> only allows for a sequence of transformations. This is too restrictive for other architectures. Keras’ so-called functional API comes in handy for more complex transformations such as residual connections. Observe that the model build adds a new “layer” called <code class="docutils literal notranslate"><span class="pre">tf.__operators__.add</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify input and output</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)(</span><span class="n">x</span> <span class="o">+</span> <span class="n">f</span><span class="p">)</span>

<span class="c1"># Build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span> <span class="c1"># compile, fit, etc. also works </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 dense_10 (Dense)               (None, 2)            6           [&#39;input_1[0][0]&#39;]                
                                                                                                  
 tf.__operators__.add (TFOpLamb  (None, 2)           0           [&#39;input_1[0][0]&#39;,                
 da)                                                              &#39;dense_10[0][0]&#39;]               
                                                                                                  
 dense_11 (Dense)               (None, 1)            3           [&#39;tf.__operators__.add[0][0]&#39;]   
                                                                                                  
==================================================================================================
Total params: 9
Trainable params: 9
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="keras-model-class">
<h3>Keras’ <code class="docutils literal notranslate"><span class="pre">Model</span></code> class<a class="headerlink" href="#keras-model-class" title="Permalink to this headline">¶</a></h3>
<p>An alternative way to build complex models is by subclassing <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code>. A model derived from <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> inherits methods such as <code class="docutils literal notranslate"><span class="pre">build()</span></code>, <code class="docutils literal notranslate"><span class="pre">compile()</span></code>, and <code class="docutils literal notranslate"><span class="pre">fit()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="c1"># Build model and model summary</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;my_model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_12 (Dense)            multiple                  12        
                                                                 
 dense_13 (Dense)            multiple                  5         
                                                                 
=================================================================
Total params: 17
Trainable params: 17
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="creating-custom-keras-layers">
<h3>Creating custom Keras layers<a class="headerlink" href="#creating-custom-keras-layers" title="Permalink to this headline">¶</a></h3>
<p>Notice that we’ve been using Keras layers in defining our models. In cases where we want to define a new layer that is not already supported by Keras, or customizing an existing layer,
we can do this by extending the <code class="docutils literal notranslate"><span class="pre">Layer</span></code> base class. In the custom class, we have to define <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> and <code class="docutils literal notranslate"><span class="pre">call()</span></code>. The <code class="docutils literal notranslate"><span class="pre">build()</span></code> method handles delayed variable initialization. Finally, we define a <code class="docutils literal notranslate"><span class="pre">get_config()</span></code> which can be useful for model serialization (saving and loading).</p>
<br>
<p><strong>Implementation.</strong> To illustrate the concept of implementing custom layers, let’s consider a simple
example. We define a new linear layer that computes <span class="math notranslate nohighlight">\((\mathbf x + \boldsymbol{\varepsilon}) \cdot \mathbf w + \boldsymbol {b}\)</span>
where <span class="math notranslate nohighlight">\(\boldsymbol\varepsilon\)</span> refers to a random variable as noise, then passes the result to a ReLU nonlinearity. We assume that <span class="math notranslate nohighlight">\(\mathbf x\)</span> is a rank 2 tensor with shape <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">d)</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code> is the batch size and <code class="docutils literal notranslate"><span class="pre">d</span></code> is the size of the (flattened) inputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NoisyLinear</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">noise_stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise_stddev</span> <span class="o">=</span> <span class="n">noise_stddev</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weights&#39;</span><span class="p">,</span> 
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
                <span class="n">shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> 
                <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
                <span class="n">stddev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_stddev</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">noise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
            <span class="s2">&quot;output_dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span>
            <span class="s2">&quot;noise_stddev&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_stddev</span>
        <span class="p">})</span>
        <span class="k">return</span> <span class="n">config</span>
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>This is analogous to <code class="docutils literal notranslate"><span class="pre">model.train()</span></code> and <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> in PyTorch. Though, I think having an explicit variable to handle this is nice.</p>
</div>
<p>Notice that in the <code class="docutils literal notranslate"><span class="pre">call()</span></code> method, we have used an
additional argument, <code class="docutils literal notranslate"><span class="pre">training=False</span></code>. This distinguishes whether a model or layer
is used at training or at inference time. This is automatically set in Keras to <code class="docutils literal notranslate"><span class="pre">True</span></code> when using <code class="docutils literal notranslate"><span class="pre">.fit</span></code> and to <code class="docutils literal notranslate"><span class="pre">False</span></code> when using <code class="docutils literal notranslate"><span class="pre">.predict</span></code>. The <code class="docutils literal notranslate"><span class="pre">training</span></code> flag is implemented there are operations
that behave differently in
training and prediction modes such as dropout and batch normalization. In the case of <code class="docutils literal notranslate"><span class="pre">NoisyLayer</span></code>, noise is only added during training; no noise is added at inference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">noisy_layer</span> <span class="o">=</span> <span class="n">NoisyLinear</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">noisy_layer</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">noisy_layer</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">noisy_layer</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[-0.010030563]
 [-0.0636793]
 [0.0242050495]
 [0.0539674945]]
[0]
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the outputs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">noise</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">noisy_layer</span> <span class="o">=</span> <span class="n">NoisyLinear</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">noise</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">noisy_layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-</span> <span class="n">noisy_layer</span><span class="o">.</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">noisy_layer</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># Plot distribution</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">noise</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/particle1331/miniforge3/envs/ml/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
</pre></div>
</div>
<img alt="../../_images/02-tensorflow-mechanics_109_1.svg" src="../../_images/02-tensorflow-mechanics_109_1.svg" /></div>
</div>
<p>Looks OK, the ReLU clips the output to the positive x-axis. Removing zero, we expect the distribution to look like a half-Gaussian.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">([</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">noise</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/particle1331/miniforge3/envs/ml/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
</pre></div>
</div>
<img alt="../../_images/02-tensorflow-mechanics_111_1.svg" src="../../_images/02-tensorflow-mechanics_111_1.svg" /></div>
</div>
<p>Testing the <code class="docutils literal notranslate"><span class="pre">.config</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Re-building from config:</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">noisy_layer</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
<span class="n">new_layer</span> <span class="o">=</span> <span class="n">NoisyLinear</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Output can be different since random state is not saved</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">noisy_layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">new_layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;name&#39;: &#39;noisy_linear_1&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;output_dim&#39;: 1, &#39;noise_stddev&#39;: 0.1}
[[0.00302205025]]
[[0.00127642462]]
</pre></div>
</div>
</div>
</div>
<p>Testing call outside training:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">noisy_layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
</pre></div>
</div>
</div>
</div>
<br>
<p><strong>Remodelling.</strong> In this section, we will add <code class="docutils literal notranslate"><span class="pre">NoisyLinear</span></code> to our previous model for XOR. Note that noise should be scaled depending on the magnitude of the input. In our case, the input features vary between <span class="math notranslate nohighlight">\(-1\)</span> to <span class="math notranslate nohighlight">\(1,\)</span> so we set <span class="math notranslate nohighlight">\(\sigma = 0.3\)</span> in the noisy linear for it to have considerable effect.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">NoisyLinear</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">noise_stddev</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_5&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 noisy_linear_2 (NoisyLinear  (None, 4)                12        
 )                                                               
                                                                 
 dense_14 (Dense)            (None, 4)                 20        
                                                                 
 dense_15 (Dense)            (None, 1)                 5         
                                                                 
=================================================================
Total params: 37
Trainable params: 37
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(),</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">()]</span>
<span class="p">)</span>

<span class="n">hist</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="c1"># fit accepts numpy arrays</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:43:32.795949: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2022-03-13 07:43:33.066809: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_training_history</span><span class="p">(</span><span class="n">hist</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:44:24.247521: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
<img alt="../../_images/02-tensorflow-mechanics_119_1.svg" src="../../_images/02-tensorflow-mechanics_119_1.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4/4 [==============================] - 0s 8ms/step - loss: 0.0095 - binary_accuracy: 1.0000
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-03-13 07:44:32.094527: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.009478285908699036, 1.0]
</pre></div>
</div>
</div>
</div>
<p>Notice that the training curve is noisier than before since we added a large amount of noise. On the other hand, the validation curves are not noisy at all. This shows Keras automatically sets <code class="docutils literal notranslate"><span class="pre">training</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> during evaluation. Also note that while the validation performance is perfect, it seems to generalize worse since the decision boundaries are too sharp.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_gradient</span><span class="p">(</span><span class="n">model</span><span class="p">);</span> <span class="c1"># surprisingly sharp</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
</pre></div>
</div>
<img alt="../../_images/02-tensorflow-mechanics_122_1.svg" src="../../_images/02-tensorflow-mechanics_122_1.svg" /></div>
</div>
</div>
<div class="section" id="saving-and-loading-models">
<h3>Saving and loading models<a class="headerlink" href="#saving-and-loading-models" title="Permalink to this headline">¶</a></h3>
<p>We can save and load a model for checkpointing as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;model.h5&#39;</span><span class="p">,</span> 
    <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">include_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># also save state of optimizer </span>
    <span class="n">save_format</span><span class="o">=</span><span class="s1">&#39;h5&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Testing load.</strong> Note that custom layers need to be taken particular care of.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_load</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="s1">&#39;model.h5&#39;</span><span class="p">,</span> 
    <span class="n">custom_objects</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;NoisyLinear&#39;</span><span class="p">:</span> <span class="n">NoisyLinear</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">model_load</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_5&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 noisy_linear_2 (NoisyLinear  (None, 4)                12        
 )                                                               
                                                                 
 dense_14 (Dense)            (None, 4)                 20        
                                                                 
 dense_15 (Dense)            (None, 1)                 5         
                                                                 
=================================================================
Total params: 37
Trainable params: 37
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>We can also save the network architecture as a JSON file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="n">json_object</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">model_load</span><span class="o">.</span><span class="n">to_json</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">json_object</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{
  &quot;class_name&quot;: &quot;Sequential&quot;,
  &quot;config&quot;: {
    &quot;name&quot;: &quot;sequential_5&quot;,
    &quot;layers&quot;: [
      {
        &quot;class_name&quot;: &quot;InputLayer&quot;,
        &quot;config&quot;: {
          &quot;batch_input_shape&quot;: [
            null,
            2
          ],
          &quot;dtype&quot;: &quot;float32&quot;,
          &quot;sparse&quot;: false,
          &quot;ragged&quot;: false,
          &quot;name&quot;: &quot;noisy_linear_2_input&quot;
        }
      },
      {
        &quot;class_name&quot;: &quot;NoisyLinear&quot;,
        &quot;config&quot;: {
          &quot;name&quot;: &quot;noisy_linear_2&quot;,
          &quot;trainable&quot;: true,
          &quot;dtype&quot;: &quot;float32&quot;,
          &quot;output_dim&quot;: 4,
          &quot;noise_stddev&quot;: 0.3
        }
      },
      {
        &quot;class_name&quot;: &quot;Dense&quot;,
        &quot;config&quot;: {
          &quot;name&quot;: &quot;dense_14&quot;,
          &quot;trainable&quot;: true,
          &quot;batch_input_shape&quot;: [
            null,
            2
          ],
          &quot;dtype&quot;: &quot;float32&quot;,
          &quot;units&quot;: 4,
          &quot;activation&quot;: &quot;tanh&quot;,
          &quot;use_bias&quot;: true,
          &quot;kernel_initializer&quot;: {
            &quot;class_name&quot;: &quot;GlorotUniform&quot;,
            &quot;config&quot;: {
              &quot;seed&quot;: null
            }
          },
          &quot;bias_initializer&quot;: {
            &quot;class_name&quot;: &quot;Zeros&quot;,
            &quot;config&quot;: {}
          },
          &quot;kernel_regularizer&quot;: null,
          &quot;bias_regularizer&quot;: null,
          &quot;activity_regularizer&quot;: null,
          &quot;kernel_constraint&quot;: null,
          &quot;bias_constraint&quot;: null
        }
      },
      {
        &quot;class_name&quot;: &quot;Dense&quot;,
        &quot;config&quot;: {
          &quot;name&quot;: &quot;dense_15&quot;,
          &quot;trainable&quot;: true,
          &quot;dtype&quot;: &quot;float32&quot;,
          &quot;units&quot;: 1,
          &quot;activation&quot;: &quot;sigmoid&quot;,
          &quot;use_bias&quot;: true,
          &quot;kernel_initializer&quot;: {
            &quot;class_name&quot;: &quot;GlorotUniform&quot;,
            &quot;config&quot;: {
              &quot;seed&quot;: null
            }
          },
          &quot;bias_initializer&quot;: {
            &quot;class_name&quot;: &quot;Zeros&quot;,
            &quot;config&quot;: {}
          },
          &quot;kernel_regularizer&quot;: null,
          &quot;bias_regularizer&quot;: null,
          &quot;activity_regularizer&quot;: null,
          &quot;kernel_constraint&quot;: null,
          &quot;bias_constraint&quot;: null
        }
      }
    ]
  },
  &quot;keras_version&quot;: &quot;2.7.0&quot;,
  &quot;backend&quot;: &quot;tensorflow&quot;
}
</pre></div>
</div>
</div>
</div>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="refadversarial"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Computing gradients of the loss with respect to the input
example is used for generating adversarial examples.</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/tensorflow"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="01-tensorflow-nn.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">TensorFlow Datasets</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="03-tensorflow-activations.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Activation Functions</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By 𝗥𝗼𝗻 𝗠𝗲𝗱𝗶𝗻𝗮. Powered by <a href="https://jupyterbook.org">Jupyter Book</a>.<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>