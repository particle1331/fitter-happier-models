
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Initialization and Optimization &#8212; 𝗜𝗻𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝗡𝗲𝘁𝘄𝗼𝗿𝗸𝘀</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../../_static/pone.0237978.g001.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Packaging Production Code" href="../deployment/production-code.html" />
    <link rel="prev" title="Activation Functions" href="03-tensorflow-activations.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/pone.0237978.g001.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">𝗜𝗻𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝗡𝗲𝘁𝘄𝗼𝗿𝗸𝘀</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  FUNDAMENTALS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/house-prices.html">
   Pipelines in
   <code class="docutils literal notranslate">
    <span class="pre">
     scikit-learn
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/blending-stacking.html">
   Blending and Stacking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/optuna.html">
   Model Tuning with Optuna
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/missing.html">
   Handling Missing Values
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DEEP LEARNING
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../fundamentals/backpropagation.html">
   Backpropagation on DAGs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01-tensorflow-nn.html">
   TensorFlow Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02-tensorflow-mechanics.html">
   Mechanics of TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-tensorflow-activations.html">
   Activation Functions
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Initialization and Optimization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MODEL DEPLOYMENT
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../deployment/production-code.html">
   Packaging Production Code
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/tensorflow/04-tensorflow-optim-init.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/particle1331/inefficient-networks"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/particle1331/inefficient-networks/issues/new?title=Issue%20on%20page%20%2Fnotebooks/tensorflow/04-tensorflow-optim-init.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/particle1331/inefficient-networks/master?urlpath=tree/docs/notebooks/tensorflow/04-tensorflow-optim-init.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparation">
   Preparation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#initialization">
   Initialization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#constant-initialization">
     Constant initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#constant-variance">
     Constant variance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-find-appropriate-initialization-values">
     How to find appropriate initialization values
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-xavier-initialization-on-tanh-networks">
       Using Xavier initialization on
       <span class="math notranslate nohighlight">
        \(\tanh\)
       </span>
       networks
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kaiming-initialization-for-relu-networks">
       Kaiming initialization for ReLU networks
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#understanding-activation-and-gradient-flow">
     Understanding activation and gradient flow
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   Optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization-algorithms">
     Optimization algorithms
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sgd">
       SGD
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sgd-with-momentum">
       SGD with Momentum
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adam">
       Adam
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparing-optimizers-on-model-training">
     Comparing optimizers on model training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-optimizers-on-exotic-surfaces">
     Testing optimizers on exotic surfaces
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pathological-curvatures">
       Pathological curvatures
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#steep-optima">
       Steep optima
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-optimizer-to-take">
     What optimizer to take
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Initialization and Optimization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparation">
   Preparation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#initialization">
   Initialization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#constant-initialization">
     Constant initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#constant-variance">
     Constant variance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-find-appropriate-initialization-values">
     How to find appropriate initialization values
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-xavier-initialization-on-tanh-networks">
       Using Xavier initialization on
       <span class="math notranslate nohighlight">
        \(\tanh\)
       </span>
       networks
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kaiming-initialization-for-relu-networks">
       Kaiming initialization for ReLU networks
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#understanding-activation-and-gradient-flow">
     Understanding activation and gradient flow
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   Optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization-algorithms">
     Optimization algorithms
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sgd">
       SGD
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sgd-with-momentum">
       SGD with Momentum
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adam">
       Adam
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparing-optimizers-on-model-training">
     Comparing optimizers on model training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-optimizers-on-exotic-surfaces">
     Testing optimizers on exotic surfaces
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pathological-curvatures">
       Pathological curvatures
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#steep-optima">
       Steep optima
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-optimizer-to-take">
     What optimizer to take
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="initialization-and-optimization">
<h1>Initialization and Optimization<a class="headerlink" href="#initialization-and-optimization" title="Permalink to this headline">¶</a></h1>
<p><img alt="Status" src="https://img.shields.io/static/v1.svg?label=Status&amp;message=Finished&amp;color=green" /></p>
<p>In this notebook, we will review techniques for optimization and initialization of neural networks. When increasing the depth of neural networks, there are various challenges we face. Most importantly, we need to have a stable gradient flow through the network, as otherwise, we might encounter vanishing or exploding gradients. This is why we will take a closer look at the following concepts: <strong>initialization</strong> and <strong>optimization</strong>.</p>
<p>In the first half of the notebook, we will review different initialization techniques, and go step by step from the simplest initialization to methods that are nowadays used in very deep networks. In the second half, we focus on optimization comparing the optimizers SGD, SGD with Momentum, and Adam.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>⚠️ <strong>Attribution:</strong> This notebook builds on <a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html">Tutorial 4: Optimization and Initialization</a> by translating all PyTorch code to TensorFlow 2 and modifying or adding to the discussion. The original tutorial is part of a lecture series on Deep Learning at the University of Amsterdam. The full list of tutorials can be found <a class="reference external" href="https://uvadlc-notebooks.rtfd.io">here</a>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.7.0
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;),
 PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf&#39;</span><span class="p">)</span>

<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>

<span class="c1"># Path to datasets</span>
<span class="n">DATASET_PATH</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span><span class="o">.</span><span class="n">parents</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="s2">&quot;data&quot;</span>
<span class="n">DATASET_PATH</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline">¶</a></h2>
<p>Throughout this notebook, we will use a deep fully connected network, similar to our previous tutorial. We will also again apply the network to FashionMNIST, so you can relate to the results in the previous notebook. We start by loading the FashionMNIST dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Transformations applied on each image. </span>
<span class="k">def</span> <span class="nf">transform_image</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">per_image_standardization</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>


<span class="n">FMNIST</span><span class="p">,</span> <span class="n">FMNIST_info</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
    <span class="s1">&#39;fashion_mnist&#39;</span><span class="p">,</span> 
    <span class="n">data_dir</span><span class="o">=</span><span class="n">DATASET_PATH</span><span class="p">,</span> 
    <span class="n">with_info</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">shuffle_files</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="n">train_ds</span><span class="p">,</span> <span class="n">test_ds</span> <span class="o">=</span> <span class="n">FMNIST</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">FMNIST</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">transform_image</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]))</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">transform_image</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Metal device set to: Apple M1

systemMemory: 8.00 GB
maxCacheSize: 2.67 GB
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:44:55.565406: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-02-23 17:44:55.565837: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)
</pre></div>
</div>
</div>
</div>
<p>In comparison to the previous tutorial, we rescaled each image from <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code> to <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>. The normalization is now designed to give us an expected mean of <code class="docutils literal notranslate"><span class="pre">0</span></code> and a standard deviation of <code class="docutils literal notranslate"><span class="pre">1</span></code> across pixels for each image. This will be particularly relevant for the discussion about initialization we will look at below, and hence we change it here. It should be noted that in most classification tasks, both normalization techniques (between -1 and 1 or mean 0 and standard deviation 1) have shown to work well.</p>
<p>Let’s look at the distribution of pixel values in a batch of images (after standardization). Note that the maximum and minimum are not 1 and -1 anymore, but shifted towards the positive values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
<span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">4096</span><span class="p">)))[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">120</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Pixel intensity&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/particle1331/miniforge3/envs/ml/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_10_1.svg" src="../../_images/04-tensorflow-optim-init_10_1.svg" /></div>
</div>
<p>Next, we create a linear neural network. We use the same setup as in the previous tutorial.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">base_net</span><span class="p">(</span><span class="n">act_fn</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Return an initialized MLP network with dense hidden layers with activation</span>
<span class="sd">    `act_fn` and width in `hidden_sizes` ordered such that index zero is nearest </span>
<span class="sd">    the input layer, and a final linear layer `num_classes` output neurons.&quot;&quot;&quot;</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

    <span class="c1"># Add hidden layers with activation</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">hidden_sizes</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
        <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">act_fn</span><span class="p">())</span>

    <span class="c1"># Add logit linear layer</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">num_classes</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<p>For the activation functions, we make use of Keras’ <code class="docutils literal notranslate"><span class="pre">tf.keras.activations</span></code> library instead of implementing ourselves. We also define an <code class="docutils literal notranslate"><span class="pre">Identity</span></code> activation function. Although this activation function would significantly limit the network’s modeling capabilities, we will use it in the first steps of our discussion about initialization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
        
        
<span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        

<span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">act_fn_by_name</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">Tanh</span><span class="p">,</span>
    <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">,</span>
    <span class="s2">&quot;identity&quot;</span><span class="p">:</span> <span class="n">Identity</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we define a few plotting functions that we will use for our discussions. These functions help us to visualize (1) the weight distribution inside a network, (2) the gradients that the parameters at different layers receive, and (3) the activations, i.e. the output of the linear layers. The detailed code is not important, but feel free to take a closer look if interested.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_distributions</span><span class="p">(</span><span class="n">dist</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="n">use_kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">cols</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
    <span class="n">fig_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="c1"># Plot distribution</span>
        <span class="n">ax_key</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">fig_index</span> <span class="o">%</span> <span class="n">cols</span><span class="p">]</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span>
            <span class="n">dist</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax_key</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="n">stat</span><span class="p">,</span> 
            <span class="n">kde</span><span class="o">=</span><span class="n">use_kde</span> <span class="ow">and</span> <span class="p">((</span><span class="n">dist</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">dist</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mf">1e-8</span><span class="p">)</span> <span class="c1"># plot KDE only if nonzero variance</span>
        <span class="p">)</span> 

        <span class="c1"># Formatting</span>
        <span class="n">ax_key</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
        <span class="n">ax_key</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
        <span class="n">ax_key</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
        <span class="n">fig_index</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fig</span>


<span class="k">def</span> <span class="nf">plot_weight_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">):</span>
    <span class="c1"># Exclude the bias to reduce the number of plots</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">key_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">layer_index</span><span class="si">}</span><span class="s2">) </span><span class="si">{</span><span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">weights</span><span class="p">[(</span><span class="n">layer_index</span><span class="p">,</span> <span class="n">key_name</span><span class="p">)]</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Plotting</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_distributions</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Weight distribution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">weights</span>


<span class="k">def</span> <span class="nf">plot_gradient_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">small_loader</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
    <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">small_loader</span><span class="p">))</span>

    <span class="c1"># Pass the batch through the network, and calculate the gradients for the weights</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
    
    <span class="c1"># Exclude the bias to reduce the number of plots</span>
    <span class="n">grads_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">variables</span><span class="p">)):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">w</span><span class="o">.</span><span class="n">trainable</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="n">grads_dict</span><span class="p">[(</span><span class="n">layer_index</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">layer_index</span><span class="si">}</span><span class="s2">) </span><span class="si">{</span><span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1">## Plotting</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_distributions</span><span class="p">(</span><span class="n">grads_dict</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Grad magnitude&quot;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Gradient distribution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">print_variance</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">grads_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2">  σ²_grad = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">grads_dict</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grads_dict</span>


<span class="k">def</span> <span class="nf">plot_activations_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">small_loader</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
    <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">small_loader</span><span class="p">))</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">images</span>
    <span class="k">for</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">):</span>
            <span class="n">activations</span><span class="p">[(</span><span class="n">layer_index</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">layer_index</span><span class="si">}</span><span class="s2">) </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1">## Plotting</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_distributions</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Activation distribution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">print_variance</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">activations</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2">  σ²_act = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">activations</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Remark.</strong> Observe that neurons in the hidden layers are identically distributed during initialization, by symmetry. This allows us to combine all values in the <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">m)</span></code> matrix into a histogram of <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">*</span> <span class="pre">m</span></code> samples whose distribution should reflect the distribution of each individual output neuron. This doesn’t hold for neurons in the input and output layers which have different distributions. Consequently, histogram plots for the input and output layers should be interpreted as an aggregated distribution which will generally differ from the individual distributions of each neuron.</p>
</div>
<div class="section" id="initialization">
<h2>Initialization<a class="headerlink" href="#initialization" title="Permalink to this headline">¶</a></h2>
<p>Training deep neural networks is essentially an optimization problem (in very high dimensions) with the network weights as parameters and the loss as objective. Thus, we have to choose initial values for the weights.
When initializing a neural network, there are a few properties we would like to have. First, the variance of the input should be propagated through the model to the last layer, so that we have a similar standard deviation for the output neurons. If the variance would vanish the deeper we go in our model, it becomes much harder to optimize the model as the input to the next layer is basically a single constant value. Similarly, if the variance increases, it is likely to explode (i.e. head to infinity) the deeper we design our model. The second property we look out for in initialization techniques is a gradient distribution with equal variance across layers. If the first layer receives much smaller gradients than the last layer, we will have difficulties in choosing an appropriate learning rate.</p>
<p>As a starting point for finding a good method, we will analyze different initialization based on our linear neural network with no activation function (i.e. an identity). We do this because initializations depend on the specific activation function used in the network, and we can adjust the initialization schemes later on for our specific choice.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">base_net</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Identity</span><span class="p">())</span> <span class="c1"># = linear transformation</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 512)               401920    
                                                                 
 identity (Identity)         (None, 512)               0         
                                                                 
 dense_1 (Dense)             (None, 256)               131328    
                                                                 
 identity_1 (Identity)       (None, 256)               0         
                                                                 
 dense_2 (Dense)             (None, 256)               65792     
                                                                 
 identity_2 (Identity)       (None, 256)               0         
                                                                 
 dense_3 (Dense)             (None, 128)               32896     
                                                                 
 identity_3 (Identity)       (None, 128)               0         
                                                                 
 dense_4 (Dense)             (None, 10)                1290      
                                                                 
=================================================================
Total params: 633,226
Trainable params: 633,226
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>To easily visualize various initialization schemes, we define the following function. Note that bias weights are initialized to zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>

<span class="k">def</span> <span class="nf">visualize_initialization</span><span class="p">(</span><span class="n">act_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">init_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> 
    <span class="n">plot_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">plot_act</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">xlim_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">xlim_act</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">base_net</span><span class="p">(</span><span class="n">act_fn</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">):</span>
            <span class="n">kernel_shape</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">bias_shape</span>   <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">fan_in</span>  <span class="o">=</span> <span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">fan_out</span> <span class="o">=</span> <span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">kernel_weights</span> <span class="o">=</span> <span class="n">init_fn</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">)(</span><span class="n">kernel_shape</span><span class="p">)</span>
            <span class="n">bias_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">Zeros</span><span class="p">()(</span><span class="n">bias_shape</span><span class="p">)</span> <span class="c1"># b = 0</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">kernel_weights</span><span class="p">,</span> <span class="n">bias_weights</span><span class="p">])</span>
    
    <span class="k">if</span> <span class="n">plot_grad</span><span class="p">:</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">plot_gradient_distribution</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span>
            <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">xlim</span><span class="o">=</span><span class="n">xlim_grad</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">if</span> <span class="n">plot_act</span><span class="p">:</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="n">plot_activations_distribution</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> 
            <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">xlim</span><span class="o">=</span><span class="n">xlim_act</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;gradients&#39;</span><span class="p">:</span> <span class="n">gradients</span><span class="p">,</span>
        <span class="s1">&#39;activations&#39;</span><span class="p">:</span> <span class="n">activations</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="constant-initialization">
<h3>Constant initialization<a class="headerlink" href="#constant-initialization" title="Permalink to this headline">¶</a></h3>
<p>The first initialization we can consider is to initialize all weights with the same constant value. Intuitively, setting all weights to zero is not a good idea as the propagated gradient will be zero since we have a constant loss surface. However, what happens if we set all weights to some nonzero constant? Note that using a large constant will make the network have exploding activations since neurons accumulate the input vector into a weighted sum. What happens if we set all weights to a value slightly larger or smaller than 0? To find out, we can implement a function for setting all parameters below and visualize the gradients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Identity</span><span class="p">(),</span> 
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">0.005</span><span class="p">),</span>
    <span class="n">xlim_act</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:01.485083: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_25_1.svg" src="../../_images/04-tensorflow-optim-init_25_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0) dense_5   σ²_grad = 7.78136e-20
(2) dense_6   σ²_grad = 0.00000e+00
(4) dense_7   σ²_grad = 0.00000e+00
(6) dense_8   σ²_grad = 0.00000e+00
(8) dense_9   σ²_grad = 2.75408e-15
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:02.553512: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_25_4.svg" src="../../_images/04-tensorflow-optim-init_25_4.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) identity_4   σ²_act = 2.72334e-12
(3) identity_5   σ²_act = 1.78477e-11
(5) identity_6   σ²_act = 2.92416e-11
(7) identity_7   σ²_act = 4.79095e-11
</pre></div>
</div>
</div>
</div>
<p>As we can see, only the first and the last layer have diverse gradient distributions each intermediate hidden layers have the same gradient for all weights (note that this value is unequal to 0, but often very close to it — see print below). Due to symmetry, all intermediate neurons belonging to the same layer will be equivalent, and therefore have the same gradient updates (essentially reducing the effective number of parameters to 1 for these layers).</p>
<p>The only sources of assymetry are the inputs and outputs, which explains the nonzero variance in the gradients of the first and last layers (see code cell below). For example, different pixels have different distributions. Thus, we cannot a constant initialization as this greatly reduces the network’s capacity to learn.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Printing gradient stats for the first, third, and last layers</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Gradients:&quot;</span><span class="p">)</span> 
<span class="n">layer_grads</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">j</span><span class="p">:</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">][</span><span class="nb">sorted</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="n">j</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;μ = (</span><span class="si">{</span><span class="n">layer_grads</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">layer_grads</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">layer_grads</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;σ = ( </span><span class="si">{</span><span class="n">layer_grads</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">layer_grads</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">,  </span><span class="si">{</span><span class="n">layer_grads</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Printing activation stats for the first, third, and last layers</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Activations:&quot;</span><span class="p">)</span> 
<span class="n">layer_acts</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">j</span><span class="p">:</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">][</span><span class="nb">sorted</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="n">j</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;μ = (</span><span class="si">{</span><span class="n">layer_acts</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">layer_acts</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">layer_acts</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;σ = ( </span><span class="si">{</span><span class="n">layer_acts</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">,  </span><span class="si">{</span><span class="n">layer_acts</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">,  </span><span class="si">{</span><span class="n">layer_acts</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.5e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Gradients:
μ = (-2.77556e-17, 4.74382e-18, -1.06581e-15)
σ = ( 2.78951e-10, 0.00000e+00,  5.24793e-08)

Activations:
μ = (-1.61243e-09, -5.28336e-09, -6.76319e-09)
σ = ( 1.65025e-06,  5.40755e-06,  6.92167e-06)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="constant-variance">
<h3>Constant variance<a class="headerlink" href="#constant-variance" title="Permalink to this headline">¶</a></h3>
<p>From the experiment above, we have seen that a constant value is not working. So to break symmetry, how about we initialize the parameters by randomly sampling from a distribution like a Gaussian? The most intuitive way would be to choose one variance that is used for all layers in the network. Let’s implement it below, and visualize the activation distribution across layers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Identity</span><span class="p">(),</span> 
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="n">xlim_grad</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">],</span>
    <span class="n">xlim_act</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:05.605717: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_30_1.svg" src="../../_images/04-tensorflow-optim-init_30_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0) dense_10  σ²_grad = 2.36352e-11
(2) dense_11  σ²_grad = 7.44267e-11
(4) dense_12  σ²_grad = 1.36769e-10
(6) dense_13  σ²_grad = 2.80448e-10
(8) dense_14  σ²_grad = 7.20163e-09
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:08.126878: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_30_4.svg" src="../../_images/04-tensorflow-optim-init_30_4.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) identity_8   σ²_act = 7.76782e-02
(3) identity_9   σ²_act = 4.06882e-03
(5) identity_10  σ²_act = 1.09182e-04
(7) identity_11  σ²_act = 2.89081e-06
</pre></div>
</div>
</div>
</div>
<p>The variance of the activation becomes smaller and smaller across layers, and almost vanishes in the last layer. Alternatively, we could use a higher standard deviation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Identity</span><span class="p">(),</span> 
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.8</span><span class="p">),</span>
    <span class="n">xlim_grad</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">25000</span><span class="p">,</span> <span class="mi">25000</span><span class="p">],</span>
    <span class="n">xlim_act</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">150000</span><span class="p">,</span> <span class="mi">150000</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:11.664390: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_32_1.svg" src="../../_images/04-tensorflow-optim-init_32_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0) dense_15  σ²_grad = 1.63114e+05
(2) dense_16  σ²_grad = 5.08641e+05
(4) dense_17  σ²_grad = 1.01448e+06
(6) dense_18  σ²_grad = 1.70529e+06
(8) dense_19  σ²_grad = 3.96022e+07
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:13.964582: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_32_4.svg" src="../../_images/04-tensorflow-optim-init_32_4.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) identity_12  σ²_act = 5.02026e+02
(3) identity_13  σ²_act = 1.62309e+05
(5) identity_14  σ²_act = 2.48822e+07
(7) identity_15  σ²_act = 3.96698e+09
</pre></div>
</div>
</div>
</div>
<p>With a higher standard deviation, the activations are likely to explode. You can play around with the specific standard deviation values, but it will be hard to find one that gives us a good activation distribution across layers and is very specific to our model. If we would change the hidden sizes or number of layers, you would have to search all over again, which is neither efficient nor recommended.</p>
</div>
<div class="section" id="how-to-find-appropriate-initialization-values">
<h3>How to find appropriate initialization values<a class="headerlink" href="#how-to-find-appropriate-initialization-values" title="Permalink to this headline">¶</a></h3>
<p>Suppose we want to design an initialization for the linear layer which computes <span class="math notranslate nohighlight">\(\mathbf y= \mathbf x \boldsymbol W + \boldsymbol b\)</span> with <span class="math notranslate nohighlight">\(\mathbf y\in\mathbb{R}^{d_{\mathbf y}}\)</span>, <span class="math notranslate nohighlight">\(\mathbf x\in\mathbb{R}^{d_{\mathbf x}}\)</span>. From our experiments above, we saw that we need to optimally sample weights to ensure healthy distribution of activation values. For this, we state two requirements:</p>
<ol class="simple">
<li><p>The mean of the activations should be zero.</p></li>
<li><p>The variance of the activations should stay the same across every layer.</p></li>
</ol>
<p>Note that the activation neurons in a single layer are independent and identically distributed, so we can write the variance as <span class="math notranslate nohighlight">\(\sigma_{\mathbf y}^{2} = \text{Var}(y_i).\)</span> It follows that <span class="math notranslate nohighlight">\(\boldsymbol b = \mathbf 0\)</span> since the bias is constant across different inputs. Next, it makes sense to set the mean of the weights to zero for the sake of symmetry. This also means that we only have to calculate the variance which can be done as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
    \sigma_{\mathbf y}^{2} = \text{Var}(y_i) &amp; = \text{Var}\left(\sum_{j} w_{ij}x_{j}\right)\\
    &amp; = \sum_{j} \text{Var}(w_{ij}x_{j})\\
    &amp; = \sum_{j} \text{Var}(w_{ij})\ \text{Var}(x_{j})\\
    &amp; = d_{\mathbf x} \  \text{Var}(w_{ij})\ \text{Var}(x_{j}) = d_{\mathbf x}\, \sigma_{\boldsymbol W}^2 \, \sigma_{\mathbf x}^{2}.
\end{split}
\end{split}\]</div>
<p>The second line follows from the variance of a sum of independent random variables, while the third line follows from the variance of a product of two independent random variables with zero mean. Note that <span class="math notranslate nohighlight">\(\mathbf x\)</span> also has zero mean since its either an output of a hidden layer (inductive hypothesis) or a feature vector (preprocessing). In the last line, we assumed that <span class="math notranslate nohighlight">\(x_j\)</span> are independent and identically distributed for all <span class="math notranslate nohighlight">\(j\)</span> (not true for input and output layers).
It follows that <span class="math notranslate nohighlight">\(\sigma_{\boldsymbol W}^2 = \frac{1}{d_{\mathbf x}}\)</span> so that <span class="math notranslate nohighlight">\(\sigma^2_{\mathbf y} = \sigma^2_{\mathbf x}.\)</span> In words, we should initialize the weight distribution with a variance equal to the inverse of the layer’s input dimension. Let’s implement it below and check whether this holds:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Identity</span><span class="p">(),</span> 
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)),</span>
    <span class="n">xlim_grad</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="n">xlim_act</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:17.114903: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_35_1.svg" src="../../_images/04-tensorflow-optim-init_35_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0) dense_20  σ²_grad = 7.44815e-05
(2) dense_21  σ²_grad = 1.45677e-04
(4) dense_22  σ²_grad = 1.43628e-04
(6) dense_23  σ²_grad = 2.64054e-04
(8) dense_24  σ²_grad = 3.25722e-03
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:19.344900: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_35_4.svg" src="../../_images/04-tensorflow-optim-init_35_4.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) identity_16  σ²_act = 9.68700e-01
(3) identity_17  σ²_act = 1.01643e+00
(5) identity_18  σ²_act = 9.70898e-01
(7) identity_19  σ²_act = 9.00436e-01
</pre></div>
</div>
</div>
</div>
<p>As we expected, the variance stays indeed constant across layers. Note that our initialization does not restrict us to a normal distribution, but allows any other distribution with a mean of <span class="math notranslate nohighlight">\(0\)</span> and variance of <span class="math notranslate nohighlight">\(\frac{1}{d_{\mathbf x}}.\)</span> You often see that a uniform distribution is used for initialization. A small benefit of using a uniform instead of a normal distribution is that we can exclude the chance of initializing very large or small weights.</p>
<p>In the above plot, we see that gradients slightly vanish nearer the inputs. Indeed, besides the variance of the activations, another variance we would like to stabilize is the one of the gradients.  This ensures a stable optimization for deep networks. From our work on backpropagation on MLPs, we know that
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal L}{\partial \mathbf x} = \frac{\partial \mathcal L}{\partial \mathbf y} \boldsymbol W^\top.\)</span>
Hence
<span class="math notranslate nohighlight">\(\sigma^2_{\boldsymbol W^\top} = \sigma^2_{\boldsymbol W} = \frac{1}{d_\mathbf y}.\)</span>
As a compromise between both constraints, <a class="reference external" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi">Glorot and Bengio (2010)</a> proposed to use the harmonic mean of both values. This leads us to the well-known <strong>Xavier initialization</strong>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol W\sim \mathcal{N}\left(0,\frac{2}{d_{\mathbf x}+d_{\mathbf y}}\right).\]</div>
<p>If we use a uniform distribution, we would initialize the weights with:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol W\sim U\left[-\frac{\sqrt{6}}{\sqrt{{d_{\mathbf x}+d_{\mathbf y}}}}, \frac{\sqrt{6}}{\sqrt{{d_{\mathbf x}+d_{\mathbf y}}}}\right].\]</div>
<p>Let’s shortly implement it and validate its effectiveness:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Identity</span><span class="p">(),</span> 
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">)))),</span>
    <span class="n">xlim_grad</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="n">xlim_act</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:22.367258: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_38_1.svg" src="../../_images/04-tensorflow-optim-init_38_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0) dense_25  σ²_grad = 7.69383e-04
(2) dense_26  σ²_grad = 1.54660e-03
(4) dense_27  σ²_grad = 1.88833e-03
(6) dense_28  σ²_grad = 2.68132e-03
(8) dense_29  σ²_grad = 2.53531e-02
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:24.707076: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_38_4.svg" src="../../_images/04-tensorflow-optim-init_38_4.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) identity_20  σ²_act = 1.25340e+00
(3) identity_21  σ²_act = 1.69678e+00
(5) identity_22  σ²_act = 1.64976e+00
(7) identity_23  σ²_act = 2.21893e+00
</pre></div>
</div>
</div>
</div>
<p>We see that the Xavier initialization balances the variance of gradients and activations (the variance of the gradients increased by one order of magnitude). Note that the significantly higher variance for the output layer is due to the large difference of input and output dimension (128 vs 10).</p>
<div class="section" id="using-xavier-initialization-on-tanh-networks">
<h4>Using Xavier initialization on <span class="math notranslate nohighlight">\(\tanh\)</span> networks<a class="headerlink" href="#using-xavier-initialization-on-tanh-networks" title="Permalink to this headline">¶</a></h4>
<p>In the discussions above, we assumed the activation function to be linear. So what happens if we add a non-linearity? In a tanh-based network, a common assumption is that for small values during the initial steps in training, the <span class="math notranslate nohighlight">\(\tanh\)</span> works as a linear function such that we don’t have to adjust our calculation. We can check if that is the case for us as well.</p>
<p>Recall <span class="math notranslate nohighlight">\(\sigma_{\mathbf x_1} = \left(\sum_{j=1}^{784} {\sigma_{ x_0, j}^{2}}\right) \sigma_{\boldsymbol W_1}^2\)</span> in the dense part of the input layer which pushes the <span class="math notranslate nohighlight">\(\tanh\)</span> activations to <span class="math notranslate nohighlight">\(\pm 1\)</span> in the first activation. This can limit the expressivity of input features. Then, the next layer will receive an input of mostly <span class="math notranslate nohighlight">\(\pm 1\)</span> so that the saturation persists in the deeper layers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Tanh</span><span class="p">(),</span> 
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span> 
    <span class="n">plot_grad</span><span class="o">=</span><span class="kc">False</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:27.819447: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_43_1.svg" src="../../_images/04-tensorflow-optim-init_43_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) tanh         σ²_act = 9.04020e-01
(3) tanh_1       σ²_act = 8.76801e-01
(5) tanh_2       σ²_act = 8.26106e-01
(7) tanh_3       σ²_act = 8.21413e-01
</pre></div>
</div>
</div>
</div>
<p>For small fixed <span class="math notranslate nohighlight">\(\sigma,\)</span> we get similar behavior with the identity network since for small input, <span class="math notranslate nohighlight">\(\tanh x \approx x.\)</span> Thus, we get vanishing activations. (See explanation above in terms of vanishing <span class="math notranslate nohighlight">\(\sigma_{\mathbf x_t}.\)</span>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Tanh</span><span class="p">(),</span> 
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> 
    <span class="n">plot_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">xlim_act</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:30.890924: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_45_1.svg" src="../../_images/04-tensorflow-optim-init_45_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) tanh_4       σ²_act = 6.91921e-02
(3) tanh_5       σ²_act = 3.50690e-03
(5) tanh_6       σ²_act = 9.26244e-05
(7) tanh_7       σ²_act = 2.35419e-06
</pre></div>
</div>
</div>
</div>
<p>Let’s try to initialize with Xavier normalization. This should work fairly well since <span class="math notranslate nohighlight">\(\tanh\)</span> is approximately linear between -1 and 1. Indeed, observe that we get healthier activation distribution compared to initializing the weights with constant variance above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Tanh</span><span class="p">(),</span> 
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">)))),</span>
    <span class="n">plot_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">xlim_act</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:34.066070: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_47_1.svg" src="../../_images/04-tensorflow-optim-init_47_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) tanh_8       σ²_act = 4.23304e-01
(3) tanh_9       σ²_act = 2.94319e-01
(5) tanh_10      σ²_act = 1.92783e-01
(7) tanh_11      σ²_act = 1.67977e-01
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="kaiming-initialization-for-relu-networks">
<h4>Kaiming initialization for ReLU networks<a class="headerlink" href="#kaiming-initialization-for-relu-networks" title="Permalink to this headline">¶</a></h4>
<p>But what about ReLU networks? Here, we cannot take the previous assumption of the non-linearity becoming linear for small values.  Suppose <span class="math notranslate nohighlight">\(\mathbf y = \mathbf x\boldsymbol W\)</span> such that <span class="math notranslate nohighlight">\(\mathbf x = \text{ReLU}(\tilde {\mathbf y}),\)</span> we want to initialize the weights such that (1) <span class="math notranslate nohighlight">\(\mathbb E[y_j] = \mathbb E[\tilde{y_j}] = 0\)</span> and (2) <span class="math notranslate nohighlight">\(\sigma^2_\mathbf y = \sigma^2_{\tilde {\mathbf y}}.\)</span> As long as the expectation of <span class="math notranslate nohighlight">\(\boldsymbol W\)</span> is zero and <span class="math notranslate nohighlight">\(\boldsymbol b= \mathbf 0\)</span>, the expectation of the output is zero. This takes care of (1). For requirement (2), the part where the calculation of the ReLU initialization differs from the identity is when determining <span class="math notranslate nohighlight">\(\text{Var}(w_{ij}x_{j})\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\text{Var}(w_{ij} x_{j} ) = 
\underbrace{\mathbb{E} [ w_{ij}^2 ]}_{=\text{Var}(w_{ij})} \;
\mathbb{E}[x_{j}^2]-\underbrace{\mathbb{E}[w_{ij}]^2}_{=0}\;\mathbb{E}[ x_{j} ]^2=\text{Var}(w_{ij})\;\mathbb{E}[x_{j}^2].
\]</div>
<p>If we assume now that <span class="math notranslate nohighlight">\(\mathbf x\)</span> is the output of a ReLU activation, we can calculate the expectation as follows. In the first equality <span class="math notranslate nohighlight">\(p\)</span> is the probability distribution of <span class="math notranslate nohighlight">\(\tilde y_j\)</span> which we can assume to be symmetric around zero:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\mathbb{E}[x_j^2] 
&amp;= \int_{-\infty}^{\infty} \max(0, t)^2 p(t) dt \\
&amp;= \int_0^{\infty} t^2 p(t) dt = \frac{1}{2}\int_{-\infty}^{\infty} t^2 p(t) dt = \frac{1}{2}\text{Var}(\tilde{y}_j)
\end{split}\end{split}\]</div>
<p>Thus, we have <span class="math notranslate nohighlight">\(\sigma^2_{\mathbf y} = \frac{1}{2}\sum_{j} \sigma^2_{\boldsymbol W} \sigma^2_{\tilde {\mathbf y}}= \frac{1}{2} d_\mathbf{x} \sigma^2_{\boldsymbol W} \sigma^2_{\tilde {\mathbf y}}\)</span> so that our desired weight variance becomes <span class="math notranslate nohighlight">\(\sigma^2_{\boldsymbol W} = \frac{2}{d_{\mathbf x}}\)</span>. This gives us the Kaiming initialization <a class="reference external" href="https://arxiv.org/pdf/1502.01852.pdf">[He, K. et al. (2015)]</a>. Note that the Kaiming initialization does not use the harmonic mean between input and output size. In their paper (Section 2.2, Backward Propagation, last paragraph), they argue that using <span class="math notranslate nohighlight">\(d_{\mathbf x}\)</span> or <span class="math notranslate nohighlight">\(d_{\mathbf y}\)</span> both lead to stable gradients throughout the network, and only depend on the overall input and output size of the network. Hence, we can use here only the input <span class="math notranslate nohighlight">\(d_{\mathbf x}.\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">)),</span>
    <span class="n">plot_grad</span><span class="o">=</span><span class="kc">False</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:37.561459: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_50_1.svg" src="../../_images/04-tensorflow-optim-init_50_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) re_lu        σ²_act = 6.38307e-01
(3) re_lu_1      σ²_act = 6.27589e-01
(5) re_lu_2      σ²_act = 6.33228e-01
(7) re_lu_3      σ²_act = 5.96459e-01
</pre></div>
</div>
</div>
</div>
<p>In contrast, having no factor of 2 results in vanishing activation values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_initialization</span><span class="p">(</span>
    <span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">)),</span>
    <span class="n">plot_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">xlim_act</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:40.755427: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<img alt="../../_images/04-tensorflow-optim-init_52_1.svg" src="../../_images/04-tensorflow-optim-init_52_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1) re_lu_4      σ²_act = 3.46577e-01
(3) re_lu_5      σ²_act = 1.83771e-01
(5) re_lu_6      σ²_act = 8.51797e-02
(7) re_lu_7      σ²_act = 4.63810e-02
</pre></div>
</div>
</div>
</div>
<p>The activation variance stays stable across layers. We can conclude that the Kaiming initialization indeed works well for ReLU-based networks. Note that for other activations we have to slightly adjust the factor in the variance. For instance, for LeakyReLU half of the values are not set to zero anymore, and calculating a similar integral as above results in a factor of <span class="math notranslate nohighlight">\(\frac{2}{1 + \alpha^2}\)</span> instead of <span class="math notranslate nohighlight">\(2\)</span> for the ReLU.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To initialize with different scale factors, Keras implements the <code class="docutils literal notranslate"><span class="pre">VarianceScaling</span></code> initializer which samples weights from a normal distribution with mean zero and standard deviation <code class="docutils literal notranslate"><span class="pre">stddev</span> <span class="pre">=</span> <span class="pre">sqrt(scale</span> <span class="pre">/</span> <span class="pre">n)</span></code> where <code class="docutils literal notranslate"><span class="pre">n</span></code> depends on the <code class="docutils literal notranslate"><span class="pre">mode</span></code> with <code class="docutils literal notranslate"><span class="pre">'fan_in'</span></code>, <code class="docutils literal notranslate"><span class="pre">'fan_out'</span></code>, and <code class="docutils literal notranslate"><span class="pre">'fan_avg'</span></code> as possible values. In particular, Kaiming corresponds to <code class="docutils literal notranslate"><span class="pre">scale=2</span></code> and <code class="docutils literal notranslate"><span class="pre">mode='fan_in'</span></code>. Xavier corresponds to <code class="docutils literal notranslate"><span class="pre">scale=1</span></code> and <code class="docutils literal notranslate"><span class="pre">mode='fan_avg'</span></code>. Note that same with other normal initializers, Keras truncates the sample space to prevent initializing too large weights.</p>
</div>
</div>
</div>
<div class="section" id="understanding-activation-and-gradient-flow">
<h3>Understanding activation and gradient flow<a class="headerlink" href="#understanding-activation-and-gradient-flow" title="Permalink to this headline">¶</a></h3>
<p>Suppose we index layers and weights as in <a class="reference internal" href="#neuralnet-layers"><span class="std std-numref">Fig. 11</span></a> with <span class="math notranslate nohighlight">\(\mathbf x_0\)</span> as input data. It follows that <span class="math notranslate nohighlight">\(\sigma_{\mathbf x_{t+1}}^{2} = d_{\mathbf x_{t}}\, \sigma_{\mathbf x_{t}}^{2}\, \sigma_{\boldsymbol W_t}^2\)</span> for <span class="math notranslate nohighlight">\(t \geq 1\)</span> and <span class="math notranslate nohighlight">\(\sigma_{\mathbf x_1} = \left(\sum_{j=1}^{784} {\sigma_{ x_0, j}^{2}}\right) \sigma_{\boldsymbol W_0}^2.\)</span> Thus, applying the formula recursively, we get</p>
<div class="math notranslate nohighlight">
\[\sigma_{\mathbf x_t}^{2} = \left( \prod_{k=1}^{t-1} d_{\mathbf x_k} \right) \left(\sum_{j=1}^{784} {\sigma_{ x_0, j}^{2}} \right)\left(\prod_{k=0}^{t-1} \sigma_{\boldsymbol W_k}^2\right).\]</div>
<p>This formula explains why activations and gradients blow up as we go deeper into the layers for a network initialized with sufficiently large constant variance for the weights, and vanishes with depth for sufficiently small constant variance. For example, <span class="math notranslate nohighlight">\(\sigma_{\boldsymbol W_k}^2 = \frac{1}{d_{\mathbf x_{k}}}\)</span> in Xavier initialization, so that everything balances out, leaving <span class="math notranslate nohighlight">\(\sigma_{\mathbf x_t}^{2} = \frac{1}{d_{\mathbf x_0}} \sum_{j=1}^{784} {\sigma_{ x_0, j}^{2}}\)</span> which has the correct scale. The sum term is due to different pixels having non-identical distributions (we still assume independence between pixels which is generally false, e.g. adjacent pixels can have some dependence). Note that we have the same equation for gradients but in reverse (starting from the logits layer).</p>
<div class="figure align-default" id="neuralnet-layers">
<a class="reference internal image-reference" href="../../_images/neuralnet-layers.png"><img alt="../../_images/neuralnet-layers.png" src="../../_images/neuralnet-layers.png" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Schematic diagram of a feedforward neural network.</span><a class="headerlink" href="#neuralnet-layers" title="Permalink to this image">¶</a></p>
</div>
<p>Recall the identity network initialized with constant variance <span class="math notranslate nohighlight">\(\sigma = 0.8\)</span> had exploding activations, while for <span class="math notranslate nohighlight">\(\sigma=0.01\)</span> it had vanishing activations. Let’s test whether our computations are consistent:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1024</span><span class="p">)))[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="mi">256</span> <span class="o">*</span> <span class="mi">256</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">*</span> <span class="n">v</span> <span class="o">*</span> <span class="p">((</span><span class="mf">0.8</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">4</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:44.210936: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2893753113.5361047
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1024</span><span class="p">)))[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="mi">256</span> <span class="o">*</span> <span class="mi">256</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">*</span> <span class="n">v</span> <span class="o">*</span> <span class="p">((</span><span class="mf">0.01</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">4</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:44.336771: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.7248112640000004e-06
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>The formula is similar for general nonlinear activations, but involves the derivative of the activation multiplied (or broadcasted) to the respective weight matrix. Hence, we expect a similar behavior.</p>
</div>
<p>Consider the input layer <span class="math notranslate nohighlight">\(\mathbf x_1 = \mathbf x_0 \boldsymbol W_0,\)</span> then we obtain <span class="math notranslate nohighlight">\(\frac{\partial \mathcal L}{\partial \boldsymbol W_0} = {\mathbf x_0^\top} \frac{\partial \mathcal L}{\partial \mathbf x_1}\)</span> by backpropagating from <span class="math notranslate nohighlight">\(\mathbf x_1\)</span> to <span class="math notranslate nohighlight">\(\boldsymbol W_0.\)</span> Similarly, we can backpropagate from <span class="math notranslate nohighlight">\(\mathbf x_2\)</span> to <span class="math notranslate nohighlight">\(\mathbf x_1\)</span> in the next layer <span class="math notranslate nohighlight">\(\mathbf x_2 = \mathbf x_1 \boldsymbol W_1\)</span> to get
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal L}{\partial \mathbf x_1} = \frac{\partial \mathcal L}{\partial \mathbf x_2} \boldsymbol W_1^\top.\)</span>
Continuing this process, we get the weight gradient of the input layer in terms of the weight gradients of the logits layer <span class="math notranslate nohighlight">\(\mathbf x_5\)</span> (which we have easy access to):</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal L}{\partial \boldsymbol W_0} = {\mathbf x_0^\top} \frac{\partial \mathcal L}{\partial \mathbf x_5} (\boldsymbol W_1\boldsymbol W_2 \boldsymbol W_3 \boldsymbol W_4)^\top\]</div>
<p>This can be extended to
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal L}{\partial \boldsymbol W_t} = {\mathbf x_t^\top} \frac{\partial \mathcal L}{\partial \mathbf x_d} \prod_{j=d-1}^{t+1} \boldsymbol W_{j}^\top\)</span> where <span class="math notranslate nohighlight">\(0 \leq t \leq d-1\)</span> and we shift the starting point, so we can get the gradient of any intermediate layer of the network. Notice the stack of weight matrices — this product can explode or vanish depending on the magnitude of the weights. Moreover, the formula lends itself to the same variance analysis used for activations with output size as factors instead of input size which motivates fan averaging in Xavier initialization:</p>
<div class="math notranslate nohighlight">
\[\sigma_{\frac{\partial \mathcal L}{\partial \boldsymbol W_t} }^{2} = {\sigma_{{\mathbf x_0^\top} \frac{\partial \mathcal L}{\partial \mathbf x_d}}^{2}} \left(\prod_{k=d-1}^{t+1} \sigma_{\boldsymbol W_k}^2 d_{\mathbf x_{k+1}}  \right).\]</div>
<p>As an aside, while you might expect that exploding activations imply exploding gradients, e.g. for the network initialized with fixed <span class="math notranslate nohighlight">\(\sigma_{\mathbf W_{k}} = 0.8\)</span>, this is not the case: shallower layers have lower activations, hence may have lower weight gradients because of the factor <span class="math notranslate nohighlight">\({\mathbf x_t^\top}.\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">base_net</span><span class="p">(</span><span class="n">act_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">Identity</span><span class="p">(),</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>

<span class="c1"># Initialization</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">Zeros</span><span class="p">()</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">):</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span>
            <span class="n">init</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
            <span class="n">zero</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="p">])</span>

<span class="c1"># Forward pass</span>
<span class="n">small_loader</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">small_loader</span><span class="p">))</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">acts_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">images</span>
<span class="n">acts_dict</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="k">for</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">):</span>
        <span class="n">acts_dict</span><span class="p">[</span><span class="n">layer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># Backward pass</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>

<span class="n">grads_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">variables</span><span class="p">)):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">w</span><span class="o">.</span><span class="n">trainable</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="n">grads_dict</span><span class="p">[</span><span class="n">layer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-23 17:45:44.534916: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
</div>
</div>
<p>Let’s check if the above formula works:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">functools</span>

<span class="k">def</span> <span class="nf">weights_gradient_formula</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="n">logits_grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">acts_dict</span><span class="p">[</span><span class="nb">sorted</span><span class="p">(</span><span class="n">acts_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="n">t</span><span class="p">]]</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">)]</span>

    <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">IndexError</span>    
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="n">t</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">logits_grad</span><span class="p">)</span>


<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grads_dict</span><span class="p">)):</span>
    <span class="n">W0_grad</span> <span class="o">=</span> <span class="n">weights_gradient_formula</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">W0_grad</span> <span class="o">-</span> <span class="n">grads_dict</span><span class="p">[</span><span class="nb">sorted</span><span class="p">(</span><span class="n">grads_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="n">t</span><span class="p">]])</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
<span class="n">errors</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(3.3638134e-08, 1.4171046e-07)
</pre></div>
</div>
</div>
</div>
<p>Note that the batch dimension gets summed over inside <span class="math notranslate nohighlight">\({\mathbf x_0^\top} \frac{\partial \mathcal L}{\partial \mathbf x_d}.\)</span> The small differences between the gradients obtained using autodifferentiation, and the gradients obtained using backpropagation is most likely due to numerical instability as <code class="docutils literal notranslate"><span class="pre">logits_grad</span></code> is sparse. Indeed, decreasing <span class="math notranslate nohighlight">\(\sigma\)</span> results in a less sparse weight gradient for the logits layer and a smaller error, and vice-versa.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits_grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="p">(</span><span class="n">logits_grad</span> <span class="o">&gt;</span> <span class="mf">1e-10</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits_grad</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.302734375
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">¶</a></h2>
<p>Besides initialization, selecting a suitable optimization algorithm can be an important choice for deep neural networks. First, we need to understand what an optimizer actually does. The optimizer is responsible to update the network’s parameters given the gradients. Hence, we effectively implement a function <span class="math notranslate nohighlight">\({\boldsymbol w}^{t} = f({\boldsymbol w}^{t-1}, {\boldsymbol g}^{t}, ...)\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol w\)</span> being the parameters, and <span class="math notranslate nohighlight">\({\boldsymbol g}^{t} = \nabla_{{\boldsymbol w}^{(t-1)}} \mathcal{L}^{(t)}\)</span> the gradients at time step <span class="math notranslate nohighlight">\(t\)</span>. A common, additional parameter to this function is the learning rate, here denoted by <span class="math notranslate nohighlight">\(\eta\)</span>. Usually, the learning rate can be seen as the “step size” of the update. A higher learning rate means that we change the weights more in the direction of the gradients, a smaller means we take shorter steps.</p>
<div class="section" id="optimization-algorithms">
<h3>Optimization algorithms<a class="headerlink" href="#optimization-algorithms" title="Permalink to this headline">¶</a></h3>
<p>As most optimizers only differ in the implementation of <span class="math notranslate nohighlight">\(f\)</span>, we can define a template for an optimizer below. We take as input the parameters of a model and a learning rate. The <code class="docutils literal notranslate"><span class="pre">step()</span></code> function tells the optimizer to update all weights based on their gradients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">OptimizerTemplate</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_param</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">update_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="sgd">
<h4>SGD<a class="headerlink" href="#sgd" title="Permalink to this headline">¶</a></h4>
<p>The first optimizer we are going to implement is the standard Stochastic Gradient Descent (SGD). SGD updates the parameters using the following equation:</p>
<div class="math notranslate nohighlight">
\[
\begin{split}
    {\boldsymbol w}^{(t)} &amp; = {\boldsymbol w}^{(t-1)} - \eta\, {\boldsymbol g}^{(t)}.
\end{split}
\]</div>
<p>Let’s implement this in the following class:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">OptimizerTemplate</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">dw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sgd-with-momentum">
<h4>SGD with Momentum<a class="headerlink" href="#sgd-with-momentum" title="Permalink to this headline">¶</a></h4>
<p>SGD can be improved using the concept of <strong>momentum</strong> which replaces the gradient in the update by an exponential average of all past gradients including the current one:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
    \boldsymbol{m}^{(t)} &amp; = \beta_1 \boldsymbol{m}^{(t-1)} + (1 - \beta_1)\, {\boldsymbol g}^{(t)}\\
    {\boldsymbol w}^{(t)} &amp; = {\boldsymbol w}^{(t-1)} - \eta\, \boldsymbol{m}^{(t)}.\\
\end{split}
\end{split}\]</div>
<p>Momentum help smooth out gradient updates. This can be helpful when dealing with oscillating updates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SGDMomentum</span><span class="p">(</span><span class="n">OptimizerTemplate</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">update_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">dw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="adam">
<h4>Adam<a class="headerlink" href="#adam" title="Permalink to this headline">¶</a></h4>
<p>Finally, we arrive at Adam. Adam combines the idea of momentum with an adaptive learning rate, which is based on an exponential average of the squared gradients, i.e. the gradients norm. Furthermore, we add a bias correction for the momentum and adaptive learning rate for the first iterations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
    {\boldsymbol m}^{(t)} &amp; = \beta_1 {\boldsymbol m}^{(t-1)} + (1 - \beta_1)\, {\boldsymbol g}^{(t)}\\
    {\boldsymbol v}^{(t)} &amp; = \beta_2 {\boldsymbol v}^{(t-1)} + (1 - \beta_2)\, ({\boldsymbol g}^{(t)})^2\\
    \hat{{\boldsymbol m}}^{(t)} &amp; = \frac{{\boldsymbol m}^{(t)}}{1-{\beta_1}^{t}},\; \hat{{\boldsymbol v}}^{(t)} = \frac{{\boldsymbol v}^{(t)}}{1-{\beta_2}^{t}}\\
    {\boldsymbol w}^{(t)} &amp; = {\boldsymbol w}^{(t-1)} - \frac{\eta}{\sqrt{\hat{{\boldsymbol v}}^{(t)}} + \epsilon} \hat{\boldsymbol m}^{(t)}\\
\end{split}
\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant used to improve numerical stability for very small gradient norms. Remember that the adaptive learning rate does not replace the learning rate hyperparameter <span class="math notranslate nohighlight">\(\eta,\)</span> but rather acts as an extra factor and ensures that the gradients of various parameters have a similar norm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">OptimizerTemplate</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">beta2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

        <span class="c1"># (t = 0)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_step</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">}</span> <span class="c1"># time steps for each variable</span>

    <span class="k">def</span> <span class="nf">update_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="c1"># fetch prev. momentum and second momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># bias correction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_step</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_step</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
        <span class="n">m_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
        <span class="n">v_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>

        <span class="c1"># update weights</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">m_hat</span> <span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v_hat</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">))</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">dw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="comparing-optimizers-on-model-training">
<h3>Comparing optimizers on model training<a class="headerlink" href="#comparing-optimizers-on-model-training" title="Permalink to this headline">¶</a></h3>
<p>After we have implemented three optimizers (SGD, SGD with momentum, and Adam), we can start to analyze and compare them.
First, we test them on how well they can optimize a neural network on the FashionMNIST dataset. We use again our linear network, this time with a ReLU activation and the Kaiming initialization, which we have found before to work well for ReLU-based networks. Note that the model is over-parameterized for this task, and we can achieve similar performance with a much smaller network (for example <code class="docutils literal notranslate"><span class="pre">100,</span> <span class="pre">100,</span> <span class="pre">100</span></code>). However, our main interest is in how well the optimizer can train <em>deep</em> neural networks, hence the over-parameterization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># add hidden layers with relu activation</span>
<span class="n">hidden_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">hidden_sizes</span><span class="p">[</span><span class="n">j</span><span class="p">],</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">HeNormal</span><span class="p">(),</span>
        <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span>
    <span class="p">))</span>

<span class="c1"># add logit linear layer</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
    <span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">HeNormal</span><span class="p">(),</span>
    <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span>
<span class="p">))</span>

<span class="c1"># build model</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s define a training function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
    <span class="c1"># loss function</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Recall shuffle, batch, repeat pattern to create epochs</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">max_epochs</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>    <span class="c1"># Prepare next elements </span>
                                                                    <span class="c1"># while current is preprocessed. </span>
                                                                    <span class="c1"># Trades off latency with memory.</span>

    <span class="n">valid_loader</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">)</span>
    <span class="n">valid_loader</span> <span class="o">=</span> <span class="n">valid_loader</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">2048</span><span class="p">)</span>
    <span class="n">valid_loader</span> <span class="o">=</span> <span class="n">valid_loader</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span>
    <span class="n">valid_iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">valid_loader</span><span class="p">)</span>

    <span class="c1"># training</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">valid_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">valid_acc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>

        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="c1"># compute valid. loss and valid. accuracy</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">valid_iterator</span><span class="p">)</span>
            <span class="n">valid_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)))</span>
            <span class="n">valid_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span>
                    <span class="n">y_valid</span><span class="p">,</span> 
                    <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">,</span>
        <span class="s2">&quot;valid_loss&quot;</span><span class="p">:</span> <span class="n">valid_loss</span><span class="p">,</span>
        <span class="s2">&quot;valid_acc&quot;</span><span class="p">:</span> <span class="n">valid_acc</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>For a fair comparison, we train the exact same model with the same initialization with the three optimizers below. Feel free to change the hyperparameters if you want.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_sgd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">clone_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">results_sgd</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model_sgd</span><span class="p">,</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model_sgd</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">),</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/2340 [00:00&lt;?, ?it/s]2022-02-23 17:45:46.994089: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
 44%|████▎     | 1023/2340 [00:13&lt;00:19, 66.82it/s]2022-02-23 17:45:59.355199: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
100%|██████████| 2340/2340 [00:28&lt;00:00, 82.17it/s] 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_sgdm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">clone_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">results_sgdm</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model_sgdm</span><span class="p">,</span> <span class="n">SGDMomentum</span><span class="p">(</span><span class="n">model_sgdm</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/2340 [00:00&lt;?, ?it/s]2022-02-23 17:46:14.972302: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
 44%|████▎     | 1019/2340 [00:14&lt;00:22, 60.01it/s]2022-02-23 17:46:29.204814: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
100%|██████████| 2340/2340 [00:34&lt;00:00, 67.77it/s] 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_adam</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">clone_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">results_adam</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model_adam</span><span class="p">,</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model_adam</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/2340 [00:00&lt;?, ?it/s]2022-02-23 17:46:49.845404: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
 44%|████▎     | 1023/2340 [00:22&lt;00:29, 44.46it/s]2022-02-23 17:47:11.887559: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
100%|██████████| 2340/2340 [00:50&lt;00:00, 46.07it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="o">*</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">results_sgd</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">]))]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_sgd</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_sgdm</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD+M&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_adam</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Train loss&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Optimizer steps&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgd</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgdm</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD+M&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_adam</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgd</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgdm</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_adam</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Valid loss&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Optimizer steps&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgd</span><span class="p">[</span><span class="s1">&#39;valid_acc&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgdm</span><span class="p">[</span><span class="s1">&#39;valid_acc&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD+M&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_adam</span><span class="p">[</span><span class="s1">&#39;valid_acc&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgd</span><span class="p">[</span><span class="s1">&#39;valid_acc&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgdm</span><span class="p">[</span><span class="s1">&#39;valid_acc&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_adam</span><span class="p">[</span><span class="s1">&#39;valid_acc&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Validation accuracy&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Optimizer steps&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/04-tensorflow-optim-init_87_0.svg" src="../../_images/04-tensorflow-optim-init_87_0.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgd</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD (val)&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgd</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_sgd</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD (train)&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Optimizer steps&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgdm</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD+M (val)&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_sgdm</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_sgdm</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD+M (train)&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Optimizer steps&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_adam</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Adam (val)&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_adam</span><span class="p">[</span><span class="s1">&#39;valid_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_adam</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Adam (train)&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Optimizer steps&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/04-tensorflow-optim-init_88_0.svg" src="../../_images/04-tensorflow-optim-init_88_0.svg" /></div>
</div>
<p>Overall accuracy on the whole test set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">10000</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracies:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SGD     </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model_sgd</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SGD+M   </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model_sgdm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Adam    </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model_adam</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test accuracies:
  SGD     68.96%
  SGD+M   81.13%
  Adam    79.37%
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="testing-optimizers-on-exotic-surfaces">
<h3>Testing optimizers on exotic surfaces<a class="headerlink" href="#testing-optimizers-on-exotic-surfaces" title="Permalink to this headline">¶</a></h3>
<p>The result above is that all optimizers perform similarly well with the given model. The differences are too small to find any significant conclusion. However, keep in mind that this can also be attributed to the initialization we chose. When changing the initialization to worse (e.g. constant initialization), Adam usually shows to be more robust because of its adaptive learning rate. To show the specific benefits of the optimizers, we will continue to look at some possible loss surfaces in which momentum and adaptive learning rate are crucial.</p>
<div class="section" id="pathological-curvatures">
<h4>Pathological curvatures<a class="headerlink" href="#pathological-curvatures" title="Permalink to this headline">¶</a></h4>
<p>A pathological curvature is a type of surface that is similar to ravines and is particularly tricky for plain SGD optimization. In words, pathological curvatures typically have a steep gradient in one direction with an optimum at the center, while in a second direction we have a slower gradient towards a (global) optimum. Let’s first create an example surface of this and visualize it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_surface</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">x_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">):</span>

    <span class="c1"># Plot surface on xy plane; choose 3d or 2d plot</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Plot    </span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#000&quot;</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    
    <span class="c1"># Formatting plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_range</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">y_range</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_1$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_2$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">ax</span>


<span class="k">def</span> <span class="nf">plot_contour</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">x_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">):</span>

    <span class="c1"># Plot surface on xy plane; choose 3d or 2d plot</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="c1"># Plot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
    
    <span class="c1"># Formatting</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">x_range</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">y_range</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_1$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_2$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<p>Consider the function below which has a long, narrow, parabolic shaped flat valley. In terms of optimization, you can image that <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span> are weight parameters, and the curvature represents the loss surface over the space of <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span>. Note that in typical networks, we have many, many more parameters than two, and such curvatures can occur in multi-dimensional spaces as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pathological_curve_loss</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
    <span class="c1"># Example of a pathological curvature. There are many more possible, </span>
    <span class="c1"># feel free to experiment here!</span>
    <span class="n">x1_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span>
    <span class="n">x2_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x1_loss</span> <span class="o">+</span> <span class="n">x2_loss</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">plot_surface</span><span class="p">(</span><span class="n">pathological_curve_loss</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Pathological curvature&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/04-tensorflow-optim-init_97_0.svg" src="../../_images/04-tensorflow-optim-init_97_0.svg" /></div>
</div>
<p>Ideally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of <span class="math notranslate nohighlight">\(w_2\)</span>. However, if we encounter a point along the ridges, the gradient is much greater in <span class="math notranslate nohighlight">\(w_1\)</span> than <span class="math notranslate nohighlight">\(w_2\)</span>, and we might end up jumping from one side to the other. Due to the large gradients, we would have to reduce our learning rate slowing down learning significantly.</p>
<p>To test our algorithms, we can implement a simple function to train two parameters on such a surface:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">OptimModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">init</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w1&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">init</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w2&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_optimizer_path</span><span class="p">(</span><span class="n">optim_fn</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">loss_surface</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Plots trajectory of optimizer from initial point (`init`) along a surface</span>
<span class="sd">    (`loss_surface`) in minimizing it via gradient updates. Here `optim_func` is</span>
<span class="sd">    a function that takes an OptimModel and returns an optimizer. The output of</span>
<span class="sd">    this function is a list of 2-tuples which correspond to (x, y) coords of the</span>
<span class="sd">    points in its trajectory.&quot;&quot;&quot;</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">OptimModel</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">optim_fn</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">path</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()]</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_surface</span><span class="p">(</span><span class="o">*</span><span class="n">weights</span><span class="p">)</span>

        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">path</span><span class="p">),</span> <span class="n">losses</span>
</pre></div>
</div>
</div>
</div>
<p>Compute trajectories of gradient descent for the three optimizers starting from <code class="docutils literal notranslate"><span class="pre">(-5,</span> <span class="pre">5)</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">adam</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sgd</span>  <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">sgdm</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="n">SGDMomentum</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">]</span>
<span class="n">path_adam</span><span class="p">,</span> <span class="n">losses_adam</span> <span class="o">=</span> <span class="n">get_optimizer_path</span><span class="p">(</span><span class="n">adam</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">pathological_curve_loss</span><span class="p">)</span>
<span class="n">path_sgd</span><span class="p">,</span>  <span class="n">losses_sgd</span>  <span class="o">=</span> <span class="n">get_optimizer_path</span><span class="p">(</span><span class="n">sgd</span><span class="p">,</span>  <span class="n">init</span><span class="p">,</span> <span class="n">pathological_curve_loss</span><span class="p">)</span>
<span class="n">path_sgdm</span><span class="p">,</span> <span class="n">losses_sgdm</span> <span class="o">=</span> <span class="n">get_optimizer_path</span><span class="p">(</span><span class="n">sgdm</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">pathological_curve_loss</span><span class="p">)</span>

<span class="c1"># Get ranges of coordinates of the optimization trajectories + padding</span>
<span class="c1"># x = w1 and y = w2, i.e. see ordering on output of get_weights function</span>
<span class="n">all_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">path_adam</span><span class="p">,</span> <span class="n">path_sgd</span><span class="p">,</span> <span class="n">path_sgdm</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">all_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">all_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">y_range</span> <span class="o">=</span> <span class="p">(</span><span class="n">all_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">5</span><span class="p">,</span> <span class="n">all_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Plot surface on box defined by coordinate ranges</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plot_contour</span><span class="p">(</span><span class="n">pathological_curve_loss</span><span class="p">,</span> <span class="s2">&quot;Pathological curvature&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_range</span><span class="o">=</span><span class="n">x_range</span><span class="p">,</span> <span class="n">y_range</span><span class="o">=</span><span class="n">y_range</span><span class="p">)</span>

<span class="c1"># Plot trajectory of optimizers</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_adam</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">path_adam</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>  <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_sgd</span> <span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">path_sgd</span> <span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_sgdm</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">path_sgdm</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD+M&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot loss per optimizer step</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_adam</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_sgd</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span><span class="p">,</span>   <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_sgdm</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD+M&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Optimizer step&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/04-tensorflow-optim-init_101_0.svg" src="../../_images/04-tensorflow-optim-init_101_0.svg" /></div>
</div>
<p>We can clearly see that SGD is not able to find the center of the optimization curve and has a problem converging due to the steep gradients in. In contrast, Adam and SGD with momentum nicely converge as the changing direction of <span class="math notranslate nohighlight">\(w_1\)</span> is <strong>canceling itself out</strong>. On such surfaces, it is crucial to use momentum. Indeed, we used a momentum with value <span class="math notranslate nohighlight">\(0.9\)</span> which means, the current gradient contributes <span class="math notranslate nohighlight">\(0.10\)</span> of its original size.</p>
</div>
<div class="section" id="steep-optima">
<h4>Steep optima<a class="headerlink" href="#steep-optima" title="Permalink to this headline">¶</a></h4>
<p>A second type of challenging loss surfaces are steep optima. In those, we have a larger part of the surface having very small gradients while around the optimum, we have very large gradients. For instance, take the following loss surfaces:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bivar_gaussian</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">x_mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">y_mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">x_sig</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">y_sig</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_sig</span> <span class="o">*</span> <span class="n">y_sig</span><span class="p">)</span>
    <span class="n">x_exp</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">w1</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x_sig</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y_exp</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">w2</span> <span class="o">-</span> <span class="n">y_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">y_sig</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">norm</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_exp</span> <span class="o">+</span> <span class="n">y_exp</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">comb_func</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="n">bivar_gaussian</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">x_mean</span><span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">y_mean</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">y_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">-=</span> <span class="n">bivar_gaussian</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">x_mean</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">y_mean</span><span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">x_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">y_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">-=</span> <span class="n">bivar_gaussian</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">x_mean</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">y_mean</span><span class="o">=-</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">x_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">y_sig</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">plot_surface</span><span class="p">(</span><span class="n">comb_func</span><span class="p">,</span> <span class="n">x_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">y_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Steep optima&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/04-tensorflow-optim-init_104_0.svg" src="../../_images/04-tensorflow-optim-init_104_0.svg" /></div>
</div>
<p>Most of the loss surface has very little to no gradients. However, close to the optima, we have very steep gradients. To reach the minimum when starting in a region with lower gradients, we expect an adaptive learning rate to be crucial. To verify this hypothesis, we can run our three optimizers on the surface:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">adam</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">sgd</span>  <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">sgdm</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="n">SGDMomentum</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]</span>
<span class="n">path_adam</span><span class="p">,</span> <span class="n">losses_adam</span> <span class="o">=</span> <span class="n">get_optimizer_path</span><span class="p">(</span><span class="n">adam</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">comb_func</span><span class="p">)</span>
<span class="n">path_sgd</span><span class="p">,</span>  <span class="n">losses_sgd</span>  <span class="o">=</span> <span class="n">get_optimizer_path</span><span class="p">(</span><span class="n">sgd</span><span class="p">,</span>  <span class="n">init</span><span class="p">,</span> <span class="n">comb_func</span><span class="p">)</span>
<span class="n">path_sgdm</span><span class="p">,</span> <span class="n">losses_sgdm</span> <span class="o">=</span> <span class="n">get_optimizer_path</span><span class="p">(</span><span class="n">sgdm</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">comb_func</span><span class="p">)</span>

<span class="c1"># Get ranges of coordinates of the optimization trajectories + padding</span>
<span class="c1"># x = w1 and y = w2, i.e. see ordering on output of get_weights function</span>
<span class="n">all_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">path_adam</span><span class="p">,</span> <span class="n">path_sgd</span><span class="p">,</span> <span class="n">path_sgdm</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">)</span>
<span class="n">y_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">)</span>

<span class="c1"># Plot surface on box defined by coordinate ranges</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plot_contour</span><span class="p">(</span><span class="n">comb_func</span><span class="p">,</span> <span class="s2">&quot;Steep optima&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_range</span><span class="o">=</span><span class="n">x_range</span><span class="p">,</span> <span class="n">y_range</span><span class="o">=</span><span class="n">y_range</span><span class="p">)</span>

<span class="c1"># Plot trajectory of optimizers</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_adam</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">path_adam</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>  <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_sgd</span> <span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">path_sgd</span> <span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_sgdm</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">path_sgdm</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD+M&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot loss per optimizer step</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_adam</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span>  <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>  <span class="n">markersize</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_sgd</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span><span class="p">,</span>   <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_sgdm</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD+M&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Optimizer step&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>

<span class="c1"># Plot x-axis of trajectory of optimizers</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_adam</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>  <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span>  <span class="n">markersize</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_sgd</span> <span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">,</span>   <span class="n">markersize</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_sgdm</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD+M&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_1$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Plot y-axis of trajectory of optimizers</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_adam</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>  <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span>  <span class="n">markersize</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_sgd</span> <span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">,</span>   <span class="n">markersize</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path_sgdm</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD+M&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_2$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/04-tensorflow-optim-init_106_0.svg" src="../../_images/04-tensorflow-optim-init_106_0.svg" /></div>
</div>
<p>SGD first takes very small steps until it touches the border of the optimum. First reaching a point around <span class="math notranslate nohighlight">\((-0.5,-0.9)\)</span>, the gradient direction has changed and pushes the parameters to <span class="math notranslate nohighlight">\((0.5, 0.9)\)</span> from which SGD cannot recover anymore (only with many, many steps). A similar problem has SGD with momentum, only that it continues the direction of the touch of the optimum. The gradients from this time step are so much larger than any other point that the momentum <span class="math notranslate nohighlight">\({\boldsymbol m}_t\)</span> is overpowered by it despite having the factor <span class="math notranslate nohighlight">\((1-\beta_1) = 0.1.\)</span> Finally, Adam is able to converge in the optimum showing the importance of adaptive learning rates.</p>
</div>
</div>
<div class="section" id="what-optimizer-to-take">
<h3>What optimizer to take<a class="headerlink" href="#what-optimizer-to-take" title="Permalink to this headline">¶</a></h3>
<p>After seeing the results on optimization, what is our conclusion? Should we always use Adam and never look at SGD anymore? The short answer: no. There are many papers saying that in certain situations, SGD (with momentum) generalizes better where Adam often tends to overfit [<a class="reference external" href="https://proceedings.neurips.cc/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf">1</a>, <a class="reference external" href="https://arxiv.org/abs/1609.04747">2</a>]. This is related to the idea of finding wider optima. For instance, see the illustration of different optima below (credit: <a class="reference external" href="https://arxiv.org/pdf/1609.04836.pdf">Keskar et al., 2017</a>):</p>
<div class="figure align-default">
<img alt="../../_images/flat_vs_sharp_minima.svg" src="../../_images/flat_vs_sharp_minima.svg" /></div>
<p>The black line represents the training loss surface, while the dotted red line is the test loss. Finding sharp, narrow minima can be helpful for finding the minimal training loss. However, this doesn’t mean that it also minimizes the test loss as especially flat minima have shown to generalize better. You can imagine that the test dataset has a slightly shifted loss surface due to the different examples than in the training set. A small change can have a significant influence for sharp minima, while flat minima are generally more robust to this change.</p>
<p>In the notebook <a class="reference external" href="https://particle1331.github.io/steepest-ascent/notebooks/seb3/06-tensorflow-inception.html">Inception, ResNet, and DenseNet</a>, we will see that some network types can still be better optimized with SGD and learning rate scheduling than Adam. Nevertheless, Adam is the most commonly used optimizer in Deep Learning as it usually performs better than other optimizers, especially for deep networks.</p>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>In this notebook, we have looked at initialization and optimization techniques for neural networks. We have seen that a good initialization has to balance the preservation of the gradient variance as well as the activation variance. This can be achieved with the Xavier initialization for tanh-based networks, and the Kaiming initialization for ReLU-based networks. In optimization, concepts like momentum and adaptive learning rate can help with challenging loss surfaces but don’t guarantee an increase in performance for neural networks.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/tensorflow"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="03-tensorflow-activations.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Activation Functions</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../deployment/production-code.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Packaging Production Code</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By 𝗥𝗼𝗻 𝗠𝗲𝗱𝗶𝗻𝗮. Powered by <a href="https://jupyterbook.org">Jupyter Book</a>.<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>